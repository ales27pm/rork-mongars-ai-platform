[
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "JSONDecodeError",
        "importPath": "json",
        "description": "json",
        "isExtraImport": true,
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "posixpath",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "posixpath",
        "description": "posixpath",
        "detail": "posixpath",
        "documentation": {}
    },
    {
        "label": "gyp.common",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp.common",
        "description": "gyp.common",
        "detail": "gyp.common",
        "documentation": {}
    },
    {
        "label": "GetEnvironFallback",
        "importPath": "gyp.common",
        "description": "gyp.common",
        "isExtraImport": true,
        "detail": "gyp.common",
        "documentation": {}
    },
    {
        "label": "GypError",
        "importPath": "gyp.common",
        "description": "gyp.common",
        "isExtraImport": true,
        "detail": "gyp.common",
        "documentation": {}
    },
    {
        "label": "OrderedSet",
        "importPath": "gyp.common",
        "description": "gyp.common",
        "isExtraImport": true,
        "detail": "gyp.common",
        "documentation": {}
    },
    {
        "label": "GetEnvironFallback",
        "importPath": "gyp.common",
        "description": "gyp.common",
        "isExtraImport": true,
        "detail": "gyp.common",
        "documentation": {}
    },
    {
        "label": "GypError",
        "importPath": "gyp.common",
        "description": "gyp.common",
        "isExtraImport": true,
        "detail": "gyp.common",
        "documentation": {}
    },
    {
        "label": "OrderedSet",
        "importPath": "gyp.common",
        "description": "gyp.common",
        "isExtraImport": true,
        "detail": "gyp.common",
        "documentation": {}
    },
    {
        "label": "OrderedSet",
        "importPath": "gyp.common",
        "description": "gyp.common",
        "isExtraImport": true,
        "detail": "gyp.common",
        "documentation": {}
    },
    {
        "label": "GypError",
        "importPath": "gyp.common",
        "description": "gyp.common",
        "isExtraImport": true,
        "detail": "gyp.common",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "RegexFlag",
        "importPath": "re",
        "description": "re",
        "isExtraImport": true,
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "gyp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp",
        "description": "gyp",
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "MSVSNew",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "MSVSProject",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "MSVSSettings",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "MSVSToolFile",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "MSVSUserFile",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "MSVSUtil",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "MSVSVersion",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "easy_xml",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "MSVSUtil",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "ninja_syntax",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "easy_xml",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "MSVSSettings",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "easy_xml",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "easy_xml",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "easy_xml",
        "importPath": "gyp",
        "description": "gyp",
        "isExtraImport": true,
        "detail": "gyp",
        "documentation": {}
    },
    {
        "label": "make",
        "importPath": "gyp.generator",
        "description": "gyp.generator",
        "isExtraImport": true,
        "detail": "gyp.generator",
        "documentation": {}
    },
    {
        "label": "msvs",
        "importPath": "gyp.generator",
        "description": "gyp.generator",
        "isExtraImport": true,
        "detail": "gyp.generator",
        "documentation": {}
    },
    {
        "label": "ninja",
        "importPath": "gyp.generator",
        "description": "gyp.generator",
        "isExtraImport": true,
        "detail": "gyp.generator",
        "documentation": {}
    },
    {
        "label": "xcode",
        "importPath": "gyp.generator",
        "description": "gyp.generator",
        "isExtraImport": true,
        "detail": "gyp.generator",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "signal",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "signal",
        "description": "signal",
        "detail": "signal",
        "documentation": {}
    },
    {
        "label": "gyp.xcode_emulation",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp.xcode_emulation",
        "description": "gyp.xcode_emulation",
        "detail": "gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "XcodeSettings",
        "importPath": "gyp.xcode_emulation",
        "description": "gyp.xcode_emulation",
        "isExtraImport": true,
        "detail": "gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "gyp.msvs_emulation",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp.msvs_emulation",
        "description": "gyp.msvs_emulation",
        "detail": "gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "shlex",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shlex",
        "description": "shlex",
        "detail": "shlex",
        "documentation": {}
    },
    {
        "label": "xml.etree.ElementTree",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xml.etree.ElementTree",
        "description": "xml.etree.ElementTree",
        "detail": "xml.etree.ElementTree",
        "documentation": {}
    },
    {
        "label": "xml.sax.saxutils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xml.sax.saxutils",
        "description": "xml.sax.saxutils",
        "detail": "xml.sax.saxutils",
        "documentation": {}
    },
    {
        "label": "escape",
        "importPath": "xml.sax.saxutils",
        "description": "xml.sax.saxutils",
        "isExtraImport": true,
        "detail": "xml.sax.saxutils",
        "documentation": {}
    },
    {
        "label": "pprint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pprint",
        "description": "pprint",
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "code",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "code",
        "description": "code",
        "detail": "code",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "sha256",
        "importPath": "hashlib",
        "description": "hashlib",
        "isExtraImport": true,
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "sha256",
        "importPath": "hashlib",
        "description": "hashlib",
        "isExtraImport": true,
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "ntpath",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ntpath",
        "description": "ntpath",
        "detail": "ntpath",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "gyp.generator.ninja",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp.generator.ninja",
        "description": "gyp.generator.ninja",
        "detail": "gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BufferedWriter",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "copy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "ctypes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ctypes",
        "description": "ctypes",
        "detail": "ctypes",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "pathlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pathlib",
        "description": "pathlib",
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "errno",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "errno",
        "description": "errno",
        "detail": "errno",
        "documentation": {}
    },
    {
        "label": "filecmp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "filecmp",
        "description": "filecmp",
        "detail": "filecmp",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "gyp.xcode_ninja",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp.xcode_ninja",
        "description": "gyp.xcode_ninja",
        "detail": "gyp.xcode_ninja",
        "documentation": {}
    },
    {
        "label": "gyp.xcodeproj_file",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp.xcodeproj_file",
        "description": "gyp.xcodeproj_file",
        "detail": "gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "operator",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "operator",
        "description": "operator",
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "attrgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "attrgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "socket",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "socket",
        "description": "socket",
        "detail": "socket",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "MutableSet",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "MagicMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "locale",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "locale",
        "description": "locale",
        "detail": "locale",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "reduce",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "cmp_to_key",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "fcntl",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcntl",
        "description": "fcntl",
        "detail": "fcntl",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "gyp.simple_copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp.simple_copy",
        "description": "gyp.simple_copy",
        "detail": "gyp.simple_copy",
        "documentation": {}
    },
    {
        "label": "gyp.input",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp.input",
        "description": "gyp.input",
        "detail": "gyp.input",
        "documentation": {}
    },
    {
        "label": "fnmatch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fnmatch",
        "description": "fnmatch",
        "detail": "fnmatch",
        "documentation": {}
    },
    {
        "label": "plistlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "plistlib",
        "description": "plistlib",
        "detail": "plistlib",
        "documentation": {}
    },
    {
        "label": "gyp.MSVSUtil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp.MSVSUtil",
        "description": "gyp.MSVSUtil",
        "detail": "gyp.MSVSUtil",
        "documentation": {}
    },
    {
        "label": "gyp.MSVSVersion",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gyp.MSVSVersion",
        "description": "gyp.MSVSVersion",
        "detail": "gyp.MSVSVersion",
        "documentation": {}
    },
    {
        "label": "textwrap",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "textwrap",
        "description": "textwrap",
        "detail": "textwrap",
        "documentation": {}
    },
    {
        "label": "stat",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "stat",
        "description": "stat",
        "detail": "stat",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "ascii_letters",
        "importPath": "string",
        "description": "string",
        "isExtraImport": true,
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "digits",
        "importPath": "string",
        "description": "string",
        "isExtraImport": true,
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "xml.dom.minidom",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xml.dom.minidom",
        "description": "xml.dom.minidom",
        "detail": "xml.dom.minidom",
        "documentation": {}
    },
    {
        "label": "enum",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "enum",
        "description": "enum",
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "IntEnum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "auto",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "auto",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "IntEnum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "IntEnum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "auto",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "IntEnum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "typing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "typing",
        "description": "typing",
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "IO",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NoReturn",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "FrozenSet",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "FrozenSet",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NewType",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "SupportsInt",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "IO",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "get_args",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "get_origin",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "get_type_hints",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "IO",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Protocol",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ClassVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "runtime_checkable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ContextManager",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ContextManager",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ContextManager",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "SupportsIndex",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "closing",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "email.feedparser",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "email.feedparser",
        "description": "email.feedparser",
        "detail": "email.feedparser",
        "documentation": {}
    },
    {
        "label": "email.header",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "email.header",
        "description": "email.header",
        "detail": "email.header",
        "documentation": {}
    },
    {
        "label": "email.message",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "email.message",
        "description": "email.message",
        "detail": "email.message",
        "documentation": {}
    },
    {
        "label": "email.parser",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "email.parser",
        "description": "email.parser",
        "detail": "email.parser",
        "documentation": {}
    },
    {
        "label": "email.policy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "email.policy",
        "description": "email.policy",
        "detail": "email.policy",
        "documentation": {}
    },
    {
        "label": "abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "abc",
        "description": "abc",
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "sysconfig",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sysconfig",
        "description": "sysconfig",
        "detail": "sysconfig",
        "documentation": {}
    },
    {
        "label": "EXTENSION_SUFFIXES",
        "importPath": "importlib.machinery",
        "description": "importlib.machinery",
        "isExtraImport": true,
        "detail": "importlib.machinery",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "get_model_name_from_env_path",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "compare_tokens",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "exit_with_warning",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "get_model_name_from_env_path",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "compare_tokens",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "compare_tokens",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "exit_with_warning",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "transformers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "transformers",
        "description": "transformers",
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForImageTextToText",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PreTrainedTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPVisionModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "SiglipVisionModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "SiglipVisionModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "SiglipVisionConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "debug_hook",
        "importPath": "utils.common",
        "description": "utils.common",
        "isExtraImport": true,
        "detail": "utils.common",
        "documentation": {}
    },
    {
        "label": "save_output_data",
        "importPath": "utils.common",
        "description": "utils.common",
        "isExtraImport": true,
        "detail": "utils.common",
        "documentation": {}
    },
    {
        "label": "save_output_data",
        "importPath": "utils.common",
        "description": "utils.common",
        "isExtraImport": true,
        "detail": "utils.common",
        "documentation": {}
    },
    {
        "label": "HfApi",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "HfApi",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "HfApi",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "HfApi",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "safe_open",
        "importPath": "safetensors",
        "description": "safetensors",
        "isExtraImport": true,
        "detail": "safetensors",
        "documentation": {}
    },
    {
        "label": "safe_open",
        "importPath": "safetensors",
        "description": "safetensors",
        "isExtraImport": true,
        "detail": "safetensors",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "concurrent.futures",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ProcessPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "faulthandler",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "faulthandler",
        "description": "faulthandler",
        "detail": "faulthandler",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "prod",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "log2",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "ceil",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "prod",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "prod",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "mmap",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mmap",
        "description": "mmap",
        "detail": "mmap",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "zipfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "zipfile",
        "description": "zipfile",
        "detail": "zipfile",
        "documentation": {}
    },
    {
        "label": "gguf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gguf",
        "description": "gguf",
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "BaseVocab",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "Vocab",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "NoVocab",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "BpeVocab",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "SentencePieceVocab",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "LlamaHfVocab",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "GGUFWriter",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "GGUFReader",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "GGUFValueType",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "ReaderTensor",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "GGUFReader",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "GGUFWriter",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "GGUFValueType",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "ReaderField",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "GGUFReader",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "GGUFReader",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "gguf",
        "description": "gguf",
        "isExtraImport": true,
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "TypeAdapter",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "create_model",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "MinLen",
        "importPath": "annotated_types",
        "description": "annotated_types",
        "isExtraImport": true,
        "detail": "annotated_types",
        "documentation": {}
    },
    {
        "label": "json,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json.",
        "description": "json.",
        "detail": "json.",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "getdoc",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "isclass",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "parse",
        "importPath": "docstring_parser",
        "description": "docstring_parser",
        "isExtraImport": true,
        "detail": "docstring_parser",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "add_run_method_to_dynamic_model",
        "importPath": "pydantic_models_to_grammar",
        "description": "pydantic_models_to_grammar",
        "isExtraImport": true,
        "detail": "pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "convert_dictionary_to_pydantic_model",
        "importPath": "pydantic_models_to_grammar",
        "description": "pydantic_models_to_grammar",
        "isExtraImport": true,
        "detail": "pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "create_dynamic_model_from_function",
        "importPath": "pydantic_models_to_grammar",
        "description": "pydantic_models_to_grammar",
        "isExtraImport": true,
        "detail": "pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_gbnf_grammar_and_documentation",
        "importPath": "pydantic_models_to_grammar",
        "description": "pydantic_models_to_grammar",
        "isExtraImport": true,
        "detail": "pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "asyncio.threads",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio.threads",
        "description": "asyncio.threads",
        "detail": "asyncio.threads",
        "documentation": {}
    },
    {
        "label": "GGUFReader",
        "importPath": "gguf.gguf_reader",
        "description": "gguf.gguf_reader",
        "isExtraImport": true,
        "detail": "gguf.gguf_reader",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLineEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFileDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTableWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTableWidgetItem",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTabWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFormLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHeaderView",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialogButtonBox",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPlainTextEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFileDialog",
        "importPath": "PySide6.QtWidgets",
        "description": "PySide6.QtWidgets",
        "isExtraImport": true,
        "detail": "PySide6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QRect",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "QSize",
        "importPath": "PySide6.QtCore",
        "description": "PySide6.QtCore",
        "isExtraImport": true,
        "detail": "PySide6.QtCore",
        "documentation": {}
    },
    {
        "label": "TokenType",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "RopeScalingType",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "PoolingType",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGMLQuantizationType",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGML_QUANT_SIZES",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGUF_DEFAULT_ALIGNMENT",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGUF_MAGIC",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGUF_VERSION",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGMLQuantizationType",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGUFValueType",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGUFEndian",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGMLQuantizationType",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGUFValueType",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "numpy.typing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy.typing",
        "description": "numpy.typing",
        "detail": "numpy.typing",
        "documentation": {}
    },
    {
        "label": "DTypeLike",
        "importPath": "numpy.typing",
        "description": "numpy.typing",
        "isExtraImport": true,
        "detail": "numpy.typing",
        "documentation": {}
    },
    {
        "label": "DTypeLike",
        "importPath": "numpy.typing",
        "description": "numpy.typing",
        "isExtraImport": true,
        "detail": "numpy.typing",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "jinja2.ext",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jinja2.ext",
        "description": "jinja2.ext",
        "detail": "jinja2.ext",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QColorConstants",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QTextCursor",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "QTextFormat",
        "importPath": "PySide6.QtGui",
        "description": "PySide6.QtGui",
        "isExtraImport": true,
        "detail": "PySide6.QtGui",
        "documentation": {}
    },
    {
        "label": "TemplateSyntaxError",
        "importPath": "jinja2",
        "description": "jinja2",
        "isExtraImport": true,
        "detail": "jinja2",
        "documentation": {}
    },
    {
        "label": "ImmutableSandboxedEnvironment",
        "importPath": "jinja2.sandbox",
        "description": "jinja2.sandbox",
        "isExtraImport": true,
        "detail": "jinja2.sandbox",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "heapq",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "heapq",
        "description": "heapq",
        "detail": "heapq",
        "documentation": {}
    },
    {
        "label": "sqlite3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlite3",
        "description": "sqlite3",
        "detail": "sqlite3",
        "documentation": {}
    },
    {
        "label": "array",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "array",
        "description": "array",
        "detail": "array",
        "documentation": {}
    },
    {
        "label": "unicodedata",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unicodedata",
        "description": "unicodedata",
        "detail": "unicodedata",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets",
        "description": "datasets",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "thread_map",
        "importPath": "tqdm.contrib.concurrent",
        "description": "tqdm.contrib.concurrent",
        "isExtraImport": true,
        "detail": "tqdm.contrib.concurrent",
        "documentation": {}
    },
    {
        "label": "urllib.request",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.request",
        "description": "urllib.request",
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "mean",
        "importPath": "statistics",
        "description": "statistics",
        "isExtraImport": true,
        "detail": "statistics",
        "documentation": {}
    },
    {
        "label": "median",
        "importPath": "statistics",
        "description": "statistics",
        "isExtraImport": true,
        "detail": "statistics",
        "documentation": {}
    },
    {
        "label": "mean",
        "importPath": "statistics",
        "description": "statistics",
        "isExtraImport": true,
        "detail": "statistics",
        "documentation": {}
    },
    {
        "label": "atexit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "atexit",
        "description": "atexit",
        "detail": "atexit",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "typer",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "typer",
        "description": "typer",
        "detail": "typer",
        "documentation": {}
    },
    {
        "label": "Buffer",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "cffi",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cffi",
        "description": "cffi",
        "detail": "cffi",
        "documentation": {}
    },
    {
        "label": "save_file",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "save_file",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "_calculate_fan_in_and_fan_out",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "ACT2FN",
        "importPath": "transformers.activations",
        "description": "transformers.activations",
        "isExtraImport": true,
        "detail": "transformers.activations",
        "documentation": {}
    },
    {
        "label": "PreTrainedModel",
        "importPath": "transformers.modeling_utils",
        "description": "transformers.modeling_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_utils",
        "documentation": {}
    },
    {
        "label": "PretrainedConfig",
        "importPath": "transformers.configuration_utils",
        "description": "transformers.configuration_utils",
        "isExtraImport": true,
        "detail": "transformers.configuration_utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "Idefics2VisionTransformer",
        "importPath": "transformers.models.idefics2.modeling_idefics2",
        "description": "transformers.models.idefics2.modeling_idefics2",
        "isExtraImport": true,
        "detail": "transformers.models.idefics2.modeling_idefics2",
        "documentation": {}
    },
    {
        "label": "Idefics2VisionConfig",
        "importPath": "transformers.models.idefics2.configuration_idefics2",
        "description": "transformers.models.idefics2.configuration_idefics2",
        "isExtraImport": true,
        "detail": "transformers.models.idefics2.configuration_idefics2",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "matplotlib.dates",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.dates",
        "description": "matplotlib.dates",
        "detail": "matplotlib.dates",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "TEST_TOOL",
        "importPath": "unit.test_tool_call",
        "description": "unit.test_tool_call",
        "isExtraImport": true,
        "detail": "unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "wget",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wget",
        "description": "wget",
        "detail": "wget",
        "documentation": {}
    },
    {
        "label": "MistralTokenizerType",
        "importPath": "gguf.vocab",
        "description": "gguf.vocab",
        "isExtraImport": true,
        "detail": "gguf.vocab",
        "documentation": {}
    },
    {
        "label": "MistralVocab",
        "importPath": "gguf.vocab",
        "description": "gguf.vocab",
        "isExtraImport": true,
        "detail": "gguf.vocab",
        "documentation": {}
    },
    {
        "label": "LazyTorchTensor",
        "importPath": "convert_hf_to_gguf",
        "description": "convert_hf_to_gguf",
        "isExtraImport": true,
        "detail": "convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ModelBase",
        "importPath": "convert_hf_to_gguf",
        "description": "convert_hf_to_gguf",
        "isExtraImport": true,
        "detail": "convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Target",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "class Target:\n    \"\"\"Holds information about a particular target:\n    deps: set of Targets this Target depends upon. This is not recursive, only the\n      direct dependent Targets.\n    match_status: one of the MatchStatus values.\n    back_deps: set of Targets that have a dependency on this Target.\n    visited: used during iteration to indicate whether we've visited this target.\n      This is used for two iterations, once in building the set of Targets and\n      again in _GetBuildTargets().\n    name: fully qualified name of the target.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "Config",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "class Config:\n    \"\"\"Details what we're looking for\n    files: set of files to search for\n    targets: see file description for details.\"\"\"\n    def __init__(self):\n        self.files = []\n        self.targets = set()\n        self.additional_compile_target_names = set()\n        self.test_target_names = set()\n    def Init(self, params):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "TargetCalculator",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "class TargetCalculator:\n    \"\"\"Calculates the matching test_targets and matching compile_targets.\"\"\"\n    def __init__(\n        self,\n        files,\n        additional_compile_target_names,\n        test_target_names,\n        data,\n        target_list,\n        target_dicts,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "CalculateVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "def CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"\n    flavor = gyp.common.GetFlavor(params)\n    if flavor == \"mac\":\n        default_variables.setdefault(\"OS\", \"mac\")\n    elif flavor == \"win\":\n        default_variables.setdefault(\"OS\", \"win\")\n        gyp.msvs_emulation.CalculateCommonVariables(default_variables, params)\n    else:\n        operating_system = flavor",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    \"\"\"Called by gyp as the final stage. Outputs results.\"\"\"\n    config = Config()\n    try:\n        config.Init(params)\n        if not config.files:\n            raise Exception(\n                \"Must specify files to analyze via config_path generator flag\"\n            )\n        toplevel_dir = _ToGypPath(os.path.abspath(params[\"options\"].toplevel_dir))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "files",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "files = [\"b.cc\", \"d.cc\"] (B depends upon b.cc and D depends upon d.cc), then\nthe following is output:\n|compile_targets| = [\"B\"] B must built as it depends upon the changed file b.cc\nand the supplied target A depends upon it. A is not output as a build_target\nas it is of type none with no rules and actions.\n|test_targets| = [\"B\"] B directly depends upon the change file b.cc.\nEven though the file d.cc, which D depends upon, has changed D is not output\nas it was not supplied by way of |additional_compile_targets| or |test_targets|.\nIf the generator flag analyzer_output_path is specified, output is written\nthere. Otherwise output is written to stdout.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "|compile_targets|",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "|compile_targets| = [\"B\"] B must built as it depends upon the changed file b.cc\nand the supplied target A depends upon it. A is not output as a build_target\nas it is of type none with no rules and actions.\n|test_targets| = [\"B\"] B directly depends upon the change file b.cc.\nEven though the file d.cc, which D depends upon, has changed D is not output\nas it was not supplied by way of |additional_compile_targets| or |test_targets|.\nIf the generator flag analyzer_output_path is specified, output is written\nthere. Otherwise output is written to stdout.\nIn Gyp the \"all\" target is shorthand for the root targets in the files passed\nto gyp. For example, if file \"a.gyp\" contains targets \"a1\" and",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "|test_targets|",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "|test_targets| = [\"B\"] B directly depends upon the change file b.cc.\nEven though the file d.cc, which D depends upon, has changed D is not output\nas it was not supplied by way of |additional_compile_targets| or |test_targets|.\nIf the generator flag analyzer_output_path is specified, output is written\nthere. Otherwise output is written to stdout.\nIn Gyp the \"all\" target is shorthand for the root targets in the files passed\nto gyp. For example, if file \"a.gyp\" contains targets \"a1\" and\n\"a2\", and file \"b.gyp\" contains targets \"b1\" and \"b2\" and \"a2\" has a dependency\non \"b2\" and gyp is supplied \"a.gyp\" then \"all\" consists of \"a1\" and \"a2\".\nNotice that \"b1\" and \"b2\" are not in the \"all\" target as \"b.gyp\" was not",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "debug",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "debug = False\nfound_dependency_string = \"Found dependency\"\nno_dependency_string = \"No dependencies\"\n# Status when it should be assumed that everything has changed.\nall_changed_string = \"Found dependency (all)\"\n# MatchStatus is used indicate if and how a target depends upon the supplied\n# sources.\n# The target's sources contain one of the supplied paths.\nMATCH_STATUS_MATCHES = 1\n# The target has a dependency on another target that contains one of the",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "found_dependency_string",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "found_dependency_string = \"Found dependency\"\nno_dependency_string = \"No dependencies\"\n# Status when it should be assumed that everything has changed.\nall_changed_string = \"Found dependency (all)\"\n# MatchStatus is used indicate if and how a target depends upon the supplied\n# sources.\n# The target's sources contain one of the supplied paths.\nMATCH_STATUS_MATCHES = 1\n# The target has a dependency on another target that contains one of the\n# supplied paths.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "no_dependency_string",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "no_dependency_string = \"No dependencies\"\n# Status when it should be assumed that everything has changed.\nall_changed_string = \"Found dependency (all)\"\n# MatchStatus is used indicate if and how a target depends upon the supplied\n# sources.\n# The target's sources contain one of the supplied paths.\nMATCH_STATUS_MATCHES = 1\n# The target has a dependency on another target that contains one of the\n# supplied paths.\nMATCH_STATUS_MATCHES_BY_DEPENDENCY = 2",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "all_changed_string",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "all_changed_string = \"Found dependency (all)\"\n# MatchStatus is used indicate if and how a target depends upon the supplied\n# sources.\n# The target's sources contain one of the supplied paths.\nMATCH_STATUS_MATCHES = 1\n# The target has a dependency on another target that contains one of the\n# supplied paths.\nMATCH_STATUS_MATCHES_BY_DEPENDENCY = 2\n# The target's sources weren't in the supplied paths and none of the target's\n# dependencies depend upon a target that matched.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "MATCH_STATUS_MATCHES",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "MATCH_STATUS_MATCHES = 1\n# The target has a dependency on another target that contains one of the\n# supplied paths.\nMATCH_STATUS_MATCHES_BY_DEPENDENCY = 2\n# The target's sources weren't in the supplied paths and none of the target's\n# dependencies depend upon a target that matched.\nMATCH_STATUS_DOESNT_MATCH = 3\n# The target doesn't contain the source, but the dependent targets have not yet\n# been visited to determine a more specific status yet.\nMATCH_STATUS_TBD = 4",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "MATCH_STATUS_MATCHES_BY_DEPENDENCY",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "MATCH_STATUS_MATCHES_BY_DEPENDENCY = 2\n# The target's sources weren't in the supplied paths and none of the target's\n# dependencies depend upon a target that matched.\nMATCH_STATUS_DOESNT_MATCH = 3\n# The target doesn't contain the source, but the dependent targets have not yet\n# been visited to determine a more specific status yet.\nMATCH_STATUS_TBD = 4\ngenerator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ngenerator_wants_static_library_dependencies_adjusted = False\ngenerator_default_variables = {}",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "MATCH_STATUS_DOESNT_MATCH",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "MATCH_STATUS_DOESNT_MATCH = 3\n# The target doesn't contain the source, but the dependent targets have not yet\n# been visited to determine a more specific status yet.\nMATCH_STATUS_TBD = 4\ngenerator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ngenerator_wants_static_library_dependencies_adjusted = False\ngenerator_default_variables = {}\nfor dirname in [\n    \"INTERMEDIATE_DIR\",\n    \"SHARED_INTERMEDIATE_DIR\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "MATCH_STATUS_TBD",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "MATCH_STATUS_TBD = 4\ngenerator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ngenerator_wants_static_library_dependencies_adjusted = False\ngenerator_default_variables = {}\nfor dirname in [\n    \"INTERMEDIATE_DIR\",\n    \"SHARED_INTERMEDIATE_DIR\",\n    \"PRODUCT_DIR\",\n    \"LIB_DIR\",\n    \"SHARED_LIB_DIR\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "generator_supports_multiple_toolsets",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "generator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ngenerator_wants_static_library_dependencies_adjusted = False\ngenerator_default_variables = {}\nfor dirname in [\n    \"INTERMEDIATE_DIR\",\n    \"SHARED_INTERMEDIATE_DIR\",\n    \"PRODUCT_DIR\",\n    \"LIB_DIR\",\n    \"SHARED_LIB_DIR\",\n]:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "generator_wants_static_library_dependencies_adjusted",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "generator_wants_static_library_dependencies_adjusted = False\ngenerator_default_variables = {}\nfor dirname in [\n    \"INTERMEDIATE_DIR\",\n    \"SHARED_INTERMEDIATE_DIR\",\n    \"PRODUCT_DIR\",\n    \"LIB_DIR\",\n    \"SHARED_LIB_DIR\",\n]:\n    generator_default_variables[dirname] = \"!!!\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "peekOfCode": "generator_default_variables = {}\nfor dirname in [\n    \"INTERMEDIATE_DIR\",\n    \"SHARED_INTERMEDIATE_DIR\",\n    \"PRODUCT_DIR\",\n    \"LIB_DIR\",\n    \"SHARED_LIB_DIR\",\n]:\n    generator_default_variables[dirname] = \"!!!\"\nfor unused in [",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.analyzer",
        "documentation": {}
    },
    {
        "label": "AndroidMkWriter",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "class AndroidMkWriter:\n    \"\"\"AndroidMkWriter packages up the writing of one target-specific Android.mk.\n    Its only real entry point is Write(), and is mostly used for namespacing.\n    \"\"\"\n    def __init__(self, android_top_dir):\n        self.android_top_dir = android_top_dir\n    def Write(\n        self,\n        qualified_target,\n        relative_target,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "IsCPPExtension",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "def IsCPPExtension(ext):\n    return make.COMPILABLE_EXTENSIONS.get(ext) == \"cxx\"\ndef Sourceify(path):\n    \"\"\"Convert a path to its source directory form. The Android backend does not\n    support options.generator_output, so this function is a noop.\"\"\"\n    return path\n# Map from qualified target to path to output.\n# For Android, the target of these maps is a tuple ('static', 'modulename'),\n# ('dynamic', 'modulename'), or ('path', 'some/path') instead of a string,\n# since we link by module.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "Sourceify",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "def Sourceify(path):\n    \"\"\"Convert a path to its source directory form. The Android backend does not\n    support options.generator_output, so this function is a noop.\"\"\"\n    return path\n# Map from qualified target to path to output.\n# For Android, the target of these maps is a tuple ('static', 'modulename'),\n# ('dynamic', 'modulename'), or ('path', 'some/path') instead of a string,\n# since we link by module.\ntarget_outputs = {}\n# Map from qualified target to any linkable output.  A subset",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "PerformBuild",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "def PerformBuild(data, configurations, params):\n    # The android backend only supports the default configuration.\n    options = params[\"options\"]\n    makefile = os.path.abspath(os.path.join(options.toplevel_dir, \"GypAndroid.mk\"))\n    env = dict(os.environ)\n    env[\"ONE_SHOT_MAKEFILE\"] = makefile\n    arguments = [\"make\", \"-C\", os.environ[\"ANDROID_BUILD_TOP\"], \"gyp_all_modules\"]\n    print(\"Building: %s\" % arguments)\n    subprocess.check_call(arguments, env=env)\ndef GenerateOutput(target_list, target_dicts, data, params):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    options = params[\"options\"]\n    generator_flags = params.get(\"generator_flags\", {})\n    limit_to_target_all = generator_flags.get(\"limit_to_target_all\", False)\n    write_alias_targets = generator_flags.get(\"write_alias_targets\", True)\n    sdk_version = generator_flags.get(\"aosp_sdk_version\", 0)\n    android_top_dir = os.environ.get(\"ANDROID_BUILD_TOP\")\n    assert android_top_dir, \"$ANDROID_BUILD_TOP not set; you need to run lunch.\"\n    def CalculateMakefilePath(build_file, base_name):\n        \"\"\"Determine where to write a Makefile for a given gyp file.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "generator_default_variables = {\n    \"OS\": \"android\",\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"STATIC_LIB_PREFIX\": \"lib\",\n    \"SHARED_LIB_PREFIX\": \"lib\",\n    \"STATIC_LIB_SUFFIX\": \".a\",\n    \"SHARED_LIB_SUFFIX\": \".so\",\n    \"INTERMEDIATE_DIR\": \"$(gyp_intermediate_dir)\",\n    \"SHARED_INTERMEDIATE_DIR\": \"$(gyp_shared_intermediate_dir)\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "generator_supports_multiple_toolsets",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "generator_supports_multiple_toolsets = True\n# Generator-specific gyp specs.\ngenerator_additional_non_configuration_keys = [\n    # Boolean to declare that this target does not want its name mangled.\n    \"android_unmangled_name\",\n    # Map of android build system variables to set.\n    \"aosp_build_settings\",\n]\ngenerator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "generator_additional_non_configuration_keys",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "generator_additional_non_configuration_keys = [\n    # Boolean to declare that this target does not want its name mangled.\n    \"android_unmangled_name\",\n    # Map of android build system variables to set.\n    \"aosp_build_settings\",\n]\ngenerator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []\nALL_MODULES_FOOTER = \"\"\"\\\n# \"gyp_all_modules\" is a concatenation of the \"gyp_all_modules\" targets from",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "generator_additional_path_sections",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "generator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []\nALL_MODULES_FOOTER = \"\"\"\\\n# \"gyp_all_modules\" is a concatenation of the \"gyp_all_modules\" targets from\n# all the included sub-makefiles. This is just here to clarify.\ngyp_all_modules:\n\"\"\"\nheader = \"\"\"\\\n# This file is generated by gyp; do not edit.\n\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "generator_extra_sources_for_rules",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "generator_extra_sources_for_rules = []\nALL_MODULES_FOOTER = \"\"\"\\\n# \"gyp_all_modules\" is a concatenation of the \"gyp_all_modules\" targets from\n# all the included sub-makefiles. This is just here to clarify.\ngyp_all_modules:\n\"\"\"\nheader = \"\"\"\\\n# This file is generated by gyp; do not edit.\n\"\"\"\n# Map gyp target types to Android module classes.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "ALL_MODULES_FOOTER",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "ALL_MODULES_FOOTER = \"\"\"\\\n# \"gyp_all_modules\" is a concatenation of the \"gyp_all_modules\" targets from\n# all the included sub-makefiles. This is just here to clarify.\ngyp_all_modules:\n\"\"\"\nheader = \"\"\"\\\n# This file is generated by gyp; do not edit.\n\"\"\"\n# Map gyp target types to Android module classes.\nMODULE_CLASSES = {",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "header",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "header = \"\"\"\\\n# This file is generated by gyp; do not edit.\n\"\"\"\n# Map gyp target types to Android module classes.\nMODULE_CLASSES = {\n    \"static_library\": \"STATIC_LIBRARIES\",\n    \"shared_library\": \"SHARED_LIBRARIES\",\n    \"executable\": \"EXECUTABLES\",\n}\ndef IsCPPExtension(ext):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "MODULE_CLASSES",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "MODULE_CLASSES = {\n    \"static_library\": \"STATIC_LIBRARIES\",\n    \"shared_library\": \"SHARED_LIBRARIES\",\n    \"executable\": \"EXECUTABLES\",\n}\ndef IsCPPExtension(ext):\n    return make.COMPILABLE_EXTENSIONS.get(ext) == \"cxx\"\ndef Sourceify(path):\n    \"\"\"Convert a path to its source directory form. The Android backend does not\n    support options.generator_output, so this function is a noop.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "target_outputs",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "target_outputs = {}\n# Map from qualified target to any linkable output.  A subset\n# of target_outputs.  E.g. when mybinary depends on liba, we want to\n# include liba in the linker line; when otherbinary depends on\n# mybinary, we just want to build mybinary first.\ntarget_link_deps = {}\nclass AndroidMkWriter:\n    \"\"\"AndroidMkWriter packages up the writing of one target-specific Android.mk.\n    Its only real entry point is Write(), and is mostly used for namespacing.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "target_link_deps",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "peekOfCode": "target_link_deps = {}\nclass AndroidMkWriter:\n    \"\"\"AndroidMkWriter packages up the writing of one target-specific Android.mk.\n    Its only real entry point is Write(), and is mostly used for namespacing.\n    \"\"\"\n    def __init__(self, android_top_dir):\n        self.android_top_dir = android_top_dir\n    def Write(\n        self,\n        qualified_target,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.android",
        "documentation": {}
    },
    {
        "label": "CMakeTargetType",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "class CMakeTargetType:\n    def __init__(self, command, modifier, property_modifier):\n        self.command = command\n        self.modifier = modifier\n        self.property_modifier = property_modifier\ncmake_target_type_from_gyp_target_type = {\n    \"executable\": CMakeTargetType(\"add_executable\", None, \"RUNTIME\"),\n    \"static_library\": CMakeTargetType(\"add_library\", \"STATIC\", \"ARCHIVE\"),\n    \"shared_library\": CMakeTargetType(\"add_library\", \"SHARED\", \"LIBRARY\"),\n    \"loadable_module\": CMakeTargetType(\"add_library\", \"MODULE\", \"LIBRARY\"),",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "CMakeNamer",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "class CMakeNamer:\n    \"\"\"Converts Gyp target names into CMake target names.\n    CMake requires that target names be globally unique. One way to ensure\n    this is to fully qualify the names of the targets. Unfortunately, this\n    ends up with all targets looking like \"chrome_chrome_gyp_chrome\" instead\n    of just \"chrome\". If this generator were only interested in building, it\n    would be possible to fully qualify all target names, then create\n    unqualified target names which depend on all qualified targets which\n    should have had that name. This is more or less what the 'make' generator\n    does with aliases. However, one goal of this generator is to create CMake",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "RemovePrefix",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def RemovePrefix(a, prefix):\n    \"\"\"Returns 'a' without 'prefix' if it starts with 'prefix'.\"\"\"\n    return a[len(prefix) :] if a.startswith(prefix) else a\ndef CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"\n    default_variables.setdefault(\"OS\", gyp.common.GetFlavor(params))\ndef Compilable(filename):\n    \"\"\"Return true if the file is compilable (should be in OBJS).\"\"\"\n    return any(filename.endswith(e) for e in COMPILABLE_EXTENSIONS)\ndef Linkable(filename):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "CalculateVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"\n    default_variables.setdefault(\"OS\", gyp.common.GetFlavor(params))\ndef Compilable(filename):\n    \"\"\"Return true if the file is compilable (should be in OBJS).\"\"\"\n    return any(filename.endswith(e) for e in COMPILABLE_EXTENSIONS)\ndef Linkable(filename):\n    \"\"\"Return true if the file is linkable (should be on the link line).\"\"\"\n    return filename.endswith(\".o\")\ndef NormjoinPathForceCMakeSource(base_path, rel_path):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "Compilable",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def Compilable(filename):\n    \"\"\"Return true if the file is compilable (should be in OBJS).\"\"\"\n    return any(filename.endswith(e) for e in COMPILABLE_EXTENSIONS)\ndef Linkable(filename):\n    \"\"\"Return true if the file is linkable (should be on the link line).\"\"\"\n    return filename.endswith(\".o\")\ndef NormjoinPathForceCMakeSource(base_path, rel_path):\n    \"\"\"Resolves rel_path against base_path and returns the result.\n    If rel_path is an absolute path it is returned unchanged.\n    Otherwise it is resolved against base_path and normalized.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "Linkable",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def Linkable(filename):\n    \"\"\"Return true if the file is linkable (should be on the link line).\"\"\"\n    return filename.endswith(\".o\")\ndef NormjoinPathForceCMakeSource(base_path, rel_path):\n    \"\"\"Resolves rel_path against base_path and returns the result.\n    If rel_path is an absolute path it is returned unchanged.\n    Otherwise it is resolved against base_path and normalized.\n    If the result is a relative path, it is forced to be relative to the\n    CMakeLists.txt.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "NormjoinPathForceCMakeSource",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def NormjoinPathForceCMakeSource(base_path, rel_path):\n    \"\"\"Resolves rel_path against base_path and returns the result.\n    If rel_path is an absolute path it is returned unchanged.\n    Otherwise it is resolved against base_path and normalized.\n    If the result is a relative path, it is forced to be relative to the\n    CMakeLists.txt.\n    \"\"\"\n    if os.path.isabs(rel_path):\n        return rel_path\n    if any(rel_path.startswith(var) for var in FULL_PATH_VARS):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "NormjoinPath",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def NormjoinPath(base_path, rel_path):\n    \"\"\"Resolves rel_path against base_path and returns the result.\n    TODO: what is this really used for?\n    If rel_path begins with '$' it is returned unchanged.\n    Otherwise it is resolved against base_path if relative, then normalized.\n    \"\"\"\n    if rel_path.startswith(\"$\") and not rel_path.startswith(\"${configuration}\"):\n        return rel_path\n    return os.path.normpath(os.path.join(base_path, rel_path))\ndef CMakeStringEscape(a):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "CMakeStringEscape",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def CMakeStringEscape(a):\n    \"\"\"Escapes the string 'a' for use inside a CMake string.\n    This means escaping\n    '\\' otherwise it may be seen as modifying the next character\n    '\"' otherwise it will end the string\n    ';' otherwise the string becomes a list\n    The following do not need to be escaped\n    '#' when the lexer is in string state, this does not start a comment\n    The following are yet unknown\n    '$' generator variables (like ${obj}) must not be escaped,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "SetFileProperty",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def SetFileProperty(output, source_name, property_name, values, sep):\n    \"\"\"Given a set of source file, sets the given property on them.\"\"\"\n    output.write(\"set_source_files_properties(\")\n    output.write(source_name)\n    output.write(\" PROPERTIES \")\n    output.write(property_name)\n    output.write(' \"')\n    for value in values:\n        output.write(CMakeStringEscape(value))\n        output.write(sep)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "SetFilesProperty",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def SetFilesProperty(output, variable, property_name, values, sep):\n    \"\"\"Given a set of source files, sets the given property on them.\"\"\"\n    output.write(\"set_source_files_properties(\")\n    WriteVariable(output, variable)\n    output.write(\" PROPERTIES \")\n    output.write(property_name)\n    output.write(' \"')\n    for value in values:\n        output.write(CMakeStringEscape(value))\n        output.write(sep)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "SetTargetProperty",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def SetTargetProperty(output, target_name, property_name, values, sep=\"\"):\n    \"\"\"Given a target, sets the given property.\"\"\"\n    output.write(\"set_target_properties(\")\n    output.write(target_name)\n    output.write(\" PROPERTIES \")\n    output.write(property_name)\n    output.write(' \"')\n    for value in values:\n        output.write(CMakeStringEscape(value))\n        output.write(sep)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "SetVariable",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def SetVariable(output, variable_name, value):\n    \"\"\"Sets a CMake variable.\"\"\"\n    output.write(\"set(\")\n    output.write(variable_name)\n    output.write(' \"')\n    output.write(CMakeStringEscape(value))\n    output.write('\")\\n')\ndef SetVariableList(output, variable_name, values):\n    \"\"\"Sets a CMake variable to a list.\"\"\"\n    if not values:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "SetVariableList",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def SetVariableList(output, variable_name, values):\n    \"\"\"Sets a CMake variable to a list.\"\"\"\n    if not values:\n        return SetVariable(output, variable_name, \"\")\n    if len(values) == 1:\n        return SetVariable(output, variable_name, values[0])\n    output.write(\"list(APPEND \")\n    output.write(variable_name)\n    output.write('\\n  \"')\n    output.write('\"\\n  \"'.join([CMakeStringEscape(value) for value in values]))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "UnsetVariable",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def UnsetVariable(output, variable_name):\n    \"\"\"Unsets a CMake variable.\"\"\"\n    output.write(\"unset(\")\n    output.write(variable_name)\n    output.write(\")\\n\")\ndef WriteVariable(output, variable_name, prepend=None):\n    if prepend:\n        output.write(prepend)\n    output.write(\"${\")\n    output.write(variable_name)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "WriteVariable",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def WriteVariable(output, variable_name, prepend=None):\n    if prepend:\n        output.write(prepend)\n    output.write(\"${\")\n    output.write(variable_name)\n    output.write(\"}\")\nclass CMakeTargetType:\n    def __init__(self, command, modifier, property_modifier):\n        self.command = command\n        self.modifier = modifier",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "StringToCMakeTargetName",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def StringToCMakeTargetName(a):\n    \"\"\"Converts the given string 'a' to a valid CMake target name.\n    All invalid characters are replaced by '_'.\n    Invalid for cmake: ' ', '/', '(', ')', '\"'\n    Invalid for make: ':'\n    Invalid for unknown reasons but cause failures: '.'\n    \"\"\"\n    return a.translate(_maketrans(' /():.\"', \"_______\"))\ndef WriteActions(target_name, actions, extra_sources, extra_deps, path_to_gyp, output):\n    \"\"\"Write CMake for the 'actions' in the target.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "WriteActions",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def WriteActions(target_name, actions, extra_sources, extra_deps, path_to_gyp, output):\n    \"\"\"Write CMake for the 'actions' in the target.\n    Args:\n      target_name: the name of the CMake target being generated.\n      actions: the Gyp 'actions' dict for this target.\n      extra_sources: [(<cmake_src>, <src>)] to append with generated source files.\n      extra_deps: [<cmake_target>] to append with generated targets.\n      path_to_gyp: relative path from CMakeLists.txt being generated to\n          the Gyp file in which the target being generated is defined.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "NormjoinRulePathForceCMakeSource",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def NormjoinRulePathForceCMakeSource(base_path, rel_path, rule_source):\n    if rel_path.startswith((\"${RULE_INPUT_PATH}\", \"${RULE_INPUT_DIRNAME}\")):\n        if any(rule_source.startswith(var) for var in FULL_PATH_VARS):\n            return rel_path\n    return NormjoinPathForceCMakeSource(base_path, rel_path)\ndef WriteRules(target_name, rules, extra_sources, extra_deps, path_to_gyp, output):\n    \"\"\"Write CMake for the 'rules' in the target.\n    Args:\n      target_name: the name of the CMake target being generated.\n      actions: the Gyp 'actions' dict for this target.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "WriteRules",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def WriteRules(target_name, rules, extra_sources, extra_deps, path_to_gyp, output):\n    \"\"\"Write CMake for the 'rules' in the target.\n    Args:\n      target_name: the name of the CMake target being generated.\n      actions: the Gyp 'actions' dict for this target.\n      extra_sources: [(<cmake_src>, <src>)] to append with generated source files.\n      extra_deps: [<cmake_target>] to append with generated targets.\n      path_to_gyp: relative path from CMakeLists.txt being generated to\n          the Gyp file in which the target being generated is defined.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "WriteCopies",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def WriteCopies(target_name, copies, extra_deps, path_to_gyp, output):\n    \"\"\"Write CMake for the 'copies' in the target.\n    Args:\n      target_name: the name of the CMake target being generated.\n      actions: the Gyp 'actions' dict for this target.\n      extra_deps: [<cmake_target>] to append with generated targets.\n      path_to_gyp: relative path from CMakeLists.txt being generated to\n          the Gyp file in which the target being generated is defined.\n    \"\"\"\n    copy_name = target_name + \"__copies\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "CreateCMakeTargetBaseName",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def CreateCMakeTargetBaseName(qualified_target):\n    \"\"\"This is the name we would like the target to have.\"\"\"\n    _, gyp_target_name, gyp_target_toolset = gyp.common.ParseQualifiedTarget(\n        qualified_target\n    )\n    cmake_target_base_name = gyp_target_name\n    if gyp_target_toolset and gyp_target_toolset != \"target\":\n        cmake_target_base_name += \"_\" + gyp_target_toolset\n    return StringToCMakeTargetName(cmake_target_base_name)\ndef CreateCMakeTargetFullName(qualified_target):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "CreateCMakeTargetFullName",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def CreateCMakeTargetFullName(qualified_target):\n    \"\"\"An unambiguous name for the target.\"\"\"\n    gyp_file, gyp_target_name, gyp_target_toolset = gyp.common.ParseQualifiedTarget(\n        qualified_target\n    )\n    cmake_target_full_name = gyp_file + \":\" + gyp_target_name\n    if gyp_target_toolset and gyp_target_toolset != \"target\":\n        cmake_target_full_name += \"_\" + gyp_target_toolset\n    return StringToCMakeTargetName(cmake_target_full_name)\nclass CMakeNamer:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "WriteTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def WriteTarget(\n    namer,\n    qualified_target,\n    target_dicts,\n    build_dir,\n    config_to_use,\n    options,\n    generator_flags,\n    all_qualified_targets,\n    flavor,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "GenerateOutputForConfig",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def GenerateOutputForConfig(target_list, target_dicts, data, params, config_to_use):\n    options = params[\"options\"]\n    generator_flags = params[\"generator_flags\"]\n    flavor = gyp.common.GetFlavor(params)\n    # generator_dir: relative path from pwd to where make puts build files.\n    # Makes migrating from make to cmake easier, cmake doesn't put anything here.\n    # Each Gyp configuration creates a different CMakeLists.txt file\n    # to avoid incompatibilities between Gyp and CMake configurations.\n    generator_dir = os.path.relpath(options.generator_output or \".\")\n    # output_dir: relative path from generator_dir to the build directory.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "PerformBuild",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def PerformBuild(data, configurations, params):\n    options = params[\"options\"]\n    generator_flags = params[\"generator_flags\"]\n    # generator_dir: relative path from pwd to where make puts build files.\n    # Makes migrating from make to cmake easier, cmake doesn't put anything here.\n    generator_dir = os.path.relpath(options.generator_output or \".\")\n    # output_dir: relative path from generator_dir to the build directory.\n    output_dir = generator_flags.get(\"output_dir\", \"out\")\n    for config_name in configurations:\n        # build_dir: relative path from source root to our output files.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "CallGenerateOutputForConfig",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def CallGenerateOutputForConfig(arglist):\n    # Ignore the interrupt signal so that the parent process catches it and\n    # kills all multiprocessing children.\n    signal.signal(signal.SIGINT, signal.SIG_IGN)\n    target_list, target_dicts, data, params, config_name = arglist\n    GenerateOutputForConfig(target_list, target_dicts, data, params, config_name)\ndef GenerateOutput(target_list, target_dicts, data, params):\n    if user_config := params.get(\"generator_flags\", {}).get(\"config\", None):\n        GenerateOutputForConfig(target_list, target_dicts, data, params, user_config)\n    else:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    if user_config := params.get(\"generator_flags\", {}).get(\"config\", None):\n        GenerateOutputForConfig(target_list, target_dicts, data, params, user_config)\n    else:\n        config_names = target_dicts[target_list[0]][\"configurations\"]\n        if params[\"parallel\"]:\n            try:\n                pool = multiprocessing.Pool(len(config_names))\n                arglists = []\n                for config_name in config_names:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "_maketrans",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "_maketrans = str.maketrans\ngenerator_default_variables = {\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"STATIC_LIB_PREFIX\": \"lib\",\n    \"STATIC_LIB_SUFFIX\": \".a\",\n    \"SHARED_LIB_PREFIX\": \"lib\",\n    \"SHARED_LIB_SUFFIX\": \".so\",\n    \"SHARED_LIB_DIR\": \"${builddir}/lib.${TOOLSET}\",\n    \"LIB_DIR\": \"${obj}.${TOOLSET}\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "generator_default_variables = {\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"STATIC_LIB_PREFIX\": \"lib\",\n    \"STATIC_LIB_SUFFIX\": \".a\",\n    \"SHARED_LIB_PREFIX\": \"lib\",\n    \"SHARED_LIB_SUFFIX\": \".so\",\n    \"SHARED_LIB_DIR\": \"${builddir}/lib.${TOOLSET}\",\n    \"LIB_DIR\": \"${obj}.${TOOLSET}\",\n    \"INTERMEDIATE_DIR\": \"${obj}.${TOOLSET}/${TARGET}/geni\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "FULL_PATH_VARS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "FULL_PATH_VARS = (\"${CMAKE_CURRENT_LIST_DIR}\", \"${builddir}\", \"${obj}\")\ngenerator_supports_multiple_toolsets = True\ngenerator_wants_static_library_dependencies_adjusted = True\nCOMPILABLE_EXTENSIONS = {\n    \".c\": \"cc\",\n    \".cc\": \"cxx\",\n    \".cpp\": \"cxx\",\n    \".cxx\": \"cxx\",\n    \".s\": \"s\",  # cc\n    \".S\": \"s\",  # cc",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "generator_supports_multiple_toolsets",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "generator_supports_multiple_toolsets = True\ngenerator_wants_static_library_dependencies_adjusted = True\nCOMPILABLE_EXTENSIONS = {\n    \".c\": \"cc\",\n    \".cc\": \"cxx\",\n    \".cpp\": \"cxx\",\n    \".cxx\": \"cxx\",\n    \".s\": \"s\",  # cc\n    \".S\": \"s\",  # cc\n}",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "generator_wants_static_library_dependencies_adjusted",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "generator_wants_static_library_dependencies_adjusted = True\nCOMPILABLE_EXTENSIONS = {\n    \".c\": \"cc\",\n    \".cc\": \"cxx\",\n    \".cpp\": \"cxx\",\n    \".cxx\": \"cxx\",\n    \".s\": \"s\",  # cc\n    \".S\": \"s\",  # cc\n}\ndef RemovePrefix(a, prefix):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "COMPILABLE_EXTENSIONS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "COMPILABLE_EXTENSIONS = {\n    \".c\": \"cc\",\n    \".cc\": \"cxx\",\n    \".cpp\": \"cxx\",\n    \".cxx\": \"cxx\",\n    \".s\": \"s\",  # cc\n    \".S\": \"s\",  # cc\n}\ndef RemovePrefix(a, prefix):\n    \"\"\"Returns 'a' without 'prefix' if it starts with 'prefix'.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "cmake_target_type_from_gyp_target_type",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "peekOfCode": "cmake_target_type_from_gyp_target_type = {\n    \"executable\": CMakeTargetType(\"add_executable\", None, \"RUNTIME\"),\n    \"static_library\": CMakeTargetType(\"add_library\", \"STATIC\", \"ARCHIVE\"),\n    \"shared_library\": CMakeTargetType(\"add_library\", \"SHARED\", \"LIBRARY\"),\n    \"loadable_module\": CMakeTargetType(\"add_library\", \"MODULE\", \"LIBRARY\"),\n    \"none\": CMakeTargetType(\"add_custom_target\", \"SOURCES\", None),\n}\ndef StringToCMakeTargetName(a):\n    \"\"\"Converts the given string 'a' to a valid CMake target name.\n    All invalid characters are replaced by '_'.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.cmake",
        "documentation": {}
    },
    {
        "label": "IsMac",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "def IsMac(params):\n    return gyp.common.GetFlavor(params) == \"mac\"\ndef CalculateVariables(default_variables, params):\n    default_variables.setdefault(\"OS\", gyp.common.GetFlavor(params))\ndef AddCommandsForTarget(cwd, target, params, per_config_commands):\n    output_dir = params[\"generator_flags\"].get(\"output_dir\", \"out\")\n    for configuration_name, configuration in target[\"configurations\"].items():\n        if IsMac(params):\n            xcode_settings = gyp.xcode_emulation.XcodeSettings(target)\n            cflags = xcode_settings.GetCflags(configuration_name)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "CalculateVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "def CalculateVariables(default_variables, params):\n    default_variables.setdefault(\"OS\", gyp.common.GetFlavor(params))\ndef AddCommandsForTarget(cwd, target, params, per_config_commands):\n    output_dir = params[\"generator_flags\"].get(\"output_dir\", \"out\")\n    for configuration_name, configuration in target[\"configurations\"].items():\n        if IsMac(params):\n            xcode_settings = gyp.xcode_emulation.XcodeSettings(target)\n            cflags = xcode_settings.GetCflags(configuration_name)\n            cflags_c = xcode_settings.GetCflagsC(configuration_name)\n            cflags_cc = xcode_settings.GetCflagsCC(configuration_name)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "AddCommandsForTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "def AddCommandsForTarget(cwd, target, params, per_config_commands):\n    output_dir = params[\"generator_flags\"].get(\"output_dir\", \"out\")\n    for configuration_name, configuration in target[\"configurations\"].items():\n        if IsMac(params):\n            xcode_settings = gyp.xcode_emulation.XcodeSettings(target)\n            cflags = xcode_settings.GetCflags(configuration_name)\n            cflags_c = xcode_settings.GetCflagsC(configuration_name)\n            cflags_cc = xcode_settings.GetCflagsCC(configuration_name)\n        else:\n            cflags = configuration.get(\"cflags\", [])",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    per_config_commands = {}\n    for qualified_target, target in target_dicts.items():\n        build_file, _target_name, _toolset = gyp.common.ParseQualifiedTarget(\n            qualified_target\n        )\n        if IsMac(params):\n            settings = data[build_file]\n            gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(settings, target)\n        cwd = os.path.dirname(build_file)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "PerformBuild",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "def PerformBuild(data, configurations, params):\n    pass",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "generator_additional_non_configuration_keys",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "generator_additional_non_configuration_keys = []\ngenerator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ngenerator_supports_multiple_toolsets = True\ngenerator_wants_sorted_dependencies = False\n# Lifted from make.py.  The actual values don't matter much.\ngenerator_default_variables = {\n    \"CONFIGURATION_NAME\": \"$(BUILDTYPE)\",\n    \"EXECUTABLE_PREFIX\": \"\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "generator_additional_path_sections",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "generator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ngenerator_supports_multiple_toolsets = True\ngenerator_wants_sorted_dependencies = False\n# Lifted from make.py.  The actual values don't matter much.\ngenerator_default_variables = {\n    \"CONFIGURATION_NAME\": \"$(BUILDTYPE)\",\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "generator_extra_sources_for_rules",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "generator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ngenerator_supports_multiple_toolsets = True\ngenerator_wants_sorted_dependencies = False\n# Lifted from make.py.  The actual values don't matter much.\ngenerator_default_variables = {\n    \"CONFIGURATION_NAME\": \"$(BUILDTYPE)\",\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"INTERMEDIATE_DIR\": \"$(obj).$(TOOLSET)/$(TARGET)/geni\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "generator_filelist_paths",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "generator_filelist_paths = None\ngenerator_supports_multiple_toolsets = True\ngenerator_wants_sorted_dependencies = False\n# Lifted from make.py.  The actual values don't matter much.\ngenerator_default_variables = {\n    \"CONFIGURATION_NAME\": \"$(BUILDTYPE)\",\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"INTERMEDIATE_DIR\": \"$(obj).$(TOOLSET)/$(TARGET)/geni\",\n    \"PRODUCT_DIR\": \"$(builddir)\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "generator_supports_multiple_toolsets",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "generator_supports_multiple_toolsets = True\ngenerator_wants_sorted_dependencies = False\n# Lifted from make.py.  The actual values don't matter much.\ngenerator_default_variables = {\n    \"CONFIGURATION_NAME\": \"$(BUILDTYPE)\",\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"INTERMEDIATE_DIR\": \"$(obj).$(TOOLSET)/$(TARGET)/geni\",\n    \"PRODUCT_DIR\": \"$(builddir)\",\n    \"RULE_INPUT_DIRNAME\": \"%(INPUT_DIRNAME)s\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "generator_wants_sorted_dependencies",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "generator_wants_sorted_dependencies = False\n# Lifted from make.py.  The actual values don't matter much.\ngenerator_default_variables = {\n    \"CONFIGURATION_NAME\": \"$(BUILDTYPE)\",\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"INTERMEDIATE_DIR\": \"$(obj).$(TOOLSET)/$(TARGET)/geni\",\n    \"PRODUCT_DIR\": \"$(builddir)\",\n    \"RULE_INPUT_DIRNAME\": \"%(INPUT_DIRNAME)s\",\n    \"RULE_INPUT_EXT\": \"$(suffix $<)\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "peekOfCode": "generator_default_variables = {\n    \"CONFIGURATION_NAME\": \"$(BUILDTYPE)\",\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"INTERMEDIATE_DIR\": \"$(obj).$(TOOLSET)/$(TARGET)/geni\",\n    \"PRODUCT_DIR\": \"$(builddir)\",\n    \"RULE_INPUT_DIRNAME\": \"%(INPUT_DIRNAME)s\",\n    \"RULE_INPUT_EXT\": \"$(suffix $<)\",\n    \"RULE_INPUT_NAME\": \"$(notdir $<)\",\n    \"RULE_INPUT_PATH\": \"$(abspath $<)\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.compile_commands_json",
        "documentation": {}
    },
    {
        "label": "CalculateVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "peekOfCode": "def CalculateVariables(default_variables, params):\n    generator_flags = params.get(\"generator_flags\", {})\n    for key, val in generator_flags.items():\n        default_variables.setdefault(key, val)\n    default_variables.setdefault(\"OS\", gyp.common.GetFlavor(params))\n    flavor = gyp.common.GetFlavor(params)\n    if flavor == \"win\":\n        gyp.msvs_emulation.CalculateCommonVariables(default_variables, params)\ndef CalculateGeneratorInputInfo(params):\n    \"\"\"Calculate the generator specific info that gets fed to input (called by",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "documentation": {}
    },
    {
        "label": "CalculateGeneratorInputInfo",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "peekOfCode": "def CalculateGeneratorInputInfo(params):\n    \"\"\"Calculate the generator specific info that gets fed to input (called by\n    gyp).\"\"\"\n    generator_flags = params.get(\"generator_flags\", {})\n    if generator_flags.get(\"adjust_static_libraries\", False):\n        global generator_wants_static_library_dependencies_adjusted\n        generator_wants_static_library_dependencies_adjusted = True\n    toplevel = params[\"options\"].toplevel_dir\n    generator_dir = os.path.relpath(params[\"options\"].generator_output or \".\")\n    # output_dir: relative path from generator_dir to the build directory.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    # Map of target -> list of targets it depends on.\n    edges = {}\n    # Queue of targets to visit.\n    targets_to_visit = target_list[:]\n    while len(targets_to_visit) > 0:\n        target = targets_to_visit.pop()\n        if target in edges:\n            continue\n        edges[target] = []",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "documentation": {}
    },
    {
        "label": "generator_supports_multiple_toolsets",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "peekOfCode": "generator_supports_multiple_toolsets = True\ngenerator_wants_static_library_dependencies_adjusted = False\ngenerator_filelist_paths = {}\ngenerator_default_variables = {}\nfor dirname in [\n    \"INTERMEDIATE_DIR\",\n    \"SHARED_INTERMEDIATE_DIR\",\n    \"PRODUCT_DIR\",\n    \"LIB_DIR\",\n    \"SHARED_LIB_DIR\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "documentation": {}
    },
    {
        "label": "generator_wants_static_library_dependencies_adjusted",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "peekOfCode": "generator_wants_static_library_dependencies_adjusted = False\ngenerator_filelist_paths = {}\ngenerator_default_variables = {}\nfor dirname in [\n    \"INTERMEDIATE_DIR\",\n    \"SHARED_INTERMEDIATE_DIR\",\n    \"PRODUCT_DIR\",\n    \"LIB_DIR\",\n    \"SHARED_LIB_DIR\",\n]:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "documentation": {}
    },
    {
        "label": "generator_filelist_paths",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "peekOfCode": "generator_filelist_paths = {}\ngenerator_default_variables = {}\nfor dirname in [\n    \"INTERMEDIATE_DIR\",\n    \"SHARED_INTERMEDIATE_DIR\",\n    \"PRODUCT_DIR\",\n    \"LIB_DIR\",\n    \"SHARED_LIB_DIR\",\n]:\n    # Some gyp steps fail if these are empty(!).",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "peekOfCode": "generator_default_variables = {}\nfor dirname in [\n    \"INTERMEDIATE_DIR\",\n    \"SHARED_INTERMEDIATE_DIR\",\n    \"PRODUCT_DIR\",\n    \"LIB_DIR\",\n    \"SHARED_LIB_DIR\",\n]:\n    # Some gyp steps fail if these are empty(!).\n    generator_default_variables[dirname] = \"dir\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.dump_dependency_json",
        "documentation": {}
    },
    {
        "label": "CalculateVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def CalculateVariables(default_variables, params):\n    generator_flags = params.get(\"generator_flags\", {})\n    for key, val in generator_flags.items():\n        default_variables.setdefault(key, val)\n    flavor = gyp.common.GetFlavor(params)\n    default_variables.setdefault(\"OS\", flavor)\n    if flavor == \"win\":\n        gyp.msvs_emulation.CalculateCommonVariables(default_variables, params)\ndef CalculateGeneratorInputInfo(params):\n    \"\"\"Calculate the generator specific info that gets fed to input (called by",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "CalculateGeneratorInputInfo",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def CalculateGeneratorInputInfo(params):\n    \"\"\"Calculate the generator specific info that gets fed to input (called by\n    gyp).\"\"\"\n    generator_flags = params.get(\"generator_flags\", {})\n    if generator_flags.get(\"adjust_static_libraries\", False):\n        global generator_wants_static_library_dependencies_adjusted\n        generator_wants_static_library_dependencies_adjusted = True\ndef GetAllIncludeDirectories(\n    target_list,\n    target_dicts,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "GetAllIncludeDirectories",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def GetAllIncludeDirectories(\n    target_list,\n    target_dicts,\n    shared_intermediate_dirs,\n    config_name,\n    params,\n    compiler_path,\n):\n    \"\"\"Calculate the set of include directories to be used.\n    Returns:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "GetCompilerPath",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def GetCompilerPath(target_list, data, options):\n    \"\"\"Determine a command that can be used to invoke the compiler.\n    Returns:\n      If this is a gyp project that has explicit make settings, try to determine\n      the compiler from that.  Otherwise, see if a compiler was specified via the\n      CC_target environment variable.\n    \"\"\"\n    # First, see if the compiler is configured in make's settings.\n    build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])\n    make_global_settings_dict = data[build_file].get(\"make_global_settings\", {})",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "GetAllDefines",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def GetAllDefines(target_list, target_dicts, data, config_name, params, compiler_path):\n    \"\"\"Calculate the defines for a project.\n    Returns:\n      A dict that includes explicit defines declared in gyp files along with all\n      of the default defines that the compiler uses.\n    \"\"\"\n    # Get defines declared in the gyp files.\n    all_defines = {}\n    flavor = gyp.common.GetFlavor(params)\n    if flavor == \"win\":",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "WriteIncludePaths",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def WriteIncludePaths(out, eclipse_langs, include_dirs):\n    \"\"\"Write the includes section of a CDT settings export file.\"\"\"\n    out.write(\n        '  <section name=\"org.eclipse.cdt.internal.ui.wizards.'\n        'settingswizards.IncludePaths\">\\n'\n    )\n    out.write('    <language name=\"holder for library settings\"></language>\\n')\n    for lang in eclipse_langs:\n        out.write('    <language name=\"%s\">\\n' % lang)\n        for include_dir in include_dirs:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "WriteMacros",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def WriteMacros(out, eclipse_langs, defines):\n    \"\"\"Write the macros section of a CDT settings export file.\"\"\"\n    out.write(\n        '  <section name=\"org.eclipse.cdt.internal.ui.wizards.'\n        'settingswizards.Macros\">\\n'\n    )\n    out.write('    <language name=\"holder for library settings\"></language>\\n')\n    for lang in eclipse_langs:\n        out.write('    <language name=\"%s\">\\n' % lang)\n        for key in sorted(defines):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "GenerateOutputForConfig",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def GenerateOutputForConfig(target_list, target_dicts, data, params, config_name):\n    options = params[\"options\"]\n    generator_flags = params.get(\"generator_flags\", {})\n    # build_dir: relative path from source root to our output files.\n    # e.g. \"out/Debug\"\n    build_dir = os.path.join(generator_flags.get(\"output_dir\", \"out\"), config_name)\n    toplevel_build = os.path.join(options.toplevel_dir, build_dir)\n    # Ninja uses out/Debug/gen while make uses out/Debug/obj/gen as the\n    # SHARED_INTERMEDIATE_DIR. Include both possible locations.\n    shared_intermediate_dirs = [",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "GenerateCdtSettingsFile",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def GenerateCdtSettingsFile(\n    target_list,\n    target_dicts,\n    data,\n    params,\n    config_name,\n    out_name,\n    options,\n    shared_intermediate_dirs,\n):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "GenerateClasspathFile",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def GenerateClasspathFile(\n    target_list, target_dicts, toplevel_dir, toplevel_build, out_name\n):\n    \"\"\"Generates a classpath file suitable for symbol navigation and code\n    completion of Java code (such as in Android projects) by finding all\n    .java and .jar files used as action inputs.\"\"\"\n    gyp.common.EnsureDirExists(out_name)\n    result = ET.Element(\"classpath\")\n    def AddElements(kind, paths):\n        # First, we need to normalize the paths so they are all relative to the",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "GetJavaJars",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def GetJavaJars(target_list, target_dicts, toplevel_dir):\n    \"\"\"Generates a sequence of all .jars used as inputs.\"\"\"\n    for target_name in target_list:\n        target = target_dicts[target_name]\n        for action in target.get(\"actions\", []):\n            for input_ in action[\"inputs\"]:\n                if os.path.splitext(input_)[1] == \".jar\" and not input_.startswith(\"$\"):\n                    if os.path.isabs(input_):\n                        yield input_\n                    else:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "GetJavaSourceDirs",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def GetJavaSourceDirs(target_list, target_dicts, toplevel_dir):\n    \"\"\"Generates a sequence of all likely java package root directories.\"\"\"\n    for target_name in target_list:\n        target = target_dicts[target_name]\n        for action in target.get(\"actions\", []):\n            for input_ in action[\"inputs\"]:\n                if os.path.splitext(input_)[1] == \".java\" and not input_.startswith(\n                    \"$\"\n                ):\n                    dir_ = os.path.dirname(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    \"\"\"Generate an XML settings file that can be imported into a CDT project.\"\"\"\n    if params[\"options\"].generator_output:\n        raise NotImplementedError(\"--generator_output not implemented for eclipse\")\n    if user_config := params.get(\"generator_flags\", {}).get(\"config\", None):\n        GenerateOutputForConfig(target_list, target_dicts, data, params, user_config)\n    else:\n        config_names = target_dicts[target_list[0]][\"configurations\"]\n        for config_name in config_names:\n            GenerateOutputForConfig(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "generator_wants_static_library_dependencies_adjusted",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "generator_wants_static_library_dependencies_adjusted = False\ngenerator_default_variables = {}\nfor dirname in [\"INTERMEDIATE_DIR\", \"PRODUCT_DIR\", \"LIB_DIR\", \"SHARED_LIB_DIR\"]:\n    # Some gyp steps fail if these are empty(!), so we convert them to variables\n    generator_default_variables[dirname] = \"$\" + dirname\nfor unused in [\n    \"RULE_INPUT_PATH\",\n    \"RULE_INPUT_ROOT\",\n    \"RULE_INPUT_NAME\",\n    \"RULE_INPUT_DIRNAME\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "generator_default_variables = {}\nfor dirname in [\"INTERMEDIATE_DIR\", \"PRODUCT_DIR\", \"LIB_DIR\", \"SHARED_LIB_DIR\"]:\n    # Some gyp steps fail if these are empty(!), so we convert them to variables\n    generator_default_variables[dirname] = \"$\" + dirname\nfor unused in [\n    \"RULE_INPUT_PATH\",\n    \"RULE_INPUT_ROOT\",\n    \"RULE_INPUT_NAME\",\n    \"RULE_INPUT_DIRNAME\",\n    \"RULE_INPUT_EXT\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "generator_default_variables[\"SHARED_INTERMEDIATE_DIR\"]",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "peekOfCode": "generator_default_variables[\"SHARED_INTERMEDIATE_DIR\"] = \"$SHARED_INTERMEDIATE_DIR\"\ndef CalculateVariables(default_variables, params):\n    generator_flags = params.get(\"generator_flags\", {})\n    for key, val in generator_flags.items():\n        default_variables.setdefault(key, val)\n    flavor = gyp.common.GetFlavor(params)\n    default_variables.setdefault(\"OS\", flavor)\n    if flavor == \"win\":\n        gyp.msvs_emulation.CalculateCommonVariables(default_variables, params)\ndef CalculateGeneratorInputInfo(params):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.eclipse",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    output_files = {}\n    for qualified_target in target_list:\n        [input_file, _target] = gyp.common.ParseQualifiedTarget(qualified_target)[0:2]\n        if input_file[-4:] != \".gyp\":\n            continue\n        input_file_stem = input_file[:-4]\n        output_file = input_file_stem + params[\"options\"].suffix + \".gypd\"\n        output_files[output_file] = output_files.get(output_file, input_file)\n    for output_file, input_file in output_files.items():",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "documentation": {}
    },
    {
        "label": "_generator_identity_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "peekOfCode": "_generator_identity_variables = [\n    \"CONFIGURATION_NAME\",\n    \"EXECUTABLE_PREFIX\",\n    \"EXECUTABLE_SUFFIX\",\n    \"INTERMEDIATE_DIR\",\n    \"LIB_DIR\",\n    \"PRODUCT_DIR\",\n    \"RULE_INPUT_ROOT\",\n    \"RULE_INPUT_DIRNAME\",\n    \"RULE_INPUT_EXT\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "peekOfCode": "generator_default_variables = {}\n# gypd supports multiple toolsets\ngenerator_supports_multiple_toolsets = True\n# TODO(mark): This always uses <, which isn't right.  The input module should\n# notify the generator to tell it which phase it is operating in, and this\n# module should use < for the early phase and then switch to > for the late\n# phase.  Bonus points for carrying @ back into the output too.\nfor v in _generator_identity_variables:\n    generator_default_variables[v] = \"<(%s)\" % v\ndef GenerateOutput(target_list, target_dicts, data, params):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "documentation": {}
    },
    {
        "label": "generator_supports_multiple_toolsets",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "peekOfCode": "generator_supports_multiple_toolsets = True\n# TODO(mark): This always uses <, which isn't right.  The input module should\n# notify the generator to tell it which phase it is operating in, and this\n# module should use < for the early phase and then switch to > for the late\n# phase.  Bonus points for carrying @ back into the output too.\nfor v in _generator_identity_variables:\n    generator_default_variables[v] = \"<(%s)\" % v\ndef GenerateOutput(target_list, target_dicts, data, params):\n    output_files = {}\n    for qualified_target in target_list:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypd",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypsh",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypsh",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    locals = {\n        \"target_list\": target_list,\n        \"target_dicts\": target_dicts,\n        \"data\": data,\n    }\n    # Use a banner that looks like the stock Python one and like what\n    # code.interact uses by default, but tack on something to indicate what\n    # locals are available, and identify gypsh.\n    banner = (",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypsh",
        "documentation": {}
    },
    {
        "label": "_generator_identity_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypsh",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypsh",
        "peekOfCode": "_generator_identity_variables = [\n    \"EXECUTABLE_PREFIX\",\n    \"EXECUTABLE_SUFFIX\",\n    \"INTERMEDIATE_DIR\",\n    \"PRODUCT_DIR\",\n    \"RULE_INPUT_ROOT\",\n    \"RULE_INPUT_DIRNAME\",\n    \"RULE_INPUT_EXT\",\n    \"RULE_INPUT_NAME\",\n    \"RULE_INPUT_PATH\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypsh",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypsh",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypsh",
        "peekOfCode": "generator_default_variables = {}\nfor v in _generator_identity_variables:\n    generator_default_variables[v] = \"<(%s)\" % v\ndef GenerateOutput(target_list, target_dicts, data, params):\n    locals = {\n        \"target_list\": target_list,\n        \"target_dicts\": target_dicts,\n        \"data\": data,\n    }\n    # Use a banner that looks like the stock Python one and like what",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.gypsh",
        "documentation": {}
    },
    {
        "label": "MakefileWriter",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "class MakefileWriter:\n    \"\"\"MakefileWriter packages up the writing of one target-specific foobar.mk.\n    Its only real entry point is Write(), and is mostly used for namespacing.\n    \"\"\"\n    def __init__(self, generator_flags, flavor):\n        self.generator_flags = generator_flags\n        self.flavor = flavor\n        self.suffix_rules_srcdir = {}\n        self.suffix_rules_objdir1 = {}\n        self.suffix_rules_objdir2 = {}",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "CalculateVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"\n    flavor = gyp.common.GetFlavor(params)\n    if flavor == \"mac\":\n        default_variables.setdefault(\"OS\", \"mac\")\n        default_variables.setdefault(\"SHARED_LIB_SUFFIX\", \".dylib\")\n        default_variables.setdefault(\n            \"SHARED_LIB_DIR\", generator_default_variables[\"PRODUCT_DIR\"]\n        )\n        default_variables.setdefault(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "CalculateGeneratorInputInfo",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def CalculateGeneratorInputInfo(params):\n    \"\"\"Calculate the generator specific info that gets fed to input (called by\n    gyp).\"\"\"\n    generator_flags = params.get(\"generator_flags\", {})\n    android_ndk_version = generator_flags.get(\"android_ndk_version\", None)\n    # Android NDK requires a strict link order.\n    if android_ndk_version:\n        global generator_wants_sorted_dependencies\n        generator_wants_sorted_dependencies = True\n    output_dir = params[\"options\"].generator_output or params[\"options\"].toplevel_dir",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "WriteRootHeaderSuffixRules",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def WriteRootHeaderSuffixRules(writer):\n    extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)\n    writer.write(\"# Suffix rules, putting all outputs into $(obj).\\n\")\n    for ext in extensions:\n        writer.write(\"$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\\n\" % ext)\n        writer.write(\"\\t@$(call do_cmd,%s,1)\\n\" % COMPILABLE_EXTENSIONS[ext])\n    writer.write(\"\\n# Try building from generated source, too.\\n\")\n    for ext in extensions:\n        writer.write(\n            \"$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\\n\" % ext",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "Compilable",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def Compilable(filename):\n    \"\"\"Return true if the file is compilable (should be in OBJS).\"\"\"\n    return any(res for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS))\ndef Linkable(filename):\n    \"\"\"Return true if the file is linkable (should be on the link line).\"\"\"\n    return filename.endswith(\".o\")\ndef Target(filename):\n    \"\"\"Translate a compilable filename to its .o target.\"\"\"\n    return os.path.splitext(filename)[0] + \".o\"\ndef EscapeShellArgument(s):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "Linkable",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def Linkable(filename):\n    \"\"\"Return true if the file is linkable (should be on the link line).\"\"\"\n    return filename.endswith(\".o\")\ndef Target(filename):\n    \"\"\"Translate a compilable filename to its .o target.\"\"\"\n    return os.path.splitext(filename)[0] + \".o\"\ndef EscapeShellArgument(s):\n    \"\"\"Quotes an argument so that it will be interpreted literally by a POSIX\n    shell. Taken from\n    http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "Target",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def Target(filename):\n    \"\"\"Translate a compilable filename to its .o target.\"\"\"\n    return os.path.splitext(filename)[0] + \".o\"\ndef EscapeShellArgument(s):\n    \"\"\"Quotes an argument so that it will be interpreted literally by a POSIX\n    shell. Taken from\n    http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python\n    \"\"\"\n    return \"'\" + s.replace(\"'\", \"'\\\\''\") + \"'\"\ndef EscapeMakeVariableExpansion(s):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "EscapeShellArgument",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def EscapeShellArgument(s):\n    \"\"\"Quotes an argument so that it will be interpreted literally by a POSIX\n    shell. Taken from\n    http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python\n    \"\"\"\n    return \"'\" + s.replace(\"'\", \"'\\\\''\") + \"'\"\ndef EscapeMakeVariableExpansion(s):\n    \"\"\"Make has its own variable expansion syntax using $. We must escape it for\n    string to be interpreted literally.\"\"\"\n    return s.replace(\"$\", \"$$\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "EscapeMakeVariableExpansion",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def EscapeMakeVariableExpansion(s):\n    \"\"\"Make has its own variable expansion syntax using $. We must escape it for\n    string to be interpreted literally.\"\"\"\n    return s.replace(\"$\", \"$$\")\ndef EscapeCppDefine(s):\n    \"\"\"Escapes a CPP define so that it will reach the compiler unaltered.\"\"\"\n    s = EscapeShellArgument(s)\n    s = EscapeMakeVariableExpansion(s)\n    # '#' characters must be escaped even embedded in a string, else Make will\n    # treat it as the start of a comment.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "EscapeCppDefine",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def EscapeCppDefine(s):\n    \"\"\"Escapes a CPP define so that it will reach the compiler unaltered.\"\"\"\n    s = EscapeShellArgument(s)\n    s = EscapeMakeVariableExpansion(s)\n    # '#' characters must be escaped even embedded in a string, else Make will\n    # treat it as the start of a comment.\n    return s.replace(\"#\", r\"\\#\")\ndef QuoteIfNecessary(string):\n    \"\"\"TODO: Should this ideally be replaced with one or more of the above\n    functions?\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "QuoteIfNecessary",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def QuoteIfNecessary(string):\n    \"\"\"TODO: Should this ideally be replaced with one or more of the above\n    functions?\"\"\"\n    if '\"' in string:\n        string = '\"' + string.replace('\"', '\\\\\"') + '\"'\n    return string\ndef replace_sep(string):\n    if sys.platform == \"win32\":\n        string = string.replace(\"\\\\\\\\\", \"/\").replace(\"\\\\\", \"/\")\n    return string",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "replace_sep",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def replace_sep(string):\n    if sys.platform == \"win32\":\n        string = string.replace(\"\\\\\\\\\", \"/\").replace(\"\\\\\", \"/\")\n    return string\ndef StringToMakefileVariable(string):\n    \"\"\"Convert a string to a value that is acceptable as a make variable name.\"\"\"\n    return re.sub(\"[^a-zA-Z0-9_]\", \"_\", string)\nsrcdir_prefix = \"\"\ndef Sourceify(path):\n    \"\"\"Convert a path to its source directory form.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "StringToMakefileVariable",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def StringToMakefileVariable(string):\n    \"\"\"Convert a string to a value that is acceptable as a make variable name.\"\"\"\n    return re.sub(\"[^a-zA-Z0-9_]\", \"_\", string)\nsrcdir_prefix = \"\"\ndef Sourceify(path):\n    \"\"\"Convert a path to its source directory form.\"\"\"\n    if \"$(\" in path:\n        return path\n    if os.path.isabs(path):\n        return path",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "Sourceify",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def Sourceify(path):\n    \"\"\"Convert a path to its source directory form.\"\"\"\n    if \"$(\" in path:\n        return path\n    if os.path.isabs(path):\n        return path\n    return srcdir_prefix + path\ndef QuoteSpaces(s, quote=r\"\\ \"):\n    return s.replace(\" \", quote)\ndef SourceifyAndQuoteSpaces(path):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "QuoteSpaces",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def QuoteSpaces(s, quote=r\"\\ \"):\n    return s.replace(\" \", quote)\ndef SourceifyAndQuoteSpaces(path):\n    \"\"\"Convert a path to its source directory form and quote spaces.\"\"\"\n    return QuoteSpaces(Sourceify(path))\n# Map from qualified target to path to output.\ntarget_outputs = {}\n# Map from qualified target to any linkable output.  A subset\n# of target_outputs.  E.g. when mybinary depends on liba, we want to\n# include liba in the linker line; when otherbinary depends on",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "SourceifyAndQuoteSpaces",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def SourceifyAndQuoteSpaces(path):\n    \"\"\"Convert a path to its source directory form and quote spaces.\"\"\"\n    return QuoteSpaces(Sourceify(path))\n# Map from qualified target to path to output.\ntarget_outputs = {}\n# Map from qualified target to any linkable output.  A subset\n# of target_outputs.  E.g. when mybinary depends on liba, we want to\n# include liba in the linker line; when otherbinary depends on\n# mybinary, we just want to build mybinary first.\ntarget_link_deps = {}",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "WriteAutoRegenerationRule",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files):\n    \"\"\"Write the target to regenerate the Makefile.\"\"\"\n    options = params[\"options\"]\n    build_files_args = [\n        gyp.common.RelativePath(filename, options.toplevel_dir)\n        for filename in params[\"build_files_arg\"]\n    ]\n    gyp_binary = gyp.common.FixIfRelativePath(\n        params[\"gyp_binary\"], options.toplevel_dir\n    )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "PerformBuild",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def PerformBuild(data, configurations, params):\n    options = params[\"options\"]\n    for config in configurations:\n        arguments = [\"make\"]\n        if options.toplevel_dir and options.toplevel_dir != \".\":\n            arguments += \"-C\", options.toplevel_dir\n        arguments.append(\"BUILDTYPE=\" + config)\n        print(f\"Building [{config}]: {arguments}\")\n        subprocess.check_call(arguments)\ndef GenerateOutput(target_list, target_dicts, data, params):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    options = params[\"options\"]\n    flavor = gyp.common.GetFlavor(params)\n    generator_flags = params.get(\"generator_flags\", {})\n    builddir_name = generator_flags.get(\"output_dir\", \"out\")\n    android_ndk_version = generator_flags.get(\"android_ndk_version\", None)\n    default_target = generator_flags.get(\"default_target\", \"all\")\n    def CalculateMakefilePath(build_file, base_name):\n        \"\"\"Determine where to write a Makefile for a given gyp file.\"\"\"\n        # Paths in gyp files are relative to the .gyp file, but we want",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "generator_default_variables = {\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"STATIC_LIB_PREFIX\": \"lib\",\n    \"SHARED_LIB_PREFIX\": \"lib\",\n    \"STATIC_LIB_SUFFIX\": \".a\",\n    \"INTERMEDIATE_DIR\": \"$(obj).$(TOOLSET)/$(TARGET)/geni\",\n    \"SHARED_INTERMEDIATE_DIR\": \"$(obj)/gen\",\n    \"PRODUCT_DIR\": \"$(builddir)\",\n    \"RULE_INPUT_ROOT\": \"%(INPUT_ROOT)s\",  # This gets expanded by Python.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "generator_supports_multiple_toolsets",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "generator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\n# Request sorted dependencies in the order from dependents to dependencies.\ngenerator_wants_sorted_dependencies = False\n# Placates pylint.\ngenerator_additional_non_configuration_keys = []\ngenerator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ndef CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "generator_wants_sorted_dependencies",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "generator_wants_sorted_dependencies = False\n# Placates pylint.\ngenerator_additional_non_configuration_keys = []\ngenerator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ndef CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"\n    flavor = gyp.common.GetFlavor(params)\n    if flavor == \"mac\":",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "generator_additional_non_configuration_keys",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "generator_additional_non_configuration_keys = []\ngenerator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ndef CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"\n    flavor = gyp.common.GetFlavor(params)\n    if flavor == \"mac\":\n        default_variables.setdefault(\"OS\", \"mac\")\n        default_variables.setdefault(\"SHARED_LIB_SUFFIX\", \".dylib\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "generator_additional_path_sections",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "generator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ndef CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"\n    flavor = gyp.common.GetFlavor(params)\n    if flavor == \"mac\":\n        default_variables.setdefault(\"OS\", \"mac\")\n        default_variables.setdefault(\"SHARED_LIB_SUFFIX\", \".dylib\")\n        default_variables.setdefault(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "generator_extra_sources_for_rules",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "generator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ndef CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"\n    flavor = gyp.common.GetFlavor(params)\n    if flavor == \"mac\":\n        default_variables.setdefault(\"OS\", \"mac\")\n        default_variables.setdefault(\"SHARED_LIB_SUFFIX\", \".dylib\")\n        default_variables.setdefault(\n            \"SHARED_LIB_DIR\", generator_default_variables[\"PRODUCT_DIR\"]",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "generator_filelist_paths",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "generator_filelist_paths = None\ndef CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"\n    flavor = gyp.common.GetFlavor(params)\n    if flavor == \"mac\":\n        default_variables.setdefault(\"OS\", \"mac\")\n        default_variables.setdefault(\"SHARED_LIB_SUFFIX\", \".dylib\")\n        default_variables.setdefault(\n            \"SHARED_LIB_DIR\", generator_default_variables[\"PRODUCT_DIR\"]\n        )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "SPACE_REPLACEMENT",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "SPACE_REPLACEMENT = \"?\"\nLINK_COMMANDS_LINUX = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Due to circular dependencies between libraries :(, we wrap the\n# special \"figure out circular dependencies\" flags around the entire\n# input list during linking.\nquiet_cmd_link = LINK($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "LINK_COMMANDS_LINUX",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "LINK_COMMANDS_LINUX = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Due to circular dependencies between libraries :(, we wrap the\n# special \"figure out circular dependencies\" flags around the entire\n# input list during linking.\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) -o $@ $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,--start-group $(LD_INPUTS) $(LIBS) -Wl,--end-group",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Due to circular dependencies between libraries :(, we wrap the\n# special \"figure out circular dependencies\" flags around the entire\n# input list during linking.\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) -o $@ $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,--start-group $(LD_INPUTS) $(LIBS) -Wl,--end-group\n# Note: this does not handle spaces in paths",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Due to circular dependencies between libraries :(, we wrap the\n# special \"figure out circular dependencies\" flags around the entire\n# input list during linking.\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) -o $@ $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,--start-group $(LD_INPUTS) $(LIBS) -Wl,--end-group\n# Note: this does not handle spaces in paths\ndefine xargs",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink_thin",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Due to circular dependencies between libraries :(, we wrap the\n# special \"figure out circular dependencies\" flags around the entire\n# input list during linking.\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) -o $@ $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,--start-group $(LD_INPUTS) $(LIBS) -Wl,--end-group\n# Note: this does not handle spaces in paths\ndefine xargs\n  $(1) $(word 1,$(2))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink_thin",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Due to circular dependencies between libraries :(, we wrap the\n# special \"figure out circular dependencies\" flags around the entire\n# input list during linking.\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) -o $@ $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,--start-group $(LD_INPUTS) $(LIBS) -Wl,--end-group\n# Note: this does not handle spaces in paths\ndefine xargs\n  $(1) $(word 1,$(2))\n$(if $(word 2,$(2)),$(call xargs,$(1),$(wordlist 2,$(words $(2)),$(2))))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) -o $@ $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,--start-group $(LD_INPUTS) $(LIBS) -Wl,--end-group\n# Note: this does not handle spaces in paths\ndefine xargs\n  $(1) $(word 1,$(2))\n$(if $(word 2,$(2)),$(call xargs,$(1),$(wordlist 2,$(words $(2)),$(2))))\nendef\ndefine write-to-file\n  @: >$(1)\n$(call xargs,@printf \"%s\\\\n\" >>$(1),$(2))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_link = $(LINK.$(TOOLSET)) -o $@ $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,--start-group $(LD_INPUTS) $(LIBS) -Wl,--end-group\n# Note: this does not handle spaces in paths\ndefine xargs\n  $(1) $(word 1,$(2))\n$(if $(word 2,$(2)),$(call xargs,$(1),$(wordlist 2,$(words $(2)),$(2))))\nendef\ndefine write-to-file\n  @: >$(1)\n$(call xargs,@printf \"%s\\\\n\" >>$(1),$(2))\nendef",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -o $@ -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -o $@ -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_MAC = \"\"\"\\\nquiet_cmd_alink = LIBTOOL-STATIC $@\ncmd_alink = rm -f $@ && %(python)s gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %%.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink = $(LINK.$(TOOLSET)) -o $@ -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -o $@ -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_MAC = \"\"\"\\\nquiet_cmd_alink = LIBTOOL-STATIC $@\ncmd_alink = rm -f $@ && %(python)s gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %%.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -o $@ -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_MAC = \"\"\"\\\nquiet_cmd_alink = LIBTOOL-STATIC $@\ncmd_alink = rm -f $@ && %(python)s gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %%.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink_module = $(LINK.$(TOOLSET)) -o $@ -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_MAC = \"\"\"\\\nquiet_cmd_alink = LIBTOOL-STATIC $@\ncmd_alink = rm -f $@ && %(python)s gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %%.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "LINK_COMMANDS_MAC",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "LINK_COMMANDS_MAC = \"\"\"\\\nquiet_cmd_alink = LIBTOOL-STATIC $@\ncmd_alink = rm -f $@ && %(python)s gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %%.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\" % {\"python\": sys.executable}  # noqa: E501",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink = LIBTOOL-STATIC $@\ncmd_alink = rm -f $@ && %(python)s gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %%.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\nLINK_COMMANDS_ANDROID = \"\"\"\\",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink = rm -f $@ && %(python)s gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %%.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\nLINK_COMMANDS_ANDROID = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\nLINK_COMMANDS_ANDROID = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\nLINK_COMMANDS_ANDROID = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\nLINK_COMMANDS_ANDROID = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o \"$@\" $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\nLINK_COMMANDS_ANDROID = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Note: this does not handle spaces in paths",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\nLINK_COMMANDS_ANDROID = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Note: this does not handle spaces in paths\ndefine xargs",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\nLINK_COMMANDS_ANDROID = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Note: this does not handle spaces in paths\ndefine xargs\n  $(1) $(word 1,$(2))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "LINK_COMMANDS_ANDROID",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "LINK_COMMANDS_ANDROID = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Note: this does not handle spaces in paths\ndefine xargs\n  $(1) $(word 1,$(2))\n$(if $(word 2,$(2)),$(call xargs,$(1),$(wordlist 2,$(words $(2)),$(2))))\nendef",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Note: this does not handle spaces in paths\ndefine xargs\n  $(1) $(word 1,$(2))\n$(if $(word 2,$(2)),$(call xargs,$(1),$(wordlist 2,$(words $(2)),$(2))))\nendef\ndefine write-to-file",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Note: this does not handle spaces in paths\ndefine xargs\n  $(1) $(word 1,$(2))\n$(if $(word 2,$(2)),$(call xargs,$(1),$(wordlist 2,$(words $(2)),$(2))))\nendef\ndefine write-to-file\n  @: >$(1)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink_thin",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Note: this does not handle spaces in paths\ndefine xargs\n  $(1) $(word 1,$(2))\n$(if $(word 2,$(2)),$(call xargs,$(1),$(wordlist 2,$(words $(2)),$(2))))\nendef\ndefine write-to-file\n  @: >$(1)\n$(call xargs,@printf \"%s\\\\n\" >>$(1),$(2))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink_thin",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\n# Note: this does not handle spaces in paths\ndefine xargs\n  $(1) $(word 1,$(2))\n$(if $(word 2,$(2)),$(call xargs,$(1),$(wordlist 2,$(words $(2)),$(2))))\nendef\ndefine write-to-file\n  @: >$(1)\n$(call xargs,@printf \"%s\\\\n\" >>$(1),$(2))\nendef",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_link = LINK($(TOOLSET)) $@\nquiet_cmd_link_host = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)\ncmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)\n# Other shared-object link notes:\n# - Set SONAME to the library filename so our binaries don't reference\n# the local, absolute paths used on the link command-line.\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_link_host",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_link_host = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)\ncmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)\n# Other shared-object link notes:\n# - Set SONAME to the library filename so our binaries don't reference\n# the local, absolute paths used on the link command-line.\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)\ncmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)\n# Other shared-object link notes:\n# - Set SONAME to the library filename so our binaries don't reference\n# the local, absolute paths used on the link command-line.\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)\nquiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_link_host",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)\n# Other shared-object link notes:\n# - Set SONAME to the library filename so our binaries don't reference\n# the local, absolute paths used on the link command-line.\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)\nquiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)\nquiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_AIX = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)\nquiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_AIX = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)\nquiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_AIX = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)\nquiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_AIX = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink_module_host",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_AIX = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink_module_host",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_AIX = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "LINK_COMMANDS_AIX",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "LINK_COMMANDS_AIX = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink_thin",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS400 = \"\"\"\\",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink_thin",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS400 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS400 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS400 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS400 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS400 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS400 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS400 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "LINK_COMMANDS_OS400",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "LINK_COMMANDS_OS400 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink_thin",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS390 = \"\"\"\\",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink_thin",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X64 crs $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS390 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS390 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS390 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS390 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS390 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS390 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\nLINK_COMMANDS_OS390 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "LINK_COMMANDS_OS390",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "LINK_COMMANDS_OS390 = \"\"\"\\\nquiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink = AR($(TOOLSET)) $@\ncmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)\nquiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_alink_thin",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_alink_thin = AR($(TOOLSET)) $@\ncmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\n# Header of toplevel Makefile.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_alink_thin",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)\nquiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\n# Header of toplevel Makefile.\n# This should go into the build tree, but it's easier to keep it here for now.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_link = LINK($(TOOLSET)) $@\ncmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\n# Header of toplevel Makefile.\n# This should go into the build tree, but it's easier to keep it here for now.\nSHARED_HEADER = (",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\n# Header of toplevel Makefile.\n# This should go into the build tree, but it's easier to keep it here for now.\nSHARED_HEADER = (\n    \"\"\"\\",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink = SOLINK($(TOOLSET)) $@\ncmd_solink = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\n# Header of toplevel Makefile.\n# This should go into the build tree, but it's easier to keep it here for now.\nSHARED_HEADER = (\n    \"\"\"\\\n# We borrow heavily from the kernel build setup, though we are simpler since",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)\nquiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\n# Header of toplevel Makefile.\n# This should go into the build tree, but it's easier to keep it here for now.\nSHARED_HEADER = (\n    \"\"\"\\\n# We borrow heavily from the kernel build setup, though we are simpler since\n# we don't have Kconfig tweaking settings on us.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@\ncmd_solink_module = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\n# Header of toplevel Makefile.\n# This should go into the build tree, but it's easier to keep it here for now.\nSHARED_HEADER = (\n    \"\"\"\\\n# We borrow heavily from the kernel build setup, though we are simpler since\n# we don't have Kconfig tweaking settings on us.\n# The implicit make rules have it looking for RCS files, among other things.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_solink_module",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_solink_module = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)\n\"\"\"  # noqa: E501\n# Header of toplevel Makefile.\n# This should go into the build tree, but it's easier to keep it here for now.\nSHARED_HEADER = (\n    \"\"\"\\\n# We borrow heavily from the kernel build setup, though we are simpler since\n# we don't have Kconfig tweaking settings on us.\n# The implicit make rules have it looking for RCS files, among other things.\n# We instead explicitly write all the rules we care about.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "SHARED_HEADER",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "SHARED_HEADER = (\n    \"\"\"\\\n# We borrow heavily from the kernel build setup, though we are simpler since\n# we don't have Kconfig tweaking settings on us.\n# The implicit make rules have it looking for RCS files, among other things.\n# We instead explicitly write all the rules we care about.\n# It's even quicker (saves ~200ms) to pass -r on the command line.\nMAKEFLAGS=-r\n# The source directory tree.\nsrcdir := %(srcdir)s",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "replace_spaces",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "replace_spaces = $(subst $(space),\"\"\"\n    + SPACE_REPLACEMENT\n    + \"\"\",$1)\nunreplace_spaces = $(subst \"\"\"\n    + SPACE_REPLACEMENT\n    + \"\"\",$(space),$1)\ndirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))\n# Flags to make gcc output dependency info.  Note that you need to be\n# careful here to use the flags that ccache and distcc can understand.\n# We write to a dep file on the side first and then rename at the end",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "unreplace_spaces",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "unreplace_spaces = $(subst \"\"\"\n    + SPACE_REPLACEMENT\n    + \"\"\",$(space),$1)\ndirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))\n# Flags to make gcc output dependency info.  Note that you need to be\n# careful here to use the flags that ccache and distcc can understand.\n# We write to a dep file on the side first and then rename at the end\n# so we can't end up with a broken dep file.\ndepfile = $(depsdir)/$(call replace_spaces,$@).d\nDEPFLAGS = %(makedep_args)s -MF $(depfile).raw",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "dirx",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))\n# Flags to make gcc output dependency info.  Note that you need to be\n# careful here to use the flags that ccache and distcc can understand.\n# We write to a dep file on the side first and then rename at the end\n# so we can't end up with a broken dep file.\ndepfile = $(depsdir)/$(call replace_spaces,$@).d\nDEPFLAGS = %(makedep_args)s -MF $(depfile).raw\n# We have to fixup the deps output in a few ways.\n# (1) the file output should mention the proper .o file.\n# ccache or distcc lose the path to the target, so we convert a rule of",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "depfile",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "depfile = $(depsdir)/$(call replace_spaces,$@).d\nDEPFLAGS = %(makedep_args)s -MF $(depfile).raw\n# We have to fixup the deps output in a few ways.\n# (1) the file output should mention the proper .o file.\n# ccache or distcc lose the path to the target, so we convert a rule of\n# the form:\n#   foobar.o: DEP1 DEP2\n# into\n#   path/to/foobar.o: DEP1 DEP2\n# (2) we want missing files not to cause us to fail to build.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "DEPFLAGS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "DEPFLAGS = %(makedep_args)s -MF $(depfile).raw\n# We have to fixup the deps output in a few ways.\n# (1) the file output should mention the proper .o file.\n# ccache or distcc lose the path to the target, so we convert a rule of\n# the form:\n#   foobar.o: DEP1 DEP2\n# into\n#   path/to/foobar.o: DEP1 DEP2\n# (2) we want missing files not to cause us to fail to build.\n# We want to rewrite",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_cc",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_cc = CC($(TOOLSET)) $@\ncmd_cc = $(CC.$(TOOLSET)) -o $@ $< $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c\nquiet_cmd_cxx = CXX($(TOOLSET)) $@\ncmd_cxx = $(CXX.$(TOOLSET)) -o $@ $< $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c\n%(extra_commands)s\nquiet_cmd_touch = TOUCH $@\ncmd_touch = touch $@\nquiet_cmd_copy = COPY $@\n# send stderr to /dev/null to ignore messages when linking directories.\ncmd_copy = ln -f \"$<\" \"$@\" 2>/dev/null || (rm -rf \"$@\" && cp %(copy_archive_args)s \"$<\" \"$@\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_cc",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_cc = $(CC.$(TOOLSET)) -o $@ $< $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c\nquiet_cmd_cxx = CXX($(TOOLSET)) $@\ncmd_cxx = $(CXX.$(TOOLSET)) -o $@ $< $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c\n%(extra_commands)s\nquiet_cmd_touch = TOUCH $@\ncmd_touch = touch $@\nquiet_cmd_copy = COPY $@\n# send stderr to /dev/null to ignore messages when linking directories.\ncmd_copy = ln -f \"$<\" \"$@\" 2>/dev/null || (rm -rf \"$@\" && cp %(copy_archive_args)s \"$<\" \"$@\")\nquiet_cmd_symlink = SYMLINK $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_cxx",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_cxx = CXX($(TOOLSET)) $@\ncmd_cxx = $(CXX.$(TOOLSET)) -o $@ $< $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c\n%(extra_commands)s\nquiet_cmd_touch = TOUCH $@\ncmd_touch = touch $@\nquiet_cmd_copy = COPY $@\n# send stderr to /dev/null to ignore messages when linking directories.\ncmd_copy = ln -f \"$<\" \"$@\" 2>/dev/null || (rm -rf \"$@\" && cp %(copy_archive_args)s \"$<\" \"$@\")\nquiet_cmd_symlink = SYMLINK $@\ncmd_symlink = ln -sf \"$<\" \"$@\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_cxx",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_cxx = $(CXX.$(TOOLSET)) -o $@ $< $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c\n%(extra_commands)s\nquiet_cmd_touch = TOUCH $@\ncmd_touch = touch $@\nquiet_cmd_copy = COPY $@\n# send stderr to /dev/null to ignore messages when linking directories.\ncmd_copy = ln -f \"$<\" \"$@\" 2>/dev/null || (rm -rf \"$@\" && cp %(copy_archive_args)s \"$<\" \"$@\")\nquiet_cmd_symlink = SYMLINK $@\ncmd_symlink = ln -sf \"$<\" \"$@\"\n%(link_commands)s",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_touch",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_touch = TOUCH $@\ncmd_touch = touch $@\nquiet_cmd_copy = COPY $@\n# send stderr to /dev/null to ignore messages when linking directories.\ncmd_copy = ln -f \"$<\" \"$@\" 2>/dev/null || (rm -rf \"$@\" && cp %(copy_archive_args)s \"$<\" \"$@\")\nquiet_cmd_symlink = SYMLINK $@\ncmd_symlink = ln -sf \"$<\" \"$@\"\n%(link_commands)s\n\"\"\"  # noqa: E501\n    r\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_touch",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_touch = touch $@\nquiet_cmd_copy = COPY $@\n# send stderr to /dev/null to ignore messages when linking directories.\ncmd_copy = ln -f \"$<\" \"$@\" 2>/dev/null || (rm -rf \"$@\" && cp %(copy_archive_args)s \"$<\" \"$@\")\nquiet_cmd_symlink = SYMLINK $@\ncmd_symlink = ln -sf \"$<\" \"$@\"\n%(link_commands)s\n\"\"\"  # noqa: E501\n    r\"\"\"\n# Define an escape_quotes function to escape single quotes.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_copy",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_copy = COPY $@\n# send stderr to /dev/null to ignore messages when linking directories.\ncmd_copy = ln -f \"$<\" \"$@\" 2>/dev/null || (rm -rf \"$@\" && cp %(copy_archive_args)s \"$<\" \"$@\")\nquiet_cmd_symlink = SYMLINK $@\ncmd_symlink = ln -sf \"$<\" \"$@\"\n%(link_commands)s\n\"\"\"  # noqa: E501\n    r\"\"\"\n# Define an escape_quotes function to escape single quotes.\n# This allows us to handle quotes properly as long as we always use",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_copy",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_copy = ln -f \"$<\" \"$@\" 2>/dev/null || (rm -rf \"$@\" && cp %(copy_archive_args)s \"$<\" \"$@\")\nquiet_cmd_symlink = SYMLINK $@\ncmd_symlink = ln -sf \"$<\" \"$@\"\n%(link_commands)s\n\"\"\"  # noqa: E501\n    r\"\"\"\n# Define an escape_quotes function to escape single quotes.\n# This allows us to handle quotes properly as long as we always use\n# use single quotes and escape_quotes.\nescape_quotes = $(subst ','\\'',$(1))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_symlink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_symlink = SYMLINK $@\ncmd_symlink = ln -sf \"$<\" \"$@\"\n%(link_commands)s\n\"\"\"  # noqa: E501\n    r\"\"\"\n# Define an escape_quotes function to escape single quotes.\n# This allows us to handle quotes properly as long as we always use\n# use single quotes and escape_quotes.\nescape_quotes = $(subst ','\\'',$(1))\n# This comment is here just to include a ' to unconfuse syntax highlighting.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_symlink",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_symlink = ln -sf \"$<\" \"$@\"\n%(link_commands)s\n\"\"\"  # noqa: E501\n    r\"\"\"\n# Define an escape_quotes function to escape single quotes.\n# This allows us to handle quotes properly as long as we always use\n# use single quotes and escape_quotes.\nescape_quotes = $(subst ','\\'',$(1))\n# This comment is here just to include a ' to unconfuse syntax highlighting.\n# Define an escape_vars function to escape '$' variable syntax.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "escape_quotes",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "escape_quotes = $(subst ','\\'',$(1))\n# This comment is here just to include a ' to unconfuse syntax highlighting.\n# Define an escape_vars function to escape '$' variable syntax.\n# This allows us to read/write command lines with shell variables (e.g.\n# $LD_LIBRARY_PATH), without triggering make substitution.\nescape_vars = $(subst $$,$$$$,$(1))\n# Helper that expands to a shell command to echo a string exactly as it is in\n# make. This uses printf instead of echo because printf's behaviour with respect\n# to escape sequences is more portable than echo's across different shells\n# (e.g., dash, bash).",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "escape_vars",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "escape_vars = $(subst $$,$$$$,$(1))\n# Helper that expands to a shell command to echo a string exactly as it is in\n# make. This uses printf instead of echo because printf's behaviour with respect\n# to escape sequences is more portable than echo's across different shells\n# (e.g., dash, bash).\nexact_echo = printf '%%s\\n' '$(call escape_quotes,$(1))'\n\"\"\"\n    \"\"\"\n# Helper to compare the command we're about to run against the command\n# we logged the last time we ran the command.  Produces an empty",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "exact_echo",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "exact_echo = printf '%%s\\n' '$(call escape_quotes,$(1))'\n\"\"\"\n    \"\"\"\n# Helper to compare the command we're about to run against the command\n# we logged the last time we ran the command.  Produces an empty\n# string (false) when the commands match.\n# Tricky point: Make has no string-equality test function.\n# The kernel uses the following, but it seems like it would have false\n# positives, where one string reordered its arguments.\n#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\\\",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "command_changed",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\\\\n                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))\n# Helper that is non-empty when a prerequisite changes.\n# Normally make does this implicitly, but we force rules to always run\n# so we can check their command lines.\n#   $? -- new prerequisites\n#   $| -- order-only dependencies\nprereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))\n# Helper that executes all postbuilds until one fails.\ndefine do_postbuilds",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "prereq_changed",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))\n# Helper that executes all postbuilds until one fails.\ndefine do_postbuilds\n  @E=0;\\\\\n  for p in $(POSTBUILDS); do\\\\\n    eval $$p;\\\\\n    E=$$?;\\\\\n    if [ $$E -ne 0 ]; then\\\\\n      break;\\\\\n    fi;\\\\",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "SHARED_HEADER_MAC_COMMANDS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "SHARED_HEADER_MAC_COMMANDS = \"\"\"\nquiet_cmd_objc = CXX($(TOOLSET)) $@\ncmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_objcxx = CXX($(TOOLSET)) $@\ncmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# Commands for precompiled header files.\nquiet_cmd_pch_c = CXX($(TOOLSET)) $@\ncmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_cc = CXX($(TOOLSET)) $@\ncmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_objc",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_objc = CXX($(TOOLSET)) $@\ncmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_objcxx = CXX($(TOOLSET)) $@\ncmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# Commands for precompiled header files.\nquiet_cmd_pch_c = CXX($(TOOLSET)) $@\ncmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_cc = CXX($(TOOLSET)) $@\ncmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_m = CXX($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_objc",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_objcxx = CXX($(TOOLSET)) $@\ncmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# Commands for precompiled header files.\nquiet_cmd_pch_c = CXX($(TOOLSET)) $@\ncmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_cc = CXX($(TOOLSET)) $@\ncmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_m = CXX($(TOOLSET)) $@\ncmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_objcxx",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_objcxx = CXX($(TOOLSET)) $@\ncmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# Commands for precompiled header files.\nquiet_cmd_pch_c = CXX($(TOOLSET)) $@\ncmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_cc = CXX($(TOOLSET)) $@\ncmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_m = CXX($(TOOLSET)) $@\ncmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_pch_mm = CXX($(TOOLSET)) $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_objcxx",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# Commands for precompiled header files.\nquiet_cmd_pch_c = CXX($(TOOLSET)) $@\ncmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_cc = CXX($(TOOLSET)) $@\ncmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_m = CXX($(TOOLSET)) $@\ncmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_pch_mm = CXX($(TOOLSET)) $@\ncmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_pch_c",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_pch_c = CXX($(TOOLSET)) $@\ncmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_cc = CXX($(TOOLSET)) $@\ncmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_m = CXX($(TOOLSET)) $@\ncmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_pch_mm = CXX($(TOOLSET)) $@\ncmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# gyp-mac-tool is written next to the root Makefile by gyp.\n# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_pch_c",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_cc = CXX($(TOOLSET)) $@\ncmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_m = CXX($(TOOLSET)) $@\ncmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_pch_mm = CXX($(TOOLSET)) $@\ncmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# gyp-mac-tool is written next to the root Makefile by gyp.\n# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd\n# already.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_pch_cc",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_pch_cc = CXX($(TOOLSET)) $@\ncmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_m = CXX($(TOOLSET)) $@\ncmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_pch_mm = CXX($(TOOLSET)) $@\ncmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# gyp-mac-tool is written next to the root Makefile by gyp.\n# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd\n# already.\nquiet_cmd_mac_tool = MACTOOL $(4) $<",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_pch_cc",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<\nquiet_cmd_pch_m = CXX($(TOOLSET)) $@\ncmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_pch_mm = CXX($(TOOLSET)) $@\ncmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# gyp-mac-tool is written next to the root Makefile by gyp.\n# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd\n# already.\nquiet_cmd_mac_tool = MACTOOL $(4) $<\ncmd_mac_tool = %(python)s gyp-mac-tool $(4) $< \"$@\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_pch_m",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_pch_m = CXX($(TOOLSET)) $@\ncmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_pch_mm = CXX($(TOOLSET)) $@\ncmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# gyp-mac-tool is written next to the root Makefile by gyp.\n# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd\n# already.\nquiet_cmd_mac_tool = MACTOOL $(4) $<\ncmd_mac_tool = %(python)s gyp-mac-tool $(4) $< \"$@\"\nquiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_pch_m",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<\nquiet_cmd_pch_mm = CXX($(TOOLSET)) $@\ncmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# gyp-mac-tool is written next to the root Makefile by gyp.\n# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd\n# already.\nquiet_cmd_mac_tool = MACTOOL $(4) $<\ncmd_mac_tool = %(python)s gyp-mac-tool $(4) $< \"$@\"\nquiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@\ncmd_mac_package_framework = %(python)s gyp-mac-tool package-framework \"$@\" $(4)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_pch_mm",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_pch_mm = CXX($(TOOLSET)) $@\ncmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# gyp-mac-tool is written next to the root Makefile by gyp.\n# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd\n# already.\nquiet_cmd_mac_tool = MACTOOL $(4) $<\ncmd_mac_tool = %(python)s gyp-mac-tool $(4) $< \"$@\"\nquiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@\ncmd_mac_package_framework = %(python)s gyp-mac-tool package-framework \"$@\" $(4)\nquiet_cmd_infoplist = INFOPLIST $@",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_pch_mm",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<\n# gyp-mac-tool is written next to the root Makefile by gyp.\n# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd\n# already.\nquiet_cmd_mac_tool = MACTOOL $(4) $<\ncmd_mac_tool = %(python)s gyp-mac-tool $(4) $< \"$@\"\nquiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@\ncmd_mac_package_framework = %(python)s gyp-mac-tool package-framework \"$@\" $(4)\nquiet_cmd_infoplist = INFOPLIST $@\ncmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) \"$<\" -o \"$@\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_mac_tool",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_mac_tool = MACTOOL $(4) $<\ncmd_mac_tool = %(python)s gyp-mac-tool $(4) $< \"$@\"\nquiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@\ncmd_mac_package_framework = %(python)s gyp-mac-tool package-framework \"$@\" $(4)\nquiet_cmd_infoplist = INFOPLIST $@\ncmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) \"$<\" -o \"$@\"\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\ndef WriteRootHeaderSuffixRules(writer):\n    extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)\n    writer.write(\"# Suffix rules, putting all outputs into $(obj).\\n\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_mac_tool",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_mac_tool = %(python)s gyp-mac-tool $(4) $< \"$@\"\nquiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@\ncmd_mac_package_framework = %(python)s gyp-mac-tool package-framework \"$@\" $(4)\nquiet_cmd_infoplist = INFOPLIST $@\ncmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) \"$<\" -o \"$@\"\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\ndef WriteRootHeaderSuffixRules(writer):\n    extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)\n    writer.write(\"# Suffix rules, putting all outputs into $(obj).\\n\")\n    for ext in extensions:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_mac_package_framework",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@\ncmd_mac_package_framework = %(python)s gyp-mac-tool package-framework \"$@\" $(4)\nquiet_cmd_infoplist = INFOPLIST $@\ncmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) \"$<\" -o \"$@\"\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\ndef WriteRootHeaderSuffixRules(writer):\n    extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)\n    writer.write(\"# Suffix rules, putting all outputs into $(obj).\\n\")\n    for ext in extensions:\n        writer.write(\"$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\\n\" % ext)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_mac_package_framework",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_mac_package_framework = %(python)s gyp-mac-tool package-framework \"$@\" $(4)\nquiet_cmd_infoplist = INFOPLIST $@\ncmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) \"$<\" -o \"$@\"\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\ndef WriteRootHeaderSuffixRules(writer):\n    extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)\n    writer.write(\"# Suffix rules, putting all outputs into $(obj).\\n\")\n    for ext in extensions:\n        writer.write(\"$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\\n\" % ext)\n        writer.write(\"\\t@$(call do_cmd,%s,1)\\n\" % COMPILABLE_EXTENSIONS[ext])",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_infoplist",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_infoplist = INFOPLIST $@\ncmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) \"$<\" -o \"$@\"\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\ndef WriteRootHeaderSuffixRules(writer):\n    extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)\n    writer.write(\"# Suffix rules, putting all outputs into $(obj).\\n\")\n    for ext in extensions:\n        writer.write(\"$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\\n\" % ext)\n        writer.write(\"\\t@$(call do_cmd,%s,1)\\n\" % COMPILABLE_EXTENSIONS[ext])\n    writer.write(\"\\n# Try building from generated source, too.\\n\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_infoplist",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) \"$<\" -o \"$@\"\n\"\"\" % {\"python\": sys.executable}  # noqa: E501\ndef WriteRootHeaderSuffixRules(writer):\n    extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)\n    writer.write(\"# Suffix rules, putting all outputs into $(obj).\\n\")\n    for ext in extensions:\n        writer.write(\"$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\\n\" % ext)\n        writer.write(\"\\t@$(call do_cmd,%s,1)\\n\" % COMPILABLE_EXTENSIONS[ext])\n    writer.write(\"\\n# Try building from generated source, too.\\n\")\n    for ext in extensions:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "SHARED_HEADER_OS390_COMMANDS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "SHARED_HEADER_OS390_COMMANDS = \"\"\"\nPLIFLAGS.target ?= -qlp=64 -qlimits=extname=31  $(PLIFLAGS)\nPLIFLAGS.host ?= -qlp=64 -qlimits=extname=31 $(PLIFLAGS)\nquiet_cmd_pli = PLI($(TOOLSET)) $@\ncmd_pli = $(PLI.$(TOOLSET)) $(GYP_PLIFLAGS) $(PLIFLAGS.$(TOOLSET)) -c $< && \\\n          if [ -f $(notdir $@) ]; then /bin/cp $(notdir $@) $@; else true; fi\n\"\"\"\nSHARED_HEADER_SUFFIX_RULES_COMMENT1 = \"\"\"\\\n# Suffix rules, putting all outputs into $(obj).\n\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "quiet_cmd_pli",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "quiet_cmd_pli = PLI($(TOOLSET)) $@\ncmd_pli = $(PLI.$(TOOLSET)) $(GYP_PLIFLAGS) $(PLIFLAGS.$(TOOLSET)) -c $< && \\\n          if [ -f $(notdir $@) ]; then /bin/cp $(notdir $@) $@; else true; fi\n\"\"\"\nSHARED_HEADER_SUFFIX_RULES_COMMENT1 = \"\"\"\\\n# Suffix rules, putting all outputs into $(obj).\n\"\"\"\nSHARED_HEADER_SUFFIX_RULES_COMMENT2 = \"\"\"\\\n# Try building from generated source, too.\n\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "cmd_pli",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "cmd_pli = $(PLI.$(TOOLSET)) $(GYP_PLIFLAGS) $(PLIFLAGS.$(TOOLSET)) -c $< && \\\n          if [ -f $(notdir $@) ]; then /bin/cp $(notdir $@) $@; else true; fi\n\"\"\"\nSHARED_HEADER_SUFFIX_RULES_COMMENT1 = \"\"\"\\\n# Suffix rules, putting all outputs into $(obj).\n\"\"\"\nSHARED_HEADER_SUFFIX_RULES_COMMENT2 = \"\"\"\\\n# Try building from generated source, too.\n\"\"\"\nSHARED_FOOTER = \"\"\"\\",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "SHARED_HEADER_SUFFIX_RULES_COMMENT1",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "SHARED_HEADER_SUFFIX_RULES_COMMENT1 = \"\"\"\\\n# Suffix rules, putting all outputs into $(obj).\n\"\"\"\nSHARED_HEADER_SUFFIX_RULES_COMMENT2 = \"\"\"\\\n# Try building from generated source, too.\n\"\"\"\nSHARED_FOOTER = \"\"\"\\\n# \"all\" is a concatenation of the \"all\" targets from all the included\n# sub-makefiles. This is just here to clarify.\nall:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "SHARED_HEADER_SUFFIX_RULES_COMMENT2",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "SHARED_HEADER_SUFFIX_RULES_COMMENT2 = \"\"\"\\\n# Try building from generated source, too.\n\"\"\"\nSHARED_FOOTER = \"\"\"\\\n# \"all\" is a concatenation of the \"all\" targets from all the included\n# sub-makefiles. This is just here to clarify.\nall:\n# Add in dependency-tracking rules.  $(all_deps) is the list of every single\n# target in our tree. Only consider the ones with .d (dependency) info:\nd_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "SHARED_FOOTER",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "SHARED_FOOTER = \"\"\"\\\n# \"all\" is a concatenation of the \"all\" targets from all the included\n# sub-makefiles. This is just here to clarify.\nall:\n# Add in dependency-tracking rules.  $(all_deps) is the list of every single\n# target in our tree. Only consider the ones with .d (dependency) info:\nd_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))\nifneq ($(d_files),)\n  include $(d_files)\nendif",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "header",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "header = \"\"\"\\\n# This file is generated by gyp; do not edit.\n\"\"\"\n# Maps every compilable file extension to the do_cmd that compiles it.\nCOMPILABLE_EXTENSIONS = {\n    \".c\": \"cc\",\n    \".cc\": \"cxx\",\n    \".cpp\": \"cxx\",\n    \".cxx\": \"cxx\",\n    \".s\": \"cc\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "COMPILABLE_EXTENSIONS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "COMPILABLE_EXTENSIONS = {\n    \".c\": \"cc\",\n    \".cc\": \"cxx\",\n    \".cpp\": \"cxx\",\n    \".cxx\": \"cxx\",\n    \".s\": \"cc\",\n    \".S\": \"cc\",\n}\ndef Compilable(filename):\n    \"\"\"Return true if the file is compilable (should be in OBJS).\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "srcdir_prefix",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "srcdir_prefix = \"\"\ndef Sourceify(path):\n    \"\"\"Convert a path to its source directory form.\"\"\"\n    if \"$(\" in path:\n        return path\n    if os.path.isabs(path):\n        return path\n    return srcdir_prefix + path\ndef QuoteSpaces(s, quote=r\"\\ \"):\n    return s.replace(\" \", quote)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "target_outputs",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "target_outputs = {}\n# Map from qualified target to any linkable output.  A subset\n# of target_outputs.  E.g. when mybinary depends on liba, we want to\n# include liba in the linker line; when otherbinary depends on\n# mybinary, we just want to build mybinary first.\ntarget_link_deps = {}\nclass MakefileWriter:\n    \"\"\"MakefileWriter packages up the writing of one target-specific foobar.mk.\n    Its only real entry point is Write(), and is mostly used for namespacing.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "target_link_deps",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "peekOfCode": "target_link_deps = {}\nclass MakefileWriter:\n    \"\"\"MakefileWriter packages up the writing of one target-specific foobar.mk.\n    Its only real entry point is Write(), and is mostly used for namespacing.\n    \"\"\"\n    def __init__(self, generator_flags, flavor):\n        self.generator_flags = generator_flags\n        self.flavor = flavor\n        self.suffix_rules_srcdir = {}\n        self.suffix_rules_objdir1 = {}",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.make",
        "documentation": {}
    },
    {
        "label": "MSBuildRule",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "class MSBuildRule:\n    \"\"\"Used to store information used to generate an MSBuild rule.\n    Attributes:\n      rule_name: The rule name, sanitized to use in XML.\n      target_name: The name of the target.\n      after_targets: The name of the AfterTargets element.\n      before_targets: The name of the BeforeTargets element.\n      depends_on: The name of the DependsOn element.\n      compute_output: The name of the ComputeOutput element.\n      dirs_to_make: The name of the DirsToMake element.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "CalculateVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "def CalculateVariables(default_variables, params):\n    \"\"\"Generated variables that require params to be known.\"\"\"\n    generator_flags = params.get(\"generator_flags\", {})\n    # Select project file format version (if unset, default to auto detecting).\n    msvs_version = MSVSVersion.SelectVisualStudioVersion(\n        generator_flags.get(\"msvs_version\", \"auto\")\n    )\n    # Stash msvs_version for later (so we don't have to probe the system twice).\n    params[\"msvs_version\"] = msvs_version\n    # Set a variable so conditions can be based on msvs_version.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "PerformBuild",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "def PerformBuild(data, configurations, params):\n    options = params[\"options\"]\n    msvs_version = params[\"msvs_version\"]\n    devenv = os.path.join(msvs_version.path, \"Common7\", \"IDE\", \"devenv.com\")\n    for build_file, build_file_dict in data.items():\n        (build_file_root, build_file_ext) = os.path.splitext(build_file)\n        if build_file_ext != \".gyp\":\n            continue\n        sln_path = build_file_root + options.suffix + \".sln\"\n        if options.generator_output:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "CalculateGeneratorInputInfo",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "def CalculateGeneratorInputInfo(params):\n    if params.get(\"flavor\") == \"ninja\":\n        toplevel = params[\"options\"].toplevel_dir\n        qualified_out_dir = os.path.normpath(\n            os.path.join(\n                toplevel,\n                ninja_generator.ComputeOutputDir(params),\n                \"gypfiles-msvs-ninja\",\n            )\n        )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    \"\"\"Generate .sln and .vcproj files.\n    This is the entry point for this generator.\n    Arguments:\n      target_list: List of target pairs: 'base/base.gyp:base'.\n      target_dicts: Dict of target properties keyed on target pair.\n      data: Dictionary containing per .gyp data.\n    \"\"\"\n    global fixpath_prefix\n    options = params[\"options\"]",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "VALID_MSVS_GUID_CHARS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "VALID_MSVS_GUID_CHARS = re.compile(r\"^[A-F0-9\\-]+$\")\ngenerator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ngenerator_default_variables = {\n    \"DRIVER_PREFIX\": \"\",\n    \"DRIVER_SUFFIX\": \".sys\",\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \".exe\",\n    \"STATIC_LIB_PREFIX\": \"\",\n    \"SHARED_LIB_PREFIX\": \"\",\n    \"STATIC_LIB_SUFFIX\": \".lib\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "generator_supports_multiple_toolsets",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "generator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ngenerator_default_variables = {\n    \"DRIVER_PREFIX\": \"\",\n    \"DRIVER_SUFFIX\": \".sys\",\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \".exe\",\n    \"STATIC_LIB_PREFIX\": \"\",\n    \"SHARED_LIB_PREFIX\": \"\",\n    \"STATIC_LIB_SUFFIX\": \".lib\",\n    \"SHARED_LIB_SUFFIX\": \".dll\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "generator_default_variables = {\n    \"DRIVER_PREFIX\": \"\",\n    \"DRIVER_SUFFIX\": \".sys\",\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \".exe\",\n    \"STATIC_LIB_PREFIX\": \"\",\n    \"SHARED_LIB_PREFIX\": \"\",\n    \"STATIC_LIB_SUFFIX\": \".lib\",\n    \"SHARED_LIB_SUFFIX\": \".dll\",\n    \"INTERMEDIATE_DIR\": \"$(IntDir)\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "generator_additional_path_sections",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "generator_additional_path_sections = [\n    \"msvs_cygwin_dirs\",\n    \"msvs_props\",\n]\ngenerator_additional_non_configuration_keys = [\n    \"msvs_cygwin_dirs\",\n    \"msvs_cygwin_shell\",\n    \"msvs_large_pdb\",\n    \"msvs_shard\",\n    \"msvs_external_builder\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "generator_additional_non_configuration_keys",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "generator_additional_non_configuration_keys = [\n    \"msvs_cygwin_dirs\",\n    \"msvs_cygwin_shell\",\n    \"msvs_large_pdb\",\n    \"msvs_shard\",\n    \"msvs_external_builder\",\n    \"msvs_external_builder_out_dir\",\n    \"msvs_external_builder_build_cmd\",\n    \"msvs_external_builder_clean_cmd\",\n    \"msvs_external_builder_clcompile_cmd\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "generator_filelist_paths",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "generator_filelist_paths = None\n# List of precompiled header related keys.\nprecomp_keys = [\n    \"msvs_precompiled_header\",\n    \"msvs_precompiled_source\",\n]\ncached_username = None\ncached_domain = None\n# TODO(gspencer): Switch the os.environ calls to be\n# win32api.GetDomainName() and win32api.GetUserName() once the",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "precomp_keys",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "precomp_keys = [\n    \"msvs_precompiled_header\",\n    \"msvs_precompiled_source\",\n]\ncached_username = None\ncached_domain = None\n# TODO(gspencer): Switch the os.environ calls to be\n# win32api.GetDomainName() and win32api.GetUserName() once the\n# python version in depot_tools has been updated to work on Vista\n# 64-bit.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "cached_username",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "cached_username = None\ncached_domain = None\n# TODO(gspencer): Switch the os.environ calls to be\n# win32api.GetDomainName() and win32api.GetUserName() once the\n# python version in depot_tools has been updated to work on Vista\n# 64-bit.\ndef _GetDomainAndUserName():\n    if sys.platform not in (\"win32\", \"cygwin\"):\n        return (\"DOMAIN\", \"USERNAME\")\n    global cached_username",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "cached_domain",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "cached_domain = None\n# TODO(gspencer): Switch the os.environ calls to be\n# win32api.GetDomainName() and win32api.GetUserName() once the\n# python version in depot_tools has been updated to work on Vista\n# 64-bit.\ndef _GetDomainAndUserName():\n    if sys.platform not in (\"win32\", \"cygwin\"):\n        return (\"DOMAIN\", \"USERNAME\")\n    global cached_username\n    global cached_domain",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "fixpath_prefix",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "fixpath_prefix = None\ndef _NormalizedSource(source):\n    \"\"\"Normalize the path.\n    But not if that gets rid of a variable, as this may expand to something\n    larger than one directory.\n    Arguments:\n        source: The path to be normalize.d\n    Returns:\n        The normalized path.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "quote_replacer_regex",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "quote_replacer_regex = re.compile(r'(\\\\*)\"')\ndef _EscapeCommandLineArgumentForMSVS(s):\n    \"\"\"Escapes a Windows command-line argument.\n    So that the Win32 CommandLineToArgv function will turn the escaped result back\n    into the original string.\n    See http://msdn.microsoft.com/en-us/library/17w5ykft.aspx\n    (\"Parsing C++ Command-Line Arguments\") to understand why we have to do\n    this.\n    Args:\n        s: the string to be escaped.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "delimiters_replacer_regex",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "delimiters_replacer_regex = re.compile(r\"(\\\\*)([,;]+)\")\ndef _EscapeVCProjCommandLineArgListItem(s):\n    \"\"\"Escapes command line arguments for MSVS.\n    The VCProj format stores string lists in a single string using commas and\n    semi-colons as separators, which must be quoted if they are to be\n    interpreted literally. However, command-line arguments may already have\n    quotes, and the VCProj parser is ignorant of the backslash escaping\n    convention used by CommandLineToArgv, so the command-line quotes and the\n    VCProj quotes may not be the same quotes. So to store a general\n    command-line argument in a VCProj list, we need to parse the existing",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "quote_replacer_regex2",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "quote_replacer_regex2 = re.compile(r'(\\\\+)\"')\ndef _EscapeCommandLineArgumentForMSBuild(s):\n    \"\"\"Escapes a Windows command-line argument for use by MSBuild.\"\"\"\n    def _Replace(match):\n        return (len(match.group(1)) / 2 * 4) * \"\\\\\" + '\\\\\"'\n    # Escape all quotes so that they are interpreted literally.\n    s = quote_replacer_regex2.sub(_Replace, s)\n    return s\ndef _EscapeMSBuildSpecialCharacters(s):\n    escape_dictionary = {",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "MSVS_VARIABLE_REFERENCE",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "peekOfCode": "MSVS_VARIABLE_REFERENCE = re.compile(r\"\\$\\(([a-zA-Z_][a-zA-Z0-9_]*)\\)\")\ndef _GetMSBuildPropertyGroup(spec, label, properties):\n    \"\"\"Returns a PropertyGroup definition for the specified properties.\n    Arguments:\n      spec: The target project dict.\n      label: An optional label for the PropertyGroup.\n      properties: The dictionary to be converted.  The key is the name of the\n          property.  The value is itself a dictionary; its key is the value and\n          the value a list of condition for which this value is true.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs",
        "documentation": {}
    },
    {
        "label": "TestSequenceFunctions",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs_test",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs_test",
        "peekOfCode": "class TestSequenceFunctions(unittest.TestCase):\n    def setUp(self):\n        self.stderr = StringIO()\n    def test_GetLibraries(self):\n        self.assertEqual(msvs._GetLibraries({}), [])\n        self.assertEqual(msvs._GetLibraries({\"libraries\": []}), [])\n        self.assertEqual(\n            msvs._GetLibraries({\"other\": \"foo\", \"libraries\": [\"a.lib\"]}), [\"a.lib\"]\n        )\n        self.assertEqual(msvs._GetLibraries({\"libraries\": [\"-la\"]}), [\"a.lib\"])",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.msvs_test",
        "documentation": {}
    },
    {
        "label": "Target",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "class Target:\n    \"\"\"Target represents the paths used within a single gyp target.\n    Conceptually, building a single target A is a series of steps:\n    1) actions/rules/copies  generates source/resources/etc.\n    2) compiles              generates .o files\n    3) link                  generates a binary (library/executable)\n    4) bundle                merges the above in a mac bundle\n    (Any of these steps can be optional.)\n    From a build ordering perspective, a dependent target B could just\n    depend on the last output of this series of steps.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "NinjaWriter",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "class NinjaWriter:\n    def __init__(\n        self,\n        hash_for_rules,\n        target_outputs,\n        base_dir,\n        build_dir,\n        output_file,\n        toplevel_build,\n        output_file_name,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "StripPrefix",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def StripPrefix(arg, prefix):\n    if arg.startswith(prefix):\n        return arg[len(prefix) :]\n    return arg\ndef QuoteShellArgument(arg, flavor):\n    \"\"\"Quote a string such that it will be interpreted as a single argument\n    by the shell.\"\"\"\n    # Rather than attempting to enumerate the bad shell characters, just\n    # allow common OK ones and quote anything else.\n    if re.match(r\"^[a-zA-Z0-9_=.\\\\/-]+$\", arg):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "QuoteShellArgument",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def QuoteShellArgument(arg, flavor):\n    \"\"\"Quote a string such that it will be interpreted as a single argument\n    by the shell.\"\"\"\n    # Rather than attempting to enumerate the bad shell characters, just\n    # allow common OK ones and quote anything else.\n    if re.match(r\"^[a-zA-Z0-9_=.\\\\/-]+$\", arg):\n        return arg  # No quoting necessary.\n    if flavor == \"win\":\n        return gyp.msvs_emulation.QuoteForRspFile(arg)\n    return \"'\" + arg.replace(\"'\", \"'\" + '\"\\'\"' + \"'\") + \"'\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "Define",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def Define(d, flavor):\n    \"\"\"Takes a preprocessor define and returns a -D parameter that's ninja- and\n    shell-escaped.\"\"\"\n    if flavor == \"win\":\n        # cl.exe replaces literal # characters with = in preprocessor definitions for\n        # some reason. Octal-encode to work around that.\n        d = d.replace(\"#\", \"\\\\%03o\" % ord(\"#\"))\n    return QuoteShellArgument(ninja_syntax.escape(\"-D\" + d), flavor)\ndef AddArch(output, arch):\n    \"\"\"Adds an arch string to an output path.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "AddArch",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def AddArch(output, arch):\n    \"\"\"Adds an arch string to an output path.\"\"\"\n    output, extension = os.path.splitext(output)\n    return f\"{output}.{arch}{extension}\"\nclass Target:\n    \"\"\"Target represents the paths used within a single gyp target.\n    Conceptually, building a single target A is a series of steps:\n    1) actions/rules/copies  generates source/resources/etc.\n    2) compiles              generates .o files\n    3) link                  generates a binary (library/executable)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "CalculateVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def CalculateVariables(default_variables, params):\n    \"\"\"Calculate additional variables for use in the build (called by gyp).\"\"\"\n    global generator_additional_non_configuration_keys\n    global generator_additional_path_sections\n    flavor = gyp.common.GetFlavor(params)\n    if flavor == \"mac\":\n        default_variables.setdefault(\"OS\", \"mac\")\n        default_variables.setdefault(\"SHARED_LIB_SUFFIX\", \".dylib\")\n        default_variables.setdefault(\n            \"SHARED_LIB_DIR\", generator_default_variables[\"PRODUCT_DIR\"]",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "ComputeOutputDir",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def ComputeOutputDir(params):\n    \"\"\"Returns the path from the toplevel_dir to the build output directory.\"\"\"\n    # generator_dir: relative path from pwd to where make puts build files.\n    # Makes migrating from make to ninja easier, ninja doesn't put anything here.\n    generator_dir = os.path.relpath(params[\"options\"].generator_output or \".\")\n    # output_dir: relative path from generator_dir to the build directory.\n    output_dir = params.get(\"generator_flags\", {}).get(\"output_dir\", \"out\")\n    # Relative path from source root to our output files.  e.g. \"out\"\n    return os.path.normpath(os.path.join(generator_dir, output_dir))\ndef CalculateGeneratorInputInfo(params):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "CalculateGeneratorInputInfo",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def CalculateGeneratorInputInfo(params):\n    \"\"\"Called by __init__ to initialize generator values based on params.\"\"\"\n    # E.g. \"out/gypfiles\"\n    toplevel = params[\"options\"].toplevel_dir\n    qualified_out_dir = os.path.normpath(\n        os.path.join(toplevel, ComputeOutputDir(params), \"gypfiles\")\n    )\n    global generator_filelist_paths\n    generator_filelist_paths = {\n        \"toplevel\": toplevel,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "OpenOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def OpenOutput(path, mode=\"w\"):\n    \"\"\"Open |path| for writing, creating directories if necessary.\"\"\"\n    gyp.common.EnsureDirExists(path)\n    return open(path, mode)\ndef CommandWithWrapper(cmd, wrappers, prog):\n    if wrapper := wrappers.get(cmd, \"\"):\n        return wrapper + \" \" + prog\n    return prog\ndef GetDefaultConcurrentLinks():\n    \"\"\"Returns a best-guess for a number of concurrent links.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "CommandWithWrapper",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def CommandWithWrapper(cmd, wrappers, prog):\n    if wrapper := wrappers.get(cmd, \"\"):\n        return wrapper + \" \" + prog\n    return prog\ndef GetDefaultConcurrentLinks():\n    \"\"\"Returns a best-guess for a number of concurrent links.\"\"\"\n    if pool_size := int(os.environ.get(\"GYP_LINK_CONCURRENCY\") or 0):\n        return pool_size\n    if sys.platform in (\"win32\", \"cygwin\"):\n        class MEMORYSTATUSEX(ctypes.Structure):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "GetDefaultConcurrentLinks",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def GetDefaultConcurrentLinks():\n    \"\"\"Returns a best-guess for a number of concurrent links.\"\"\"\n    if pool_size := int(os.environ.get(\"GYP_LINK_CONCURRENCY\") or 0):\n        return pool_size\n    if sys.platform in (\"win32\", \"cygwin\"):\n        class MEMORYSTATUSEX(ctypes.Structure):\n            _fields_ = [\n                (\"dwLength\", ctypes.c_ulong),\n                (\"dwMemoryLoad\", ctypes.c_ulong),\n                (\"ullTotalPhys\", ctypes.c_ulonglong),",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "GenerateOutputForConfig",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def GenerateOutputForConfig(target_list, target_dicts, data, params, config_name):\n    options = params[\"options\"]\n    flavor = gyp.common.GetFlavor(params)\n    generator_flags = params.get(\"generator_flags\", {})\n    generate_compile_commands = generator_flags.get(\"compile_commands\", False)\n    # build_dir: relative path from source root to our output files.\n    # e.g. \"out/Debug\"\n    build_dir = os.path.normpath(os.path.join(ComputeOutputDir(params), config_name))\n    toplevel_build = os.path.join(options.toplevel_dir, build_dir)\n    master_ninja_file = OpenOutput(os.path.join(toplevel_build, \"build.ninja\"))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "GenerateCompileDBWithNinja",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def GenerateCompileDBWithNinja(path, targets=[\"all\"]):\n    \"\"\"Generates a compile database using ninja.\n    Args:\n        path: The build directory to generate a compile database for.\n        targets: Additional targets to pass to ninja.\n    Returns:\n        List of the contents of the compile database.\n    \"\"\"\n    ninja_path = shutil.which(\"ninja\")\n    if ninja_path is None:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "PerformBuild",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def PerformBuild(data, configurations, params):\n    options = params[\"options\"]\n    for config in configurations:\n        builddir = os.path.join(options.toplevel_dir, \"out\", config)\n        arguments = [\"ninja\", \"-C\", builddir]\n        print(f\"Building [{config}]: {arguments}\")\n        subprocess.check_call(arguments)\ndef CallGenerateOutputForConfig(arglist):\n    # Ignore the interrupt signal so that the parent process catches it and\n    # kills all multiprocessing children.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "CallGenerateOutputForConfig",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def CallGenerateOutputForConfig(arglist):\n    # Ignore the interrupt signal so that the parent process catches it and\n    # kills all multiprocessing children.\n    signal.signal(signal.SIGINT, signal.SIG_IGN)\n    (target_list, target_dicts, data, params, config_name) = arglist\n    GenerateOutputForConfig(target_list, target_dicts, data, params, config_name)\ndef GenerateOutput(target_list, target_dicts, data, params):\n    # Update target_dicts for iOS device builds.\n    target_dicts = gyp.xcode_emulation.CloneConfigurationForDeviceAndEmulator(\n        target_dicts",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    # Update target_dicts for iOS device builds.\n    target_dicts = gyp.xcode_emulation.CloneConfigurationForDeviceAndEmulator(\n        target_dicts\n    )\n    user_config = params.get(\"generator_flags\", {}).get(\"config\", None)\n    if gyp.common.GetFlavor(params) == \"win\":\n        target_list, target_dicts = MSVSUtil.ShardTargets(target_list, target_dicts)\n        target_list, target_dicts = MSVSUtil.InsertLargePdbShims(\n            target_list, target_dicts, generator_default_variables",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "generator_default_variables = {\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"STATIC_LIB_PREFIX\": \"lib\",\n    \"STATIC_LIB_SUFFIX\": \".a\",\n    \"SHARED_LIB_PREFIX\": \"lib\",\n    # Gyp expects the following variables to be expandable by the build\n    # system to the appropriate locations.  Ninja prefers paths to be\n    # known at gyp time.  To resolve this, introduce special\n    # variables starting with $! and $| (which begin with a $ so gyp knows it",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "generator_additional_non_configuration_keys",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "generator_additional_non_configuration_keys = []\ngenerator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ngenerator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ndef StripPrefix(arg, prefix):\n    if arg.startswith(prefix):\n        return arg[len(prefix) :]\n    return arg\ndef QuoteShellArgument(arg, flavor):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "generator_additional_path_sections",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "generator_additional_path_sections = []\ngenerator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ngenerator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ndef StripPrefix(arg, prefix):\n    if arg.startswith(prefix):\n        return arg[len(prefix) :]\n    return arg\ndef QuoteShellArgument(arg, flavor):\n    \"\"\"Quote a string such that it will be interpreted as a single argument",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "generator_extra_sources_for_rules",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "generator_extra_sources_for_rules = []\ngenerator_filelist_paths = None\ngenerator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ndef StripPrefix(arg, prefix):\n    if arg.startswith(prefix):\n        return arg[len(prefix) :]\n    return arg\ndef QuoteShellArgument(arg, flavor):\n    \"\"\"Quote a string such that it will be interpreted as a single argument\n    by the shell.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "generator_filelist_paths",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "generator_filelist_paths = None\ngenerator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ndef StripPrefix(arg, prefix):\n    if arg.startswith(prefix):\n        return arg[len(prefix) :]\n    return arg\ndef QuoteShellArgument(arg, flavor):\n    \"\"\"Quote a string such that it will be interpreted as a single argument\n    by the shell.\"\"\"\n    # Rather than attempting to enumerate the bad shell characters, just",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "generator_supports_multiple_toolsets",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "peekOfCode": "generator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()\ndef StripPrefix(arg, prefix):\n    if arg.startswith(prefix):\n        return arg[len(prefix) :]\n    return arg\ndef QuoteShellArgument(arg, flavor):\n    \"\"\"Quote a string such that it will be interpreted as a single argument\n    by the shell.\"\"\"\n    # Rather than attempting to enumerate the bad shell characters, just\n    # allow common OK ones and quote anything else.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja",
        "documentation": {}
    },
    {
        "label": "TestPrefixesAndSuffixes",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja_test",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja_test",
        "peekOfCode": "class TestPrefixesAndSuffixes(unittest.TestCase):\n    def test_BinaryNamesWindows(self):\n        # These cannot run on non-Windows as they require a VS installation to\n        # correctly handle variable expansion.\n        if sys.platform.startswith(\"win\"):\n            writer = ninja.NinjaWriter(\n                \"foo\", \"wee\", \".\", \".\", \"build.ninja\", \".\", \"build.ninja\", \"win\"\n            )\n            spec = {\"target_name\": \"wee\"}\n            self.assertTrue(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.ninja_test",
        "documentation": {}
    },
    {
        "label": "XcodeProject",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "class XcodeProject:\n    def __init__(self, gyp_path, path, build_file_dict):\n        self.gyp_path = gyp_path\n        self.path = path\n        self.project = gyp.xcodeproj_file.PBXProject(path=path)\n        projectDirPath = gyp.common.RelativePath(\n            os.path.dirname(os.path.abspath(self.gyp_path)),\n            os.path.dirname(path) or \".\",\n        )\n        self.project.SetProperty(\"projectDirPath\", projectDirPath)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "CreateXCConfigurationList",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "def CreateXCConfigurationList(configuration_names):\n    xccl = gyp.xcodeproj_file.XCConfigurationList({\"buildConfigurations\": []})\n    if len(configuration_names) == 0:\n        configuration_names = [\"Default\"]\n    for configuration_name in configuration_names:\n        xcbc = gyp.xcodeproj_file.XCBuildConfiguration({\"name\": configuration_name})\n        xccl.AppendProperty(\"buildConfigurations\", xcbc)\n    xccl.SetProperty(\"defaultConfigurationName\", configuration_names[0])\n    return xccl\nclass XcodeProject:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "AddSourceToTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "def AddSourceToTarget(source, type, pbxp, xct):\n    # TODO(mark): Perhaps source_extensions and library_extensions can be made a\n    # little bit fancier.\n    source_extensions = [\"c\", \"cc\", \"cpp\", \"cxx\", \"m\", \"mm\", \"s\", \"swift\"]\n    # .o is conceptually more of a \"source\" than a \"library,\" but Xcode thinks\n    # of \"sources\" as things to compile and \"libraries\" (or \"frameworks\") as\n    # things to link with. Adding an object file to an Xcode target's frameworks\n    # phase works properly.\n    library_extensions = [\"a\", \"dylib\", \"framework\", \"o\"]\n    basename = posixpath.basename(source)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "AddResourceToTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "def AddResourceToTarget(resource, pbxp, xct):\n    # TODO(mark): Combine with AddSourceToTarget above?  Or just inline this call\n    # where it's used.\n    xct.ResourcesPhase().AddFile(resource)\ndef AddHeaderToTarget(header, pbxp, xct, is_public):\n    # TODO(mark): Combine with AddSourceToTarget above?  Or just inline this call\n    # where it's used.\n    settings = \"{ATTRIBUTES = (%s, ); }\" % (\"Private\", \"Public\")[is_public]\n    xct.HeadersPhase().AddFile(header, settings)\n_xcode_variable_re = re.compile(r\"(\\$\\((.*?)\\))\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "AddHeaderToTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "def AddHeaderToTarget(header, pbxp, xct, is_public):\n    # TODO(mark): Combine with AddSourceToTarget above?  Or just inline this call\n    # where it's used.\n    settings = \"{ATTRIBUTES = (%s, ); }\" % (\"Private\", \"Public\")[is_public]\n    xct.HeadersPhase().AddFile(header, settings)\n_xcode_variable_re = re.compile(r\"(\\$\\((.*?)\\))\")\ndef ExpandXcodeVariables(string, expansions):\n    \"\"\"Expands Xcode-style $(VARIABLES) in string per the expansions dict.\n    In some rare cases, it is appropriate to expand Xcode variables when a\n    project file is generated.  For any substring $(VAR) in string, if VAR is a",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "ExpandXcodeVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "def ExpandXcodeVariables(string, expansions):\n    \"\"\"Expands Xcode-style $(VARIABLES) in string per the expansions dict.\n    In some rare cases, it is appropriate to expand Xcode variables when a\n    project file is generated.  For any substring $(VAR) in string, if VAR is a\n    key in the expansions dict, $(VAR) will be replaced with expansions[VAR].\n    Any $(VAR) substring in string for which VAR is not a key in the expansions\n    dict will remain in the returned string.\n    \"\"\"\n    matches = _xcode_variable_re.findall(string)\n    if matches is None:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "EscapeXcodeDefine",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "def EscapeXcodeDefine(s):\n    \"\"\"We must escape the defines that we give to XCode so that it knows not to\n    split on spaces and to respect backslash and quote literals. However, we\n    must not quote the define, or Xcode will incorrectly interpret variables\n    especially $(inherited).\"\"\"\n    return re.sub(_xcode_define_re, r\"\\\\\\1\", s)\ndef PerformBuild(data, configurations, params):\n    options = params[\"options\"]\n    for build_file, build_file_dict in data.items():\n        (build_file_root, build_file_ext) = os.path.splitext(build_file)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "PerformBuild",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "def PerformBuild(data, configurations, params):\n    options = params[\"options\"]\n    for build_file, build_file_dict in data.items():\n        (build_file_root, build_file_ext) = os.path.splitext(build_file)\n        if build_file_ext != \".gyp\":\n            continue\n        xcodeproj_path = build_file_root + options.suffix + \".xcodeproj\"\n        if options.generator_output:\n            xcodeproj_path = os.path.join(options.generator_output, xcodeproj_path)\n    for config in configurations:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "CalculateGeneratorInputInfo",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "def CalculateGeneratorInputInfo(params):\n    toplevel = params[\"options\"].toplevel_dir\n    if params.get(\"flavor\") == \"ninja\":\n        generator_dir = os.path.relpath(params[\"options\"].generator_output or \".\")\n        output_dir = params.get(\"generator_flags\", {}).get(\"output_dir\", \"out\")\n        output_dir = os.path.normpath(os.path.join(generator_dir, output_dir))\n        qualified_out_dir = os.path.normpath(\n            os.path.join(toplevel, output_dir, \"gypfiles-xcode-ninja\")\n        )\n    else:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "GenerateOutput",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "def GenerateOutput(target_list, target_dicts, data, params):\n    # Optionally configure each spec to use ninja as the external builder.\n    ninja_wrapper = params.get(\"flavor\") == \"ninja\"\n    if ninja_wrapper:\n        (target_list, target_dicts, data) = gyp.xcode_ninja.CreateWrapper(\n            target_list, target_dicts, data, params\n        )\n    options = params[\"options\"]\n    generator_flags = params.get(\"generator_flags\", {})\n    parallel_builds = generator_flags.get(\"xcode_parallel_builds\", True)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "_intermediate_var",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "_intermediate_var = \"INTERMEDIATE_DIR\"\n# SHARED_INTERMEDIATE_DIR is the same, except that it is shared among all\n# targets that share the same BUILT_PRODUCTS_DIR.\n_shared_intermediate_var = \"SHARED_INTERMEDIATE_DIR\"\n_library_search_paths_var = \"LIBRARY_SEARCH_PATHS\"\ngenerator_default_variables = {\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"STATIC_LIB_PREFIX\": \"lib\",\n    \"SHARED_LIB_PREFIX\": \"lib\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "_shared_intermediate_var",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "_shared_intermediate_var = \"SHARED_INTERMEDIATE_DIR\"\n_library_search_paths_var = \"LIBRARY_SEARCH_PATHS\"\ngenerator_default_variables = {\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"STATIC_LIB_PREFIX\": \"lib\",\n    \"SHARED_LIB_PREFIX\": \"lib\",\n    \"STATIC_LIB_SUFFIX\": \".a\",\n    \"SHARED_LIB_SUFFIX\": \".dylib\",\n    # INTERMEDIATE_DIR is a place for targets to build up intermediate products.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "_library_search_paths_var",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "_library_search_paths_var = \"LIBRARY_SEARCH_PATHS\"\ngenerator_default_variables = {\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"STATIC_LIB_PREFIX\": \"lib\",\n    \"SHARED_LIB_PREFIX\": \"lib\",\n    \"STATIC_LIB_SUFFIX\": \".a\",\n    \"SHARED_LIB_SUFFIX\": \".dylib\",\n    # INTERMEDIATE_DIR is a place for targets to build up intermediate products.\n    # It is specific to each build environment.  It is only guaranteed to exist",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "generator_default_variables",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "generator_default_variables = {\n    \"EXECUTABLE_PREFIX\": \"\",\n    \"EXECUTABLE_SUFFIX\": \"\",\n    \"STATIC_LIB_PREFIX\": \"lib\",\n    \"SHARED_LIB_PREFIX\": \"lib\",\n    \"STATIC_LIB_SUFFIX\": \".a\",\n    \"SHARED_LIB_SUFFIX\": \".dylib\",\n    # INTERMEDIATE_DIR is a place for targets to build up intermediate products.\n    # It is specific to each build environment.  It is only guaranteed to exist\n    # and be constant within the context of a project, corresponding to a single",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "generator_additional_path_sections",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "generator_additional_path_sections = [\n    \"mac_bundle_resources\",\n    \"mac_framework_headers\",\n    \"mac_framework_private_headers\",\n    # 'mac_framework_dirs', input already handles _dirs endings.\n]\n# The Xcode-specific keys that exist on targets and aren't moved down to\n# configurations.\ngenerator_additional_non_configuration_keys = [\n    \"ios_app_extension\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "generator_additional_non_configuration_keys",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "generator_additional_non_configuration_keys = [\n    \"ios_app_extension\",\n    \"ios_watch_app\",\n    \"ios_watchkit_extension\",\n    \"mac_bundle\",\n    \"mac_bundle_resources\",\n    \"mac_framework_headers\",\n    \"mac_framework_private_headers\",\n    \"mac_xctest_bundle\",\n    \"mac_xcuitest_bundle\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "generator_extra_sources_for_rules",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "generator_extra_sources_for_rules = [\n    \"mac_bundle_resources\",\n    \"mac_framework_headers\",\n    \"mac_framework_private_headers\",\n]\ngenerator_filelist_paths = None\n# Xcode's standard set of library directories, which don't need to be duplicated\n# in LIBRARY_SEARCH_PATHS. This list is not exhaustive, but that's okay.\nxcode_standard_library_dirs = frozenset(\n    [\"$(SDKROOT)/usr/lib\", \"$(SDKROOT)/usr/local/lib\"]",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "generator_filelist_paths",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "generator_filelist_paths = None\n# Xcode's standard set of library directories, which don't need to be duplicated\n# in LIBRARY_SEARCH_PATHS. This list is not exhaustive, but that's okay.\nxcode_standard_library_dirs = frozenset(\n    [\"$(SDKROOT)/usr/lib\", \"$(SDKROOT)/usr/local/lib\"]\n)\ndef CreateXCConfigurationList(configuration_names):\n    xccl = gyp.xcodeproj_file.XCConfigurationList({\"buildConfigurations\": []})\n    if len(configuration_names) == 0:\n        configuration_names = [\"Default\"]",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "xcode_standard_library_dirs",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "xcode_standard_library_dirs = frozenset(\n    [\"$(SDKROOT)/usr/lib\", \"$(SDKROOT)/usr/local/lib\"]\n)\ndef CreateXCConfigurationList(configuration_names):\n    xccl = gyp.xcodeproj_file.XCConfigurationList({\"buildConfigurations\": []})\n    if len(configuration_names) == 0:\n        configuration_names = [\"Default\"]\n    for configuration_name in configuration_names:\n        xcbc = gyp.xcodeproj_file.XCBuildConfiguration({\"name\": configuration_name})\n        xccl.AppendProperty(\"buildConfigurations\", xcbc)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "file",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "file = open('$TMPDIR/GYP_serialize_test_runs', 'a')\nfcntl.flock(file.fileno(), fcntl.LOCK_EX)\nsys.exit(subprocess.call(sys.argv[1:]))\" \"\"\"\n                # If we were unable to exec for some reason, we want to exit\n                # with an error, and fixup variable references to be shell\n                # syntax instead of xcode syntax.\n                script = (\n                    script\n                    + \"exec \"\n                    + command_prefix",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "_xcode_variable_re",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "_xcode_variable_re = re.compile(r\"(\\$\\((.*?)\\))\")\ndef ExpandXcodeVariables(string, expansions):\n    \"\"\"Expands Xcode-style $(VARIABLES) in string per the expansions dict.\n    In some rare cases, it is appropriate to expand Xcode variables when a\n    project file is generated.  For any substring $(VAR) in string, if VAR is a\n    key in the expansions dict, $(VAR) will be replaced with expansions[VAR].\n    Any $(VAR) substring in string for which VAR is not a key in the expansions\n    dict will remain in the returned string.\n    \"\"\"\n    matches = _xcode_variable_re.findall(string)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "_xcode_define_re",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "peekOfCode": "_xcode_define_re = re.compile(r\"([\\\\\\\"\\' ])\")\ndef EscapeXcodeDefine(s):\n    \"\"\"We must escape the defines that we give to XCode so that it knows not to\n    split on spaces and to respect backslash and quote literals. However, we\n    must not quote the define, or Xcode will incorrectly interpret variables\n    especially $(inherited).\"\"\"\n    return re.sub(_xcode_define_re, r\"\\\\\\1\", s)\ndef PerformBuild(data, configurations, params):\n    options = params[\"options\"]\n    for build_file, build_file_dict in data.items():",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode",
        "documentation": {}
    },
    {
        "label": "TestEscapeXcodeDefine",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode_test",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode_test",
        "peekOfCode": "class TestEscapeXcodeDefine(unittest.TestCase):\n    if sys.platform == \"darwin\":\n        def test_InheritedRemainsUnescaped(self):\n            self.assertEqual(xcode.EscapeXcodeDefine(\"$(inherited)\"), \"$(inherited)\")\n        def test_Escaping(self):\n            self.assertEqual(xcode.EscapeXcodeDefine('a b\"c\\\\'), 'a\\\\ b\\\\\"c\\\\\\\\')\nif __name__ == \"__main__\":\n    unittest.main()",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.generator.xcode_test",
        "documentation": {}
    },
    {
        "label": "MSVSSolutionEntry",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "peekOfCode": "class MSVSSolutionEntry:\n    def __cmp__(self, other):\n        # Sort by name then guid (so things are in order on vs2008).\n        return cmp((self.name, self.get_guid()), (other.name, other.get_guid()))\nclass MSVSFolder(MSVSSolutionEntry):\n    \"\"\"Folder in a Visual Studio project or solution.\"\"\"\n    def __init__(self, path, name=None, entries=None, guid=None, items=None):\n        \"\"\"Initializes the folder.\n        Args:\n          path: Full path to the folder.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "documentation": {}
    },
    {
        "label": "MSVSFolder",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "peekOfCode": "class MSVSFolder(MSVSSolutionEntry):\n    \"\"\"Folder in a Visual Studio project or solution.\"\"\"\n    def __init__(self, path, name=None, entries=None, guid=None, items=None):\n        \"\"\"Initializes the folder.\n        Args:\n          path: Full path to the folder.\n          name: Name of the folder.\n          entries: List of folder entries to nest inside this folder.  May contain\n              Folder or Project objects.  May be None, if the folder is empty.\n          guid: GUID to use for folder, if not None.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "documentation": {}
    },
    {
        "label": "MSVSProject",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "peekOfCode": "class MSVSProject(MSVSSolutionEntry):\n    \"\"\"Visual Studio project.\"\"\"\n    def __init__(\n        self,\n        path,\n        name=None,\n        dependencies=None,\n        guid=None,\n        spec=None,\n        build_file=None,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "documentation": {}
    },
    {
        "label": "MSVSSolution",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "peekOfCode": "class MSVSSolution:\n    \"\"\"Visual Studio solution.\"\"\"\n    def __init__(\n        self, path, version, entries=None, variants=None, websiteProperties=True\n    ):\n        \"\"\"Initializes the solution.\n        Args:\n          path: Path to solution file.\n          version: Format version to emit.\n          entries: List of entries in solution.  May contain Folder or Project",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "documentation": {}
    },
    {
        "label": "cmp",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "peekOfCode": "def cmp(x, y):\n    return (x > y) - (x < y)\n# Initialize random number generator\nrandom.seed()\n# GUIDs for project types\nENTRY_TYPE_GUIDS = {\n    \"project\": \"{8BC9CEB8-8B4A-11D0-8D11-00A0C91BC942}\",\n    \"folder\": \"{2150E333-8FDC-42A3-9474-1A3956D46DE8}\",\n}\n# ------------------------------------------------------------------------------",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "documentation": {}
    },
    {
        "label": "MakeGuid",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "peekOfCode": "def MakeGuid(name, seed=\"msvs_new\"):\n    \"\"\"Returns a GUID for the specified target name.\n    Args:\n      name: Target name.\n      seed: Seed for MD5 hash.\n    Returns:\n      A GUID-line string calculated from the name and seed.\n    This generates something which looks like a GUID, but depends only on the\n    name and seed.  This means the same name/seed will always generate the same\n    GUID, so that projects and solutions which refer to each other can explicitly",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "documentation": {}
    },
    {
        "label": "ENTRY_TYPE_GUIDS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "peekOfCode": "ENTRY_TYPE_GUIDS = {\n    \"project\": \"{8BC9CEB8-8B4A-11D0-8D11-00A0C91BC942}\",\n    \"folder\": \"{2150E333-8FDC-42A3-9474-1A3956D46DE8}\",\n}\n# ------------------------------------------------------------------------------\n# Helper functions\ndef MakeGuid(name, seed=\"msvs_new\"):\n    \"\"\"Returns a GUID for the specified target name.\n    Args:\n      name: Target name.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSNew",
        "documentation": {}
    },
    {
        "label": "Tool",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSProject",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSProject",
        "peekOfCode": "class Tool:\n    \"\"\"Visual Studio tool.\"\"\"\n    def __init__(self, name, attrs=None):\n        \"\"\"Initializes the tool.\n        Args:\n          name: Tool name.\n          attrs: Dict of tool attributes; may be None.\n        \"\"\"\n        self._attrs = attrs or {}\n        self._attrs[\"Name\"] = name",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSProject",
        "documentation": {}
    },
    {
        "label": "Filter",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSProject",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSProject",
        "peekOfCode": "class Filter:\n    \"\"\"Visual Studio filter - that is, a virtual folder.\"\"\"\n    def __init__(self, name, contents=None):\n        \"\"\"Initializes the folder.\n        Args:\n          name: Filter (folder) name.\n          contents: List of filenames and/or Filter objects contained.\n        \"\"\"\n        self.name = name\n        self.contents = list(contents or [])",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSProject",
        "documentation": {}
    },
    {
        "label": "Writer",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSProject",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSProject",
        "peekOfCode": "class Writer:\n    \"\"\"Visual Studio XML project writer.\"\"\"\n    def __init__(self, project_path, version, name, guid=None, platforms=None):\n        \"\"\"Initializes the project.\n        Args:\n          project_path: Path to the project file.\n          version: Format version to emit.\n          name: Name of the project.\n          guid: GUID to use for project, if not None.\n          platforms: Array of string, the supported platforms.  If null, ['Win32']",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSProject",
        "documentation": {}
    },
    {
        "label": "_Tool",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "class _Tool:\n    \"\"\"Represents a tool used by MSVS or MSBuild.\n    Attributes:\n        msvs_name: The name of the tool in MSVS.\n        msbuild_name: The name of the tool in MSBuild.\n    \"\"\"\n    def __init__(self, msvs_name, msbuild_name):\n        self.msvs_name = msvs_name\n        self.msbuild_name = msbuild_name\ndef _AddTool(tool):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_Type",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "class _Type:\n    \"\"\"Type of settings (Base class).\"\"\"\n    def ValidateMSVS(self, value):\n        \"\"\"Verifies that the value is legal for MSVS.\n        Args:\n          value: the value to check for this type.\n        Raises:\n          ValueError if value is not valid for MSVS.\n        \"\"\"\n    def ValidateMSBuild(self, value):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "class _String(_Type):\n    \"\"\"A setting that's just a string.\"\"\"\n    def ValidateMSVS(self, value):\n        if not isinstance(value, str):\n            raise ValueError(\"expected string; got %r\" % value)\n    def ValidateMSBuild(self, value):\n        if not isinstance(value, str):\n            raise ValueError(\"expected string; got %r\" % value)\n    def ConvertToMSBuild(self, value):\n        # Convert the macros",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_StringList",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "class _StringList(_Type):\n    \"\"\"A settings that's a list of strings.\"\"\"\n    def ValidateMSVS(self, value):\n        if not isinstance(value, (list, str)):\n            raise ValueError(\"expected string list; got %r\" % value)\n    def ValidateMSBuild(self, value):\n        if not isinstance(value, (list, str)):\n            raise ValueError(\"expected string list; got %r\" % value)\n    def ConvertToMSBuild(self, value):\n        # Convert the macros",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_Boolean",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "class _Boolean(_Type):\n    \"\"\"Boolean settings, can have the values 'false' or 'true'.\"\"\"\n    def _Validate(self, value):\n        if value not in {\"true\", \"false\"}:\n            raise ValueError(\"expected bool; got %r\" % value)\n    def ValidateMSVS(self, value):\n        self._Validate(value)\n    def ValidateMSBuild(self, value):\n        self._Validate(value)\n    def ConvertToMSBuild(self, value):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_Integer",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "class _Integer(_Type):\n    \"\"\"Integer settings.\"\"\"\n    def __init__(self, msbuild_base=10):\n        _Type.__init__(self)\n        self._msbuild_base = msbuild_base\n    def ValidateMSVS(self, value):\n        # Try to convert, this will raise ValueError if invalid.\n        self.ConvertToMSBuild(value)\n    def ValidateMSBuild(self, value):\n        # Try to convert, this will raise ValueError if invalid.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_Enumeration",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "class _Enumeration(_Type):\n    \"\"\"Type of settings that is an enumeration.\n    In MSVS, the values are indexes like '0', '1', and '2'.\n    MSBuild uses text labels that are more representative, like 'Win32'.\n    Constructor args:\n      label_list: an array of MSBuild labels that correspond to the MSVS index.\n          In the rare cases where MSVS has skipped an index value, None is\n          used in the array to indicate the unused spot.\n      new: an array of labels that are new to MSBuild.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "FixVCMacroSlashes",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "def FixVCMacroSlashes(s):\n    \"\"\"Replace macros which have excessive following slashes.\n    These macros are known to have a built-in trailing slash. Furthermore, many\n    scripts hiccup on processing paths with extra slashes in the middle.\n    This list is probably not exhaustive.  Add as needed.\n    \"\"\"\n    if \"$\" in s:\n        s = fix_vc_macro_slashes_regex.sub(r\"\\1\", s)\n    return s\ndef ConvertVCMacrosToMSBuild(s):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "ConvertVCMacrosToMSBuild",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "def ConvertVCMacrosToMSBuild(s):\n    \"\"\"Convert the MSVS macros found in the string to the MSBuild equivalent.\n    This list is probably not exhaustive.  Add as needed.\n    \"\"\"\n    if \"$\" in s:\n        replace_map = {\n            \"$(ConfigurationName)\": \"$(Configuration)\",\n            \"$(InputDir)\": \"%(RelativeDir)\",\n            \"$(InputExt)\": \"%(Extension)\",\n            \"$(InputFileName)\": \"%(Filename)%(Extension)\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "ConvertToMSBuildSettings",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "def ConvertToMSBuildSettings(msvs_settings, stderr=sys.stderr):\n    \"\"\"Converts MSVS settings (VS2008 and earlier) to MSBuild settings (VS2010+).\n    Args:\n        msvs_settings: A dictionary.  The key is the tool name.  The values are\n            themselves dictionaries of settings and their values.\n        stderr: The stream receiving the error messages.\n    Returns:\n        A dictionary of MSBuild settings.  The key is either the MSBuild tool name\n        or the empty string (for the global settings).  The values are themselves\n        dictionaries of settings and their values.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "ValidateMSVSSettings",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "def ValidateMSVSSettings(settings, stderr=sys.stderr):\n    \"\"\"Validates that the names of the settings are valid for MSVS.\n    Args:\n        settings: A dictionary.  The key is the tool name.  The values are\n            themselves dictionaries of settings and their values.\n        stderr: The stream receiving the error messages.\n    \"\"\"\n    _ValidateSettings(_msvs_validators, settings, stderr)\ndef ValidateMSBuildSettings(settings, stderr=sys.stderr):\n    \"\"\"Validates that the names of the settings are valid for MSBuild.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "ValidateMSBuildSettings",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "def ValidateMSBuildSettings(settings, stderr=sys.stderr):\n    \"\"\"Validates that the names of the settings are valid for MSBuild.\n    Args:\n        settings: A dictionary.  The key is the tool name.  The values are\n            themselves dictionaries of settings and their values.\n        stderr: The stream receiving the error messages.\n    \"\"\"\n    _ValidateSettings(_msbuild_validators, settings, stderr)\ndef _ValidateSettings(validators, settings, stderr):\n    \"\"\"Validates that the settings are valid for MSBuild or MSVS.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_msvs_validators",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_msvs_validators = {}\n_msbuild_validators = {}\n# A dictionary of settings converters. The key is the tool name, the value is\n# a dictionary mapping setting names to conversion functions.\n_msvs_to_msbuild_converters = {}\n# Tool name mapping from MSVS to MSBuild.\n_msbuild_name_of_tool = {}\nclass _Tool:\n    \"\"\"Represents a tool used by MSVS or MSBuild.\n    Attributes:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_msbuild_validators",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_msbuild_validators = {}\n# A dictionary of settings converters. The key is the tool name, the value is\n# a dictionary mapping setting names to conversion functions.\n_msvs_to_msbuild_converters = {}\n# Tool name mapping from MSVS to MSBuild.\n_msbuild_name_of_tool = {}\nclass _Tool:\n    \"\"\"Represents a tool used by MSVS or MSBuild.\n    Attributes:\n        msvs_name: The name of the tool in MSVS.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_msvs_to_msbuild_converters",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_msvs_to_msbuild_converters = {}\n# Tool name mapping from MSVS to MSBuild.\n_msbuild_name_of_tool = {}\nclass _Tool:\n    \"\"\"Represents a tool used by MSVS or MSBuild.\n    Attributes:\n        msvs_name: The name of the tool in MSVS.\n        msbuild_name: The name of the tool in MSBuild.\n    \"\"\"\n    def __init__(self, msvs_name, msbuild_name):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_msbuild_name_of_tool",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_msbuild_name_of_tool = {}\nclass _Tool:\n    \"\"\"Represents a tool used by MSVS or MSBuild.\n    Attributes:\n        msvs_name: The name of the tool in MSVS.\n        msbuild_name: The name of the tool in MSBuild.\n    \"\"\"\n    def __init__(self, msvs_name, msbuild_name):\n        self.msvs_name = msvs_name\n        self.msbuild_name = msbuild_name",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_boolean",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_boolean = _Boolean()\n_integer = _Integer()\n# For now, we don't do any special validation on these types:\n_string = _String()\n_file_name = _String()\n_folder_name = _String()\n_file_list = _StringList()\n_folder_list = _StringList()\n_string_list = _StringList()\n# Some boolean settings went from numerical values to boolean.  The",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_integer",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_integer = _Integer()\n# For now, we don't do any special validation on these types:\n_string = _String()\n_file_name = _String()\n_folder_name = _String()\n_file_list = _StringList()\n_folder_list = _StringList()\n_string_list = _StringList()\n# Some boolean settings went from numerical values to boolean.  The\n# mapping is 0: default, 1: false, 2: true.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_string",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_string = _String()\n_file_name = _String()\n_folder_name = _String()\n_file_list = _StringList()\n_folder_list = _StringList()\n_string_list = _StringList()\n# Some boolean settings went from numerical values to boolean.  The\n# mapping is 0: default, 1: false, 2: true.\n_newly_boolean = _Enumeration([\"\", \"false\", \"true\"])\ndef _Same(tool, name, setting_type):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_file_name",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_file_name = _String()\n_folder_name = _String()\n_file_list = _StringList()\n_folder_list = _StringList()\n_string_list = _StringList()\n# Some boolean settings went from numerical values to boolean.  The\n# mapping is 0: default, 1: false, 2: true.\n_newly_boolean = _Enumeration([\"\", \"false\", \"true\"])\ndef _Same(tool, name, setting_type):\n    \"\"\"Defines a setting that has the same name in MSVS and MSBuild.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_folder_name",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_folder_name = _String()\n_file_list = _StringList()\n_folder_list = _StringList()\n_string_list = _StringList()\n# Some boolean settings went from numerical values to boolean.  The\n# mapping is 0: default, 1: false, 2: true.\n_newly_boolean = _Enumeration([\"\", \"false\", \"true\"])\ndef _Same(tool, name, setting_type):\n    \"\"\"Defines a setting that has the same name in MSVS and MSBuild.\n    Args:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_file_list",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_file_list = _StringList()\n_folder_list = _StringList()\n_string_list = _StringList()\n# Some boolean settings went from numerical values to boolean.  The\n# mapping is 0: default, 1: false, 2: true.\n_newly_boolean = _Enumeration([\"\", \"false\", \"true\"])\ndef _Same(tool, name, setting_type):\n    \"\"\"Defines a setting that has the same name in MSVS and MSBuild.\n    Args:\n      tool: a dictionary that gives the names of the tool for MSVS and MSBuild.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_folder_list",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_folder_list = _StringList()\n_string_list = _StringList()\n# Some boolean settings went from numerical values to boolean.  The\n# mapping is 0: default, 1: false, 2: true.\n_newly_boolean = _Enumeration([\"\", \"false\", \"true\"])\ndef _Same(tool, name, setting_type):\n    \"\"\"Defines a setting that has the same name in MSVS and MSBuild.\n    Args:\n      tool: a dictionary that gives the names of the tool for MSVS and MSBuild.\n      name: the name of the setting.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_string_list",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_string_list = _StringList()\n# Some boolean settings went from numerical values to boolean.  The\n# mapping is 0: default, 1: false, 2: true.\n_newly_boolean = _Enumeration([\"\", \"false\", \"true\"])\ndef _Same(tool, name, setting_type):\n    \"\"\"Defines a setting that has the same name in MSVS and MSBuild.\n    Args:\n      tool: a dictionary that gives the names of the tool for MSVS and MSBuild.\n      name: the name of the setting.\n      setting_type: the type of this setting.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_newly_boolean",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_newly_boolean = _Enumeration([\"\", \"false\", \"true\"])\ndef _Same(tool, name, setting_type):\n    \"\"\"Defines a setting that has the same name in MSVS and MSBuild.\n    Args:\n      tool: a dictionary that gives the names of the tool for MSVS and MSBuild.\n      name: the name of the setting.\n      setting_type: the type of this setting.\n    \"\"\"\n    _Renamed(tool, name, name, setting_type)\ndef _Renamed(tool, msvs_name, msbuild_name, setting_type):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "fix_vc_macro_slashes_regex_list",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "fix_vc_macro_slashes_regex_list = (\"IntDir\", \"OutDir\")\nfix_vc_macro_slashes_regex = re.compile(\n    r\"(\\$\\((?:%s)\\))(?:[\\\\/]+)\" % \"|\".join(fix_vc_macro_slashes_regex_list)\n)\n# Regular expression to detect keys that were generated by exclusion lists\n_EXCLUDED_SUFFIX_RE = re.compile(\"^(.*)_excluded$\")\ndef _ValidateExclusionSetting(setting, settings, error_msg, stderr=sys.stderr):\n    \"\"\"Verify that 'setting' is valid if it is generated from an exclusion list.\n    If the setting appears to be generated from an exclusion list, the root name\n    is checked.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "fix_vc_macro_slashes_regex",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "fix_vc_macro_slashes_regex = re.compile(\n    r\"(\\$\\((?:%s)\\))(?:[\\\\/]+)\" % \"|\".join(fix_vc_macro_slashes_regex_list)\n)\n# Regular expression to detect keys that were generated by exclusion lists\n_EXCLUDED_SUFFIX_RE = re.compile(\"^(.*)_excluded$\")\ndef _ValidateExclusionSetting(setting, settings, error_msg, stderr=sys.stderr):\n    \"\"\"Verify that 'setting' is valid if it is generated from an exclusion list.\n    If the setting appears to be generated from an exclusion list, the root name\n    is checked.\n    Args:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_EXCLUDED_SUFFIX_RE",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_EXCLUDED_SUFFIX_RE = re.compile(\"^(.*)_excluded$\")\ndef _ValidateExclusionSetting(setting, settings, error_msg, stderr=sys.stderr):\n    \"\"\"Verify that 'setting' is valid if it is generated from an exclusion list.\n    If the setting appears to be generated from an exclusion list, the root name\n    is checked.\n    Args:\n        setting:   A string that is the setting name to validate\n        settings:  A dictionary where the keys are valid settings\n        error_msg: The message to emit in the event of error\n        stderr:    The stream receiving the error messages.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_compile",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_compile = _Tool(\"VCCLCompilerTool\", \"ClCompile\")\n_link = _Tool(\"VCLinkerTool\", \"Link\")\n_midl = _Tool(\"VCMIDLTool\", \"Midl\")\n_rc = _Tool(\"VCResourceCompilerTool\", \"ResourceCompile\")\n_lib = _Tool(\"VCLibrarianTool\", \"Lib\")\n_manifest = _Tool(\"VCManifestTool\", \"Manifest\")\n_masm = _Tool(\"MASM\", \"MASM\")\n_armasm = _Tool(\"ARMASM\", \"ARMASM\")\n_AddTool(_compile)\n_AddTool(_link)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_link",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_link = _Tool(\"VCLinkerTool\", \"Link\")\n_midl = _Tool(\"VCMIDLTool\", \"Midl\")\n_rc = _Tool(\"VCResourceCompilerTool\", \"ResourceCompile\")\n_lib = _Tool(\"VCLibrarianTool\", \"Lib\")\n_manifest = _Tool(\"VCManifestTool\", \"Manifest\")\n_masm = _Tool(\"MASM\", \"MASM\")\n_armasm = _Tool(\"ARMASM\", \"ARMASM\")\n_AddTool(_compile)\n_AddTool(_link)\n_AddTool(_midl)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_midl",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_midl = _Tool(\"VCMIDLTool\", \"Midl\")\n_rc = _Tool(\"VCResourceCompilerTool\", \"ResourceCompile\")\n_lib = _Tool(\"VCLibrarianTool\", \"Lib\")\n_manifest = _Tool(\"VCManifestTool\", \"Manifest\")\n_masm = _Tool(\"MASM\", \"MASM\")\n_armasm = _Tool(\"ARMASM\", \"ARMASM\")\n_AddTool(_compile)\n_AddTool(_link)\n_AddTool(_midl)\n_AddTool(_rc)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_rc",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_rc = _Tool(\"VCResourceCompilerTool\", \"ResourceCompile\")\n_lib = _Tool(\"VCLibrarianTool\", \"Lib\")\n_manifest = _Tool(\"VCManifestTool\", \"Manifest\")\n_masm = _Tool(\"MASM\", \"MASM\")\n_armasm = _Tool(\"ARMASM\", \"ARMASM\")\n_AddTool(_compile)\n_AddTool(_link)\n_AddTool(_midl)\n_AddTool(_rc)\n_AddTool(_lib)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_lib",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_lib = _Tool(\"VCLibrarianTool\", \"Lib\")\n_manifest = _Tool(\"VCManifestTool\", \"Manifest\")\n_masm = _Tool(\"MASM\", \"MASM\")\n_armasm = _Tool(\"ARMASM\", \"ARMASM\")\n_AddTool(_compile)\n_AddTool(_link)\n_AddTool(_midl)\n_AddTool(_rc)\n_AddTool(_lib)\n_AddTool(_manifest)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_manifest",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_manifest = _Tool(\"VCManifestTool\", \"Manifest\")\n_masm = _Tool(\"MASM\", \"MASM\")\n_armasm = _Tool(\"ARMASM\", \"ARMASM\")\n_AddTool(_compile)\n_AddTool(_link)\n_AddTool(_midl)\n_AddTool(_rc)\n_AddTool(_lib)\n_AddTool(_manifest)\n_AddTool(_masm)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_masm",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_masm = _Tool(\"MASM\", \"MASM\")\n_armasm = _Tool(\"ARMASM\", \"ARMASM\")\n_AddTool(_compile)\n_AddTool(_link)\n_AddTool(_midl)\n_AddTool(_rc)\n_AddTool(_lib)\n_AddTool(_manifest)\n_AddTool(_masm)\n_AddTool(_armasm)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_armasm",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_armasm = _Tool(\"ARMASM\", \"ARMASM\")\n_AddTool(_compile)\n_AddTool(_link)\n_AddTool(_midl)\n_AddTool(_rc)\n_AddTool(_lib)\n_AddTool(_manifest)\n_AddTool(_masm)\n_AddTool(_armasm)\n# Add sections only found in the MSBuild settings.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_msbuild_validators[\"\"]",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_msbuild_validators[\"\"] = {}\n_msbuild_validators[\"ProjectReference\"] = {}\n_msbuild_validators[\"ManifestResourceCompile\"] = {}\n# Descriptions of the compiler options, i.e. VCCLCompilerTool in MSVS and\n# ClCompile in MSBuild.\n# See \"c:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\1033\\cl.xml\" for\n# the schema of the MSBuild ClCompile settings.\n# Options that have the same name in MSVS and MSBuild\n_Same(_compile, \"AdditionalIncludeDirectories\", _folder_list)  # /I\n_Same(_compile, \"AdditionalOptions\", _string_list)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_msbuild_validators[\"ProjectReference\"]",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_msbuild_validators[\"ProjectReference\"] = {}\n_msbuild_validators[\"ManifestResourceCompile\"] = {}\n# Descriptions of the compiler options, i.e. VCCLCompilerTool in MSVS and\n# ClCompile in MSBuild.\n# See \"c:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\1033\\cl.xml\" for\n# the schema of the MSBuild ClCompile settings.\n# Options that have the same name in MSVS and MSBuild\n_Same(_compile, \"AdditionalIncludeDirectories\", _folder_list)  # /I\n_Same(_compile, \"AdditionalOptions\", _string_list)\n_Same(_compile, \"AdditionalUsingDirectories\", _folder_list)  # /AI",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_msbuild_validators[\"ManifestResourceCompile\"]",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_msbuild_validators[\"ManifestResourceCompile\"] = {}\n# Descriptions of the compiler options, i.e. VCCLCompilerTool in MSVS and\n# ClCompile in MSBuild.\n# See \"c:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\1033\\cl.xml\" for\n# the schema of the MSBuild ClCompile settings.\n# Options that have the same name in MSVS and MSBuild\n_Same(_compile, \"AdditionalIncludeDirectories\", _folder_list)  # /I\n_Same(_compile, \"AdditionalOptions\", _string_list)\n_Same(_compile, \"AdditionalUsingDirectories\", _folder_list)  # /AI\n_Same(_compile, \"AssemblerListingLocation\", _file_name)  # /Fa",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_subsystem_enumeration",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_subsystem_enumeration = _Enumeration(\n    [\n        \"NotSet\",\n        \"Console\",  # /SUBSYSTEM:CONSOLE\n        \"Windows\",  # /SUBSYSTEM:WINDOWS\n        \"Native\",  # /SUBSYSTEM:NATIVE\n        \"EFI Application\",  # /SUBSYSTEM:EFI_APPLICATION\n        \"EFI Boot Service Driver\",  # /SUBSYSTEM:EFI_BOOT_SERVICE_DRIVER\n        \"EFI ROM\",  # /SUBSYSTEM:EFI_ROM\n        \"EFI Runtime\",  # /SUBSYSTEM:EFI_RUNTIME_DRIVER",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "_target_machine_enumeration",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "peekOfCode": "_target_machine_enumeration = _Enumeration(\n    [\n        \"NotSet\",\n        \"MachineX86\",  # /MACHINE:X86\n        None,\n        \"MachineARM\",  # /MACHINE:ARM\n        \"MachineEBC\",  # /MACHINE:EBC\n        \"MachineIA64\",  # /MACHINE:IA64\n        None,\n        \"MachineMIPS\",  # /MACHINE:MIPS",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings",
        "documentation": {}
    },
    {
        "label": "TestSequenceFunctions",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings_test",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings_test",
        "peekOfCode": "class TestSequenceFunctions(unittest.TestCase):\n    def setUp(self):\n        self.stderr = StringIO()\n    def _ExpectedWarnings(self, expected):\n        \"\"\"Compares recorded lines to expected warnings.\"\"\"\n        self.stderr.seek(0)\n        actual = self.stderr.read().split(\"\\n\")\n        actual = [line for line in actual if line]\n        self.assertEqual(sorted(expected), sorted(actual))\n    def testValidateMSVSSettings_tool_names(self):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSSettings_test",
        "documentation": {}
    },
    {
        "label": "Writer",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSToolFile",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSToolFile",
        "peekOfCode": "class Writer:\n    \"\"\"Visual Studio XML tool file writer.\"\"\"\n    def __init__(self, tool_file_path, name):\n        \"\"\"Initializes the tool file.\n        Args:\n          tool_file_path: Path to the tool file.\n          name: Name of the tool file.\n        \"\"\"\n        self.tool_file_path = tool_file_path\n        self.name = name",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSToolFile",
        "documentation": {}
    },
    {
        "label": "Writer",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUserFile",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUserFile",
        "peekOfCode": "class Writer:\n    \"\"\"Visual Studio XML user user file writer.\"\"\"\n    def __init__(self, user_file_path, version, name):\n        \"\"\"Initializes the user file.\n        Args:\n          user_file_path: Path to the user file.\n          version: Version info.\n          name: Name of the user file.\n        \"\"\"\n        self.user_file_path = user_file_path",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUserFile",
        "documentation": {}
    },
    {
        "label": "ShardTargets",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUtil",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUtil",
        "peekOfCode": "def ShardTargets(target_list, target_dicts):\n    \"\"\"Shard some targets apart to work around the linkers limits.\n    Arguments:\n      target_list: List of target pairs: 'base/base.gyp:base'.\n      target_dicts: Dict of target properties keyed on target pair.\n    Returns:\n      Tuple of the new sharded versions of the inputs.\n    \"\"\"\n    # Gather the targets to shard, and how many pieces.\n    targets_to_shard = {}",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUtil",
        "documentation": {}
    },
    {
        "label": "InsertLargePdbShims",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUtil",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUtil",
        "peekOfCode": "def InsertLargePdbShims(target_list, target_dicts, vars):\n    \"\"\"Insert a shim target that forces the linker to use 4KB pagesize PDBs.\n    This is a workaround for targets with PDBs greater than 1GB in size, the\n    limit for the 1KB pagesize PDBs created by the linker by default.\n    Arguments:\n      target_list: List of target pairs: 'base/base.gyp:base'.\n      target_dicts: Dict of target properties keyed on target pair.\n      vars: A dictionary of common GYP variables with generator-specific values.\n    Returns:\n      Tuple of the shimmed version of the inputs.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUtil",
        "documentation": {}
    },
    {
        "label": "TARGET_TYPE_EXT",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUtil",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUtil",
        "peekOfCode": "TARGET_TYPE_EXT = {\n    \"executable\": \"exe\",\n    \"loadable_module\": \"dll\",\n    \"shared_library\": \"dll\",\n    \"static_library\": \"lib\",\n    \"windows_driver\": \"sys\",\n}\ndef _GetLargePdbShimCcPath():\n    \"\"\"Returns the path of the large_pdb_shim.cc file.\"\"\"\n    this_dir = os.path.abspath(os.path.dirname(__file__))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSUtil",
        "documentation": {}
    },
    {
        "label": "VisualStudioVersion",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSVersion",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSVersion",
        "peekOfCode": "class VisualStudioVersion:\n    \"\"\"Information regarding a version of Visual Studio.\"\"\"\n    def __init__(\n        self,\n        short_name,\n        description,\n        solution_version,\n        project_version,\n        flat_sln,\n        uses_vcxproj,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSVersion",
        "documentation": {}
    },
    {
        "label": "JoinPath",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSVersion",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSVersion",
        "peekOfCode": "def JoinPath(*args):\n    return os.path.normpath(os.path.join(*args))\nclass VisualStudioVersion:\n    \"\"\"Information regarding a version of Visual Studio.\"\"\"\n    def __init__(\n        self,\n        short_name,\n        description,\n        solution_version,\n        project_version,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSVersion",
        "documentation": {}
    },
    {
        "label": "SelectVisualStudioVersion",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSVersion",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSVersion",
        "peekOfCode": "def SelectVisualStudioVersion(version=\"auto\", allow_fallback=True):\n    \"\"\"Select which version of Visual Studio projects to generate.\n    Arguments:\n      version: Hook to allow caller to force a particular version (vs auto).\n    Returns:\n      An object representing a visual studio project format version.\n    \"\"\"\n    # In auto mode, check environment variable for override.\n    if version == \"auto\":\n        version = os.environ.get(\"GYP_MSVS_VERSION\", \"auto\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.MSVSVersion",
        "documentation": {}
    },
    {
        "label": "memoize",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "class memoize:\n    def __init__(self, func):\n        self.func = func\n        self.cache = {}\n    def __call__(self, *args):\n        try:\n            return self.cache[args]\n        except KeyError:\n            result = self.func(*args)\n            self.cache[args] = result",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "GypError",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "class GypError(Exception):\n    \"\"\"Error class representing an error, which is to be presented\n    to the user.  The main entry point will catch and display this.\n    \"\"\"\ndef ExceptionAppend(e, msg):\n    \"\"\"Append a message to the given exception's message.\"\"\"\n    if not e.args:\n        e.args = (msg,)\n    elif len(e.args) == 1:\n        e.args = (str(e.args[0]) + \" \" + msg,)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "OrderedSet",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "class OrderedSet(MutableSet):  # noqa: PLW1641\n    # TODO (cclauss): Fix eq-without-hash ruff rule PLW1641\n    def __init__(self, iterable=None):\n        self.end = end = []\n        end += [None, end, end]  # sentinel node for doubly linked list\n        self.map = {}  # key --> [key, prev, next]\n        if iterable is not None:\n            self |= iterable\n    def __len__(self):\n        return len(self.map)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "CycleError",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "class CycleError(Exception):\n    \"\"\"An exception raised when an unexpected cycle is detected.\"\"\"\n    def __init__(self, nodes):\n        self.nodes = nodes\n    def __str__(self):\n        return \"CycleError: cycle involving: \" + str(self.nodes)\ndef TopologicallySorted(graph, get_edges):\n    r\"\"\"Topologically sort based on a user provided edge definition.\n    Args:\n      graph: A list of node names.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "ExceptionAppend",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def ExceptionAppend(e, msg):\n    \"\"\"Append a message to the given exception's message.\"\"\"\n    if not e.args:\n        e.args = (msg,)\n    elif len(e.args) == 1:\n        e.args = (str(e.args[0]) + \" \" + msg,)\n    else:\n        e.args = (str(e.args[0]) + \" \" + msg,) + e.args[1:]\ndef FindQualifiedTargets(target, qualified_list):\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "FindQualifiedTargets",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def FindQualifiedTargets(target, qualified_list):\n    \"\"\"\n    Given a list of qualified targets, return the qualified targets for the\n    specified |target|.\n    \"\"\"\n    return [t for t in qualified_list if ParseQualifiedTarget(t)[1] == target]\ndef ParseQualifiedTarget(target):\n    # Splits a qualified target into a build file, target name and toolset.\n    # NOTE: rsplit is used to disambiguate the Windows drive letter separator.\n    target_split = target.rsplit(\":\", 1)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "ParseQualifiedTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def ParseQualifiedTarget(target):\n    # Splits a qualified target into a build file, target name and toolset.\n    # NOTE: rsplit is used to disambiguate the Windows drive letter separator.\n    target_split = target.rsplit(\":\", 1)\n    if len(target_split) == 2:\n        [build_file, target] = target_split\n    else:\n        build_file = None\n    target_split = target.rsplit(\"#\", 1)\n    if len(target_split) == 2:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "ResolveTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def ResolveTarget(build_file, target, toolset):\n    # This function resolves a target into a canonical form:\n    # - a fully defined build file, either absolute or relative to the current\n    # directory\n    # - a target name\n    # - a toolset\n    #\n    # build_file is the file relative to which 'target' is defined.\n    # target is the qualified target.\n    # toolset is the default toolset for that target.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "BuildFile",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def BuildFile(fully_qualified_target):\n    # Extracts the build file from the fully qualified target.\n    return ParseQualifiedTarget(fully_qualified_target)[0]\ndef GetEnvironFallback(var_list, default):\n    \"\"\"Look up a key in the environment, with fallback to secondary keys\n    and finally falling back to a default value.\"\"\"\n    for var in var_list:\n        if var in os.environ:\n            return os.environ[var]\n    return default",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "GetEnvironFallback",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def GetEnvironFallback(var_list, default):\n    \"\"\"Look up a key in the environment, with fallback to secondary keys\n    and finally falling back to a default value.\"\"\"\n    for var in var_list:\n        if var in os.environ:\n            return os.environ[var]\n    return default\ndef QualifiedTarget(build_file, target, toolset):\n    # \"Qualified\" means the file that a target was defined in and the target\n    # name, separated by a colon, suffixed by a # and the toolset name:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "QualifiedTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def QualifiedTarget(build_file, target, toolset):\n    # \"Qualified\" means the file that a target was defined in and the target\n    # name, separated by a colon, suffixed by a # and the toolset name:\n    # /path/to/file.gyp:target_name#toolset\n    fully_qualified = build_file + \":\" + target\n    if toolset:\n        fully_qualified = fully_qualified + \"#\" + toolset\n    return fully_qualified\n@memoize\ndef RelativePath(path, relative_to, follow_path_symlink=True):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "RelativePath",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def RelativePath(path, relative_to, follow_path_symlink=True):\n    # Assuming both |path| and |relative_to| are relative to the current\n    # directory, returns a relative path that identifies path relative to\n    # relative_to.\n    # If |follow_symlink_path| is true (default) and |path| is a symlink, then\n    # this method returns a path to the real file represented by |path|. If it is\n    # false, this method returns a path to the symlink. If |path| is not a\n    # symlink, this option has no effect.\n    # Convert to normalized (and therefore absolute paths).\n    path = os.path.realpath(path) if follow_path_symlink else os.path.abspath(path)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "InvertRelativePath",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def InvertRelativePath(path, toplevel_dir=None):\n    \"\"\"Given a path like foo/bar that is relative to toplevel_dir, return\n    the inverse relative path back to the toplevel_dir.\n    E.g. os.path.normpath(os.path.join(path, InvertRelativePath(path)))\n    should always produce the empty string, unless the path contains symlinks.\n    \"\"\"\n    if not path:\n        return path\n    toplevel_dir = \".\" if toplevel_dir is None else toplevel_dir\n    return RelativePath(toplevel_dir, os.path.join(toplevel_dir, path))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "FixIfRelativePath",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def FixIfRelativePath(path, relative_to):\n    # Like RelativePath but returns |path| unchanged if it is absolute.\n    if os.path.isabs(path):\n        return path\n    return RelativePath(path, relative_to)\ndef UnrelativePath(path, relative_to):\n    # Assuming that |relative_to| is relative to the current directory, and |path|\n    # is a path relative to the dirname of |relative_to|, returns a path that\n    # identifies |path| relative to the current directory.\n    rel_dir = os.path.dirname(relative_to)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "UnrelativePath",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def UnrelativePath(path, relative_to):\n    # Assuming that |relative_to| is relative to the current directory, and |path|\n    # is a path relative to the dirname of |relative_to|, returns a path that\n    # identifies |path| relative to the current directory.\n    rel_dir = os.path.dirname(relative_to)\n    return os.path.normpath(os.path.join(rel_dir, path))\n# re objects used by EncodePOSIXShellArgument.  See IEEE 1003.1 XCU.2.2 at\n# http://www.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_02\n# and the documentation for various shells.\n# _quote is a pattern that should match any argument that needs to be quoted",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "EncodePOSIXShellArgument",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def EncodePOSIXShellArgument(argument):\n    \"\"\"Encodes |argument| suitably for consumption by POSIX shells.\n    argument may be quoted and escaped as necessary to ensure that POSIX shells\n    treat the returned value as a literal representing the argument passed to\n    this function.  Parameter (variable) expansions beginning with $ are allowed\n    to remain intact without escaping the $, to allow the argument to contain\n    references to variables to be expanded by the shell.\n    \"\"\"\n    if not isinstance(argument, str):\n        argument = str(argument)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "EncodePOSIXShellList",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def EncodePOSIXShellList(list):\n    \"\"\"Encodes |list| suitably for consumption by POSIX shells.\n    Returns EncodePOSIXShellArgument for each item in list, and joins them\n    together using the space character as an argument separator.\n    \"\"\"\n    encoded_arguments = []\n    for argument in list:\n        encoded_arguments.append(EncodePOSIXShellArgument(argument))\n    return \" \".join(encoded_arguments)\ndef DeepDependencyTargets(target_dicts, roots):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "DeepDependencyTargets",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def DeepDependencyTargets(target_dicts, roots):\n    \"\"\"Returns the recursive list of target dependencies.\"\"\"\n    dependencies = set()\n    pending = set(roots)\n    while pending:\n        # Pluck out one.\n        r = pending.pop()\n        # Skip if visited already.\n        if r in dependencies:\n            continue",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "BuildFileTargets",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def BuildFileTargets(target_list, build_file):\n    \"\"\"From a target_list, returns the subset from the specified build_file.\"\"\"\n    return [p for p in target_list if BuildFile(p) == build_file]\ndef AllTargets(target_list, target_dicts, build_file):\n    \"\"\"Returns all targets (direct and dependencies) for the specified build_file.\"\"\"\n    bftargets = BuildFileTargets(target_list, build_file)\n    deptargets = DeepDependencyTargets(target_dicts, bftargets)\n    return bftargets + deptargets\ndef WriteOnDiff(filename):\n    \"\"\"Write to a file only if the new contents differ.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "AllTargets",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def AllTargets(target_list, target_dicts, build_file):\n    \"\"\"Returns all targets (direct and dependencies) for the specified build_file.\"\"\"\n    bftargets = BuildFileTargets(target_list, build_file)\n    deptargets = DeepDependencyTargets(target_dicts, bftargets)\n    return bftargets + deptargets\ndef WriteOnDiff(filename):\n    \"\"\"Write to a file only if the new contents differ.\n    Arguments:\n      filename: name of the file to potentially write to.\n    Returns:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "WriteOnDiff",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def WriteOnDiff(filename):\n    \"\"\"Write to a file only if the new contents differ.\n    Arguments:\n      filename: name of the file to potentially write to.\n    Returns:\n      A file like object which will write to temporary file and only overwrite\n      the target if it differs (on close).\n    \"\"\"\n    class Writer:\n        \"\"\"Wrapper around file which only covers the target if it differs.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "EnsureDirExists",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def EnsureDirExists(path):\n    \"\"\"Make sure the directory for |path| exists.\"\"\"\n    try:\n        os.makedirs(os.path.dirname(path))\n    except OSError:\n        pass\ndef GetCompilerPredefines():  # -> dict\n    cmd = []\n    defines = {}\n    # shlex.split() will eat '\\' in posix mode, but",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "GetCompilerPredefines",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def GetCompilerPredefines():  # -> dict\n    cmd = []\n    defines = {}\n    # shlex.split() will eat '\\' in posix mode, but\n    # setting posix=False will preserve extra '\"' cause CreateProcess fail on Windows\n    # this makes '\\' in %CC_target% and %CFLAGS% work\n    def replace_sep(s):\n        return s.replace(os.sep, \"/\") if os.sep != \"/\" else s\n    if CC := os.environ.get(\"CC_target\") or os.environ.get(\"CC\"):\n        cmd += shlex.split(replace_sep(CC))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "GetFlavorByPlatform",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def GetFlavorByPlatform():\n    \"\"\"Returns |params.flavor| if it's set, the system's default flavor else.\"\"\"\n    flavors = {\n        \"cygwin\": \"win\",\n        \"win32\": \"win\",\n        \"darwin\": \"mac\",\n    }\n    if sys.platform in flavors:\n        return flavors[sys.platform]\n    if sys.platform.startswith(\"sunos\"):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "GetFlavor",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def GetFlavor(params):\n    if \"flavor\" in params:\n        return params[\"flavor\"]\n    defines = GetCompilerPredefines()\n    if \"__EMSCRIPTEN__\" in defines:\n        return \"emscripten\"\n    if \"__wasm__\" in defines:\n        return \"wasi\" if \"__wasi__\" in defines else \"wasm\"\n    return GetFlavorByPlatform()\ndef CopyTool(flavor, out_path, generator_flags={}):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "CopyTool",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def CopyTool(flavor, out_path, generator_flags={}):\n    \"\"\"Finds (flock|mac|win)_tool.gyp in the gyp directory and copies it\n    to |out_path|.\"\"\"\n    # aix and solaris just need flock emulation. mac and win use more complicated\n    # support scripts.\n    prefix = {\n        \"aix\": \"flock\",\n        \"os400\": \"flock\",\n        \"solaris\": \"flock\",\n        \"mac\": \"mac\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "uniquer",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def uniquer(seq, idfun=lambda x: x):\n    seen = {}\n    result = []\n    for item in seq:\n        marker = idfun(item)\n        if marker in seen:\n            continue\n        seen[marker] = 1\n        result.append(item)\n    return result",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "TopologicallySorted",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def TopologicallySorted(graph, get_edges):\n    r\"\"\"Topologically sort based on a user provided edge definition.\n    Args:\n      graph: A list of node names.\n      get_edges: A function mapping from node name to a hashable collection\n                 of node names which this node has outgoing edges to.\n    Returns:\n      A list containing all of the node in graph in topological order.\n      It is assumed that calling get_edges once for each node and caching is\n      cheaper than repeatedly calling get_edges.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "CrossCompileRequested",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def CrossCompileRequested():\n    # TODO: figure out how to not build extra host objects in the\n    # non-cross-compile case when this is enabled, and enable unconditionally.\n    return (\n        os.environ.get(\"GYP_CROSSCOMPILE\")\n        or os.environ.get(\"AR_host\")\n        or os.environ.get(\"CC_host\")\n        or os.environ.get(\"CXX_host\")\n        or os.environ.get(\"AR_target\")\n        or os.environ.get(\"CC_target\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "IsCygwin",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "def IsCygwin():\n    try:\n        out = subprocess.Popen(\n            \"uname\", stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n        )\n        stdout = out.communicate()[0].decode(\"utf-8\")\n        return \"CYGWIN\" in str(stdout)\n    except Exception:\n        return False",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "_quote",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "_quote = re.compile(\"[\\t\\n #$%&'()*;<=>?[{|}~]|^$\")\n# _escape is a pattern that should match any character that needs to be\n# escaped with a backslash, whether or not the argument matched the _quote\n# pattern.  _escape is used with re.sub to backslash anything in _escape's\n# first match group, hence the (parentheses) in the regular expression.\n#\n# _escape matches the following characters appearing anywhere in an argument:\n#   \"  to prevent POSIX shells from interpreting this character for quoting\n#   \\  to prevent POSIX shells from interpreting this character for escaping\n#   `  to prevent POSIX shells from interpreting this character for command",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "_escape",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "peekOfCode": "_escape = re.compile(r'([\"\\\\`])')\ndef EncodePOSIXShellArgument(argument):\n    \"\"\"Encodes |argument| suitably for consumption by POSIX shells.\n    argument may be quoted and escaped as necessary to ensure that POSIX shells\n    treat the returned value as a literal representing the argument passed to\n    this function.  Parameter (variable) expansions beginning with $ are allowed\n    to remain intact without escaping the $, to allow the argument to contain\n    references to variables to be expanded by the shell.\n    \"\"\"\n    if not isinstance(argument, str):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common",
        "documentation": {}
    },
    {
        "label": "TestTopologicallySorted",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common_test",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common_test",
        "peekOfCode": "class TestTopologicallySorted(unittest.TestCase):\n    def test_Valid(self):\n        \"\"\"Test that sorting works on a valid graph with one possible order.\"\"\"\n        graph = {\n            \"a\": [\"b\", \"c\"],\n            \"b\": [],\n            \"c\": [\"d\"],\n            \"d\": [\"b\"],\n        }\n        def GetEdge(node):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common_test",
        "documentation": {}
    },
    {
        "label": "TestGetFlavor",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common_test",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common_test",
        "peekOfCode": "class TestGetFlavor(unittest.TestCase):\n    \"\"\"Test that gyp.common.GetFlavor works as intended\"\"\"\n    original_platform = \"\"\n    def setUp(self):\n        self.original_platform = sys.platform\n    def tearDown(self):\n        sys.platform = self.original_platform\n    def assertFlavor(self, expected, argument, param):\n        sys.platform = argument\n        assert expected == gyp.common.GetFlavor(param)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.common_test",
        "documentation": {}
    },
    {
        "label": "XmlToString",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "peekOfCode": "def XmlToString(content, encoding=\"utf-8\", pretty=False):\n    \"\"\"Writes the XML content to disk, touching the file only if it has changed.\n    Visual Studio files have a lot of pre-defined structures.  This function makes\n    it easy to represent these structures as Python data structures, instead of\n    having to create a lot of function calls.\n    Each XML element of the content is represented as a list composed of:\n    1. The name of the element, a string,\n    2. The attributes of the element, a dictionary (optional), and\n    3+. The content of the element, if any.  Strings are simple text nodes and\n        lists are child elements.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "documentation": {}
    },
    {
        "label": "WriteXmlIfChanged",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "peekOfCode": "def WriteXmlIfChanged(\n    content, path, encoding=\"utf-8\", pretty=False, win32=(sys.platform == \"win32\")\n):\n    \"\"\"Writes the XML content to disk, touching the file only if it has changed.\n    Args:\n      content:  The structured content to be written.\n      path: Location of the file.\n      encoding: The encoding to report on the first line of the XML file.\n      pretty: True if we want pretty printing with indents and new lines.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "documentation": {}
    },
    {
        "label": "_xml_escape_map",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "peekOfCode": "_xml_escape_map = {\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\",\n    \"<\": \"&lt;\",\n    \">\": \"&gt;\",\n    \"&\": \"&amp;\",\n    \"\\n\": \"&#xA;\",\n    \"\\r\": \"&#xD;\",\n}\n_xml_escape_re = re.compile(\"(%s)\" % \"|\".join(map(re.escape, _xml_escape_map.keys())))",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "documentation": {}
    },
    {
        "label": "_xml_escape_re",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "peekOfCode": "_xml_escape_re = re.compile(\"(%s)\" % \"|\".join(map(re.escape, _xml_escape_map.keys())))\ndef _XmlEscape(value, attr=False):\n    \"\"\"Escape a string for inclusion in XML.\"\"\"\n    def replace(match):\n        m = match.string[match.start() : match.end()]\n        # don't replace single quotes in attrs\n        if attr and m == \"'\":\n            return m\n        return _xml_escape_map[m]\n    return _xml_escape_re.sub(replace, value)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml",
        "documentation": {}
    },
    {
        "label": "TestSequenceFunctions",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml_test",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml_test",
        "peekOfCode": "class TestSequenceFunctions(unittest.TestCase):\n    def setUp(self):\n        self.stderr = StringIO()\n    def test_EasyXml_simple(self):\n        self.assertEqual(\n            easy_xml.XmlToString([\"test\"]),\n            '<?xml version=\"1.0\" encoding=\"utf-8\"?><test/>',\n        )\n        self.assertEqual(\n            easy_xml.XmlToString([\"test\"], encoding=\"Windows-1252\"),",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.easy_xml_test",
        "documentation": {}
    },
    {
        "label": "FlockTool",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.flock_tool",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.flock_tool",
        "peekOfCode": "class FlockTool:\n    \"\"\"This class emulates the 'flock' command.\"\"\"\n    def Dispatch(self, args):\n        \"\"\"Dispatches a string command to a method.\"\"\"\n        if len(args) < 1:\n            raise Exception(\"Not enough arguments\")\n        method = \"Exec%s\" % self._CommandifyName(args[0])\n        getattr(self, method)(*args[1:])\n    def _CommandifyName(self, name_string):\n        \"\"\"Transforms a tool name like copy-info-plist to CopyInfoPlist\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.flock_tool",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.flock_tool",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.flock_tool",
        "peekOfCode": "def main(args):\n    executor = FlockTool()\n    executor.Dispatch(args)\nclass FlockTool:\n    \"\"\"This class emulates the 'flock' command.\"\"\"\n    def Dispatch(self, args):\n        \"\"\"Dispatches a string command to a method.\"\"\"\n        if len(args) < 1:\n            raise Exception(\"Not enough arguments\")\n        method = \"Exec%s\" % self._CommandifyName(args[0])",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.flock_tool",
        "documentation": {}
    },
    {
        "label": "ParallelProcessingError",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "class ParallelProcessingError(Exception):\n    pass\nclass ParallelState:\n    \"\"\"Class to keep track of state when processing input files in parallel.\n    If build files are loaded in parallel, use this to keep track of\n    state during farming out and processing parallel jobs. It's stored\n    in a global so that the callback function can have access to it.\n    \"\"\"\n    def __init__(self):\n        # The multiprocessing pool.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ParallelState",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "class ParallelState:\n    \"\"\"Class to keep track of state when processing input files in parallel.\n    If build files are loaded in parallel, use this to keep track of\n    state during farming out and processing parallel jobs. It's stored\n    in a global so that the callback function can have access to it.\n    \"\"\"\n    def __init__(self):\n        # The multiprocessing pool.\n        self.pool = None\n        # The condition variable used to protect this object and notify",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "DependencyGraphNode",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "class DependencyGraphNode:\n    \"\"\"\n    Attributes:\n      ref: A reference to an object that this DependencyGraphNode represents.\n      dependencies: List of DependencyGraphNodes on which this one depends.\n      dependents: List of DependencyGraphNodes that depend on this one.\n    \"\"\"\n    class CircularException(GypError):\n        pass\n    def __init__(self, ref):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "IsPathSection",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def IsPathSection(section):\n    # If section ends in one of the '=+?!' characters, it's applied to a section\n    # without the trailing characters.  '/' is notably absent from this list,\n    # because there's no way for a regular expression to be treated as a path.\n    while section and section[-1:] in \"=+?!\":\n        section = section[:-1]\n    if section in path_sections:\n        return True\n    # Sections matching the regexp '_(dir|file|path)s?$' are also\n    # considered PathSections. Using manual string matching since that",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "GetIncludedBuildFiles",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def GetIncludedBuildFiles(build_file_path, aux_data, included=None):\n    \"\"\"Return a list of all build files included into build_file_path.\n    The returned list will contain build_file_path as well as all other files\n    that it included, either directly or indirectly.  Note that the list may\n    contain files that were included into a conditional section that evaluated\n    to false and was not merged into build_file_path's dict.\n    aux_data is a dict containing a key for each build file or included build\n    file.  Those keys provide access to dicts whose \"included\" keys contain\n    lists of all other files included by the build file.\n    included should be left at its default None value by external callers.  It",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "CheckedEval",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def CheckedEval(file_contents):\n    \"\"\"Return the eval of a gyp file.\n    The gyp file is restricted to dictionaries and lists only, and\n    repeated keys are not allowed.\n    Note that this is slower than eval() is.\n    \"\"\"\n    syntax_tree = ast.parse(file_contents)\n    assert isinstance(syntax_tree, ast.Module)\n    c1 = syntax_tree.body\n    assert len(c1) == 1",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "CheckNode",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def CheckNode(node, keypath):\n    if isinstance(node, ast.Dict):\n        dict = {}\n        for key, value in zip(node.keys, node.values):\n            assert isinstance(key, ast.Str)\n            key = key.s\n            if key in dict:\n                raise GypError(\n                    \"Key '\"\n                    + key",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "LoadOneBuildFile",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def LoadOneBuildFile(build_file_path, data, aux_data, includes, is_target, check):\n    if build_file_path in data:\n        return data[build_file_path]\n    if os.path.exists(build_file_path):\n        build_file_contents = open(build_file_path, encoding=\"utf-8\").read()\n    else:\n        raise GypError(f\"{build_file_path} not found (cwd: {os.getcwd()})\")\n    build_file_data = None\n    try:\n        if check:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "LoadBuildFileIncludesIntoDict",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def LoadBuildFileIncludesIntoDict(\n    subdict, subdict_path, data, aux_data, includes, check\n):\n    includes_list = []\n    if includes is not None:\n        includes_list.extend(includes)\n    if \"includes\" in subdict:\n        for include in subdict[\"includes\"]:\n            # \"include\" is specified relative to subdict_path, so compute the real\n            # path to include by appending the provided \"include\" to the directory",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "LoadBuildFileIncludesIntoList",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def LoadBuildFileIncludesIntoList(sublist, sublist_path, data, aux_data, check):\n    for item in sublist:\n        if isinstance(item, dict):\n            LoadBuildFileIncludesIntoDict(\n                item, sublist_path, data, aux_data, None, check\n            )\n        elif isinstance(item, list):\n            LoadBuildFileIncludesIntoList(item, sublist_path, data, aux_data, check)\n# Processes toolsets in all the targets. This recurses into condition entries\n# since they can contain toolsets as well.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ProcessToolsetsInDict",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ProcessToolsetsInDict(data):\n    if \"targets\" in data:\n        target_list = data[\"targets\"]\n        new_target_list = []\n        for target in target_list:\n            # If this target already has an explicit 'toolset', and no 'toolsets'\n            # list, don't modify it further.\n            if \"toolset\" in target and \"toolsets\" not in target:\n                new_target_list.append(target)\n                continue",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "LoadTargetBuildFile",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def LoadTargetBuildFile(\n    build_file_path,\n    data,\n    aux_data,\n    variables,\n    includes,\n    depth,\n    check,\n    load_dependencies,\n):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "CallLoadTargetBuildFile",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def CallLoadTargetBuildFile(\n    global_flags,\n    build_file_path,\n    variables,\n    includes,\n    depth,\n    check,\n    generator_input_info,\n):\n    \"\"\"Wrapper around LoadTargetBuildFile for parallel processing.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "LoadTargetBuildFilesParallel",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def LoadTargetBuildFilesParallel(\n    build_files, data, variables, includes, depth, check, generator_input_info\n):\n    parallel_state = ParallelState()\n    parallel_state.condition = threading.Condition()\n    # Make copies of the build_files argument that we can modify while working.\n    parallel_state.dependencies = list(build_files)\n    parallel_state.scheduled = set(build_files)\n    parallel_state.pending = 0\n    parallel_state.data = data",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "FindEnclosingBracketGroup",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def FindEnclosingBracketGroup(input_str):\n    stack = []\n    start = -1\n    for index, char in enumerate(input_str):\n        if char in LBRACKETS:\n            stack.append(char)\n            if start == -1:\n                start = index\n        elif char in BRACKETS:\n            if not stack:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "IsStrCanonicalInt",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def IsStrCanonicalInt(string):\n    \"\"\"Returns True if |string| is in its canonical integer form.\n    The canonical form is such that str(int(string)) == string.\n    \"\"\"\n    if isinstance(string, str):\n        # This function is called a lot so for maximum performance, avoid\n        # involving regexps which would otherwise make the code much\n        # shorter. Regexps would need twice the time of this function.\n        if string:\n            if string == \"0\":",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "FixupPlatformCommand",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def FixupPlatformCommand(cmd):\n    if sys.platform == \"win32\":\n        if isinstance(cmd, list):\n            cmd = [re.sub(\"^cat \", \"type \", cmd[0])] + cmd[1:]\n        else:\n            cmd = re.sub(\"^cat \", \"type \", cmd)\n    return cmd\nPHASE_EARLY = 0\nPHASE_LATE = 1\nPHASE_LATELATE = 2",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ExpandVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ExpandVariables(input, phase, variables, build_file):\n    # Look for the pattern that gets expanded into variables\n    if phase == PHASE_EARLY:\n        variable_re = early_variable_re\n        expansion_symbol = \"<\"\n    elif phase == PHASE_LATE:\n        variable_re = late_variable_re\n        expansion_symbol = \">\"\n    elif phase == PHASE_LATELATE:\n        variable_re = latelate_variable_re",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "EvalCondition",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def EvalCondition(condition, conditions_key, phase, variables, build_file):\n    \"\"\"Returns the dict that should be used or None if the result was\n    that nothing should be used.\"\"\"\n    if not isinstance(condition, list):\n        raise GypError(conditions_key + \" must be a list\")\n    if len(condition) < 2:\n        # It's possible that condition[0] won't work in which case this\n        # attempt will raise its own IndexError.  That's probably fine.\n        raise GypError(\n            conditions_key",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "EvalSingleCondition",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def EvalSingleCondition(cond_expr, true_dict, false_dict, phase, variables, build_file):\n    \"\"\"Returns true_dict if cond_expr evaluates to true, and false_dict\n    otherwise.\"\"\"\n    # Do expansions on the condition itself.  Since the condition can naturally\n    # contain variable references without needing to resort to GYP expansion\n    # syntax, this is of dubious value for variables, but someone might want to\n    # use a command expansion directly inside a condition.\n    cond_expr_expanded = ExpandVariables(cond_expr, phase, variables, build_file)\n    if type(cond_expr_expanded) not in (str, int):\n        raise ValueError(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ProcessConditionsInDict",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ProcessConditionsInDict(the_dict, phase, variables, build_file):\n    # Process a 'conditions' or 'target_conditions' section in the_dict,\n    # depending on phase.\n    # early -> conditions\n    # late -> target_conditions\n    # latelate -> no conditions\n    #\n    # Each item in a conditions list consists of cond_expr, a string expression\n    # evaluated as the condition, and true_dict, a dict that will be merged into\n    # the_dict if cond_expr evaluates to true.  Optionally, a third item,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "LoadAutomaticVariablesFromDict",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def LoadAutomaticVariablesFromDict(variables, the_dict):\n    # Any keys with plain string values in the_dict become automatic variables.\n    # The variable name is the key name with a \"_\" character prepended.\n    for key, value in the_dict.items():\n        if type(value) in (str, int, list):\n            variables[\"_\" + key] = value\ndef LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key):\n    # Any keys in the_dict's \"variables\" dict, if it has one, becomes a\n    # variable.  The variable name is the key name in the \"variables\" dict.\n    # Variables that end with the % character are set only if they are unset in",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "LoadVariablesFromVariablesDict",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key):\n    # Any keys in the_dict's \"variables\" dict, if it has one, becomes a\n    # variable.  The variable name is the key name in the \"variables\" dict.\n    # Variables that end with the % character are set only if they are unset in\n    # the variables dict.  the_dict_key is the name of the key that accesses\n    # the_dict in the_dict's parent dict.  If the_dict's parent is not a dict\n    # (it could be a list or it could be parentless because it is a root dict),\n    # the_dict_key will be None.\n    for key, value in the_dict.get(\"variables\", {}).items():\n        if type(value) not in (str, int, list):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ProcessVariablesAndConditionsInDict",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ProcessVariablesAndConditionsInDict(\n    the_dict, phase, variables_in, build_file, the_dict_key=None\n):\n    \"\"\"Handle all variable and command expansion and conditional evaluation.\n    This function is the public entry point for all variable expansions and\n    conditional evaluations.  The variables_in dictionary will not be modified\n    by this function.\n    \"\"\"\n    # Make a copy of the variables_in dict that can be modified during the\n    # loading of automatics and the loading of the variables dict.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ProcessVariablesAndConditionsInList",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ProcessVariablesAndConditionsInList(the_list, phase, variables, build_file):\n    # Iterate using an index so that new values can be assigned into the_list.\n    index = 0\n    while index < len(the_list):\n        item = the_list[index]\n        if isinstance(item, dict):\n            # Make a copy of the variables dict so that it won't influence anything\n            # outside of its own scope.\n            ProcessVariablesAndConditionsInDict(item, phase, variables, build_file)\n        elif isinstance(item, list):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "BuildTargetsDict",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def BuildTargetsDict(data):\n    \"\"\"Builds a dict mapping fully-qualified target names to their target dicts.\n    |data| is a dict mapping loaded build files by pathname relative to the\n    current directory.  Values in |data| are build file contents.  For each\n    |data| value with a \"targets\" key, the value of the \"targets\" key is taken\n    as a list containing target dicts.  Each target's fully-qualified name is\n    constructed from the pathname of the build file (|data| key) and its\n    \"target_name\" property.  These fully-qualified names are used as the keys\n    in the returned dict.  These keys provide access to the target dicts,\n    the dicts in the \"targets\" lists.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "QualifyDependencies",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def QualifyDependencies(targets):\n    \"\"\"Make dependency links fully-qualified relative to the current directory.\n    |targets| is a dict mapping fully-qualified target names to their target\n    dicts.  For each target in this dict, keys known to contain dependency\n    links are examined, and any dependencies referenced will be rewritten\n    so that they are fully-qualified and relative to the current directory.\n    All rewritten dependencies are suitable for use as keys to |targets| or a\n    similar dict.\n    \"\"\"\n    all_dependency_sections = [",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ExpandWildcardDependencies",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ExpandWildcardDependencies(targets, data):\n    \"\"\"Expands dependencies specified as build_file:*.\n    For each target in |targets|, examines sections containing links to other\n    targets.  If any such section contains a link of the form build_file:*, it\n    is taken as a wildcard link, and is expanded to list each target in\n    build_file.  The |data| dict provides access to build file dicts.\n    Any target that does not wish to be included by wildcard can provide an\n    optional \"suppress_wildcard\" key in its target dict.  When present and\n    true, a wildcard dependency link will not include such targets.\n    All dependency names, including the keys to |targets| and the values in each",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "Unify",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def Unify(items):\n    \"\"\"Removes duplicate elements from items, keeping the first element.\"\"\"\n    seen = {}\n    return [seen.setdefault(e, e) for e in items if e not in seen]\ndef RemoveDuplicateDependencies(targets):\n    \"\"\"Makes sure every dependency appears only once in all targets's dependency\n    lists.\"\"\"\n    for target_name, target_dict in targets.items():\n        for dependency_key in dependency_sections:\n            dependencies = target_dict.get(dependency_key, [])",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "RemoveDuplicateDependencies",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def RemoveDuplicateDependencies(targets):\n    \"\"\"Makes sure every dependency appears only once in all targets's dependency\n    lists.\"\"\"\n    for target_name, target_dict in targets.items():\n        for dependency_key in dependency_sections:\n            dependencies = target_dict.get(dependency_key, [])\n            if dependencies:\n                target_dict[dependency_key] = Unify(dependencies)\ndef Filter(items, item):\n    \"\"\"Removes item from items.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "Filter",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def Filter(items, item):\n    \"\"\"Removes item from items.\"\"\"\n    res = {}\n    return [res.setdefault(e, e) for e in items if e != item]\ndef RemoveSelfDependencies(targets):\n    \"\"\"Remove self dependencies from targets that have the prune_self_dependency\n    variable set.\"\"\"\n    for target_name, target_dict in targets.items():\n        for dependency_key in dependency_sections:\n            dependencies = target_dict.get(dependency_key, [])",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "RemoveSelfDependencies",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def RemoveSelfDependencies(targets):\n    \"\"\"Remove self dependencies from targets that have the prune_self_dependency\n    variable set.\"\"\"\n    for target_name, target_dict in targets.items():\n        for dependency_key in dependency_sections:\n            dependencies = target_dict.get(dependency_key, [])\n            if dependencies:\n                for t in dependencies:\n                    if t == target_name and (\n                        targets[t].get(\"variables\", {}).get(\"prune_self_dependency\", 0)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "RemoveLinkDependenciesFromNoneTargets",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def RemoveLinkDependenciesFromNoneTargets(targets):\n    \"\"\"Remove dependencies having the 'link_dependency' attribute from the 'none'\n    targets.\"\"\"\n    for target_name, target_dict in targets.items():\n        for dependency_key in dependency_sections:\n            dependencies = target_dict.get(dependency_key, [])\n            if dependencies:\n                for t in dependencies:\n                    if target_dict.get(\"type\", None) == \"none\":\n                        if targets[t].get(\"variables\", {}).get(\"link_dependency\", 0):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "BuildDependencyList",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def BuildDependencyList(targets):\n    # Create a DependencyGraphNode for each target.  Put it into a dict for easy\n    # access.\n    dependency_nodes = {}\n    for target, spec in targets.items():\n        if target not in dependency_nodes:\n            dependency_nodes[target] = DependencyGraphNode(target)\n    # Set up the dependency links.  Targets that have no dependencies are treated\n    # as dependent on root_node.\n    root_node = DependencyGraphNode(None)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "VerifyNoGYPFileCircularDependencies",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def VerifyNoGYPFileCircularDependencies(targets):\n    # Create a DependencyGraphNode for each gyp file containing a target.  Put\n    # it into a dict for easy access.\n    dependency_nodes = {}\n    for target in targets:\n        build_file = gyp.common.BuildFile(target)\n        if build_file not in dependency_nodes:\n            dependency_nodes[build_file] = DependencyGraphNode(build_file)\n    # Set up the dependency links.\n    for target, spec in targets.items():",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "DoDependentSettings",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def DoDependentSettings(key, flat_list, targets, dependency_nodes):\n    # key should be one of all_dependent_settings, direct_dependent_settings,\n    # or link_settings.\n    for target in flat_list:\n        target_dict = targets[target]\n        build_file = gyp.common.BuildFile(target)\n        if key == \"all_dependent_settings\":\n            dependencies = dependency_nodes[target].DeepDependencies()\n        elif key == \"direct_dependent_settings\":\n            dependencies = dependency_nodes[target].DirectAndImportedDependencies(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "AdjustStaticLibraryDependencies",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def AdjustStaticLibraryDependencies(\n    flat_list, targets, dependency_nodes, sort_dependencies\n):\n    # Recompute target \"dependencies\" properties.  For each static library\n    # target, remove \"dependencies\" entries referring to other static libraries,\n    # unless the dependency has the \"hard_dependency\" attribute set.  For each\n    # linkable target, add a \"dependencies\" entry referring to all of the\n    # target's computed list of link dependencies (including static libraries\n    # if no such entry is already present.\n    for target in flat_list:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "MakePathRelative",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def MakePathRelative(to_file, fro_file, item):\n    # If item is a relative path, it's relative to the build file dict that it's\n    # coming from.  Fix it up to make it relative to the build file dict that\n    # it's going into.\n    # Exception: any |item| that begins with these special characters is\n    # returned without modification.\n    #   /   Used when a path is already absolute (shortcut optimization;\n    #       such paths would be returned as absolute anyway)\n    #   $   Used for build environment variables\n    #   -   Used for some build environment flags (such as -lapr-1 in a",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "MergeLists",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def MergeLists(to, fro, to_file, fro_file, is_paths=False, append=True):\n    # Python documentation recommends objects which do not support hash\n    # set this value to None. Python library objects follow this rule.\n    def is_hashable(val):\n        return val.__hash__\n    # If x is hashable, returns whether x is in s. Else returns whether x is in items.\n    def is_in_set_or_list(x, s, items):\n        if is_hashable(x):\n            return x in s\n        return x in items",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "MergeDicts",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def MergeDicts(to, fro, to_file, fro_file):\n    # I wanted to name the parameter \"from\" but it's a Python keyword...\n    for k, v in fro.items():\n        # It would be nice to do \"if not k in to: to[k] = v\" but that wouldn't give\n        # copy semantics.  Something else may want to merge from the |fro| dict\n        # later, and having the same dict ref pointed to twice in the tree isn't\n        # what anyone wants considering that the dicts may subsequently be\n        # modified.\n        if k in to:\n            bad_merge = False",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "MergeConfigWithInheritance",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def MergeConfigWithInheritance(\n    new_configuration_dict, build_file, target_dict, configuration, visited\n):\n    # Skip if previously visited.\n    if configuration in visited:\n        return\n    # Look at this configuration.\n    configuration_dict = target_dict[\"configurations\"][configuration]\n    # Merge in parents.\n    for parent in configuration_dict.get(\"inherit_from\", []):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "SetUpConfigurations",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def SetUpConfigurations(target, target_dict):\n    # key_suffixes is a list of key suffixes that might appear on key names.\n    # These suffixes are handled in conditional evaluations (for =, +, and ?)\n    # and rules/exclude processing (for ! and /).  Keys with these suffixes\n    # should be treated the same as keys without.\n    key_suffixes = [\"=\", \"+\", \"?\", \"!\", \"/\"]\n    build_file = gyp.common.BuildFile(target)\n    # Provide a single configuration by default if none exists.\n    # TODO(mark): Signal an error if default_configurations exists but\n    # configurations does not.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ProcessListFiltersInDict",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ProcessListFiltersInDict(name, the_dict):\n    \"\"\"Process regular expression and exclusion-based filters on lists.\n    An exclusion list is in a dict key named with a trailing \"!\", like\n    \"sources!\".  Every item in such a list is removed from the associated\n    main list, which in this example, would be \"sources\".  Removed items are\n    placed into a \"sources_excluded\" list in the dict.\n    Regular expression (regex) filters are contained in dict keys named with a\n    trailing \"/\", such as \"sources/\" to operate on the \"sources\" list.  Regex\n    filters in a dict take the form:\n      'sources/': [ ['exclude', '_(linux|mac|win)\\\\.cc$'],",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ProcessListFiltersInList",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ProcessListFiltersInList(name, the_list):\n    for item in the_list:\n        if isinstance(item, dict):\n            ProcessListFiltersInDict(name, item)\n        elif isinstance(item, list):\n            ProcessListFiltersInList(name, item)\ndef ValidateTargetType(target, target_dict):\n    \"\"\"Ensures the 'type' field on the target is one of the known types.\n    Arguments:\n      target: string, name of target.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ValidateTargetType",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ValidateTargetType(target, target_dict):\n    \"\"\"Ensures the 'type' field on the target is one of the known types.\n    Arguments:\n      target: string, name of target.\n      target_dict: dict, target spec.\n    Raises an exception on error.\n    \"\"\"\n    VALID_TARGET_TYPES = (\n        \"executable\",\n        \"loadable_module\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ValidateRulesInTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ValidateRulesInTarget(target, target_dict, extra_sources_for_rules):\n    \"\"\"Ensures that the rules sections in target_dict are valid and consistent,\n    and determines which sources they apply to.\n    Arguments:\n      target: string, name of target.\n      target_dict: dict, target spec containing \"rules\" and \"sources\" lists.\n      extra_sources_for_rules: a list of keys to scan for rule matches in\n          addition to 'sources'.\n    \"\"\"\n    # Dicts to map between values found in rules' 'rule_name' and 'extension'",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ValidateRunAsInTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ValidateRunAsInTarget(target, target_dict, build_file):\n    target_name = target_dict.get(\"target_name\")\n    run_as = target_dict.get(\"run_as\")\n    if not run_as:\n        return\n    if not isinstance(run_as, dict):\n        raise GypError(\n            \"The 'run_as' in target %s from file %s should be a \"\n            \"dictionary.\" % (target_name, build_file)\n        )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "ValidateActionsInTarget",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def ValidateActionsInTarget(target, target_dict, build_file):\n    \"\"\"Validates the inputs to the actions in a target.\"\"\"\n    target_name = target_dict.get(\"target_name\")\n    actions = target_dict.get(\"actions\", [])\n    for action in actions:\n        action_name = action.get(\"action_name\")\n        if not action_name:\n            raise GypError(\n                \"Anonymous action in target %s.  \"\n                \"An action must have an 'action_name' field.\" % target_name",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "TurnIntIntoStrInDict",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def TurnIntIntoStrInDict(the_dict):\n    \"\"\"Given dict the_dict, recursively converts all integers into strings.\"\"\"\n    # Use items instead of iteritems because there's no need to try to look at\n    # reinserted keys and their associated values.\n    for k, v in the_dict.items():\n        if isinstance(v, int):\n            v = str(v)\n            the_dict[k] = v\n        elif isinstance(v, dict):\n            TurnIntIntoStrInDict(v)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "TurnIntIntoStrInList",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def TurnIntIntoStrInList(the_list):\n    \"\"\"Given list the_list, recursively converts all integers into strings.\"\"\"\n    for index, item in enumerate(the_list):\n        if isinstance(item, int):\n            the_list[index] = str(item)\n        elif isinstance(item, dict):\n            TurnIntIntoStrInDict(item)\n        elif isinstance(item, list):\n            TurnIntIntoStrInList(item)\ndef PruneUnwantedTargets(targets, flat_list, dependency_nodes, root_targets, data):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "PruneUnwantedTargets",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def PruneUnwantedTargets(targets, flat_list, dependency_nodes, root_targets, data):\n    \"\"\"Return only the targets that are deep dependencies of |root_targets|.\"\"\"\n    qualified_root_targets = []\n    for target in root_targets:\n        target = target.strip()\n        qualified_targets = gyp.common.FindQualifiedTargets(target, flat_list)\n        if not qualified_targets:\n            raise GypError(\"Could not find target %s\" % target)\n        qualified_root_targets.extend(qualified_targets)\n    wanted_targets = {}",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "VerifyNoCollidingTargets",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def VerifyNoCollidingTargets(targets):\n    \"\"\"Verify that no two targets in the same directory share the same name.\n    Arguments:\n      targets: A list of targets in the form 'path/to/file.gyp:target_name'.\n    \"\"\"\n    # Keep a dict going from 'subdirectory:target_name' to 'foo.gyp'.\n    used = {}\n    for target in targets:\n        # Separate out 'path/to/file.gyp, 'target_name' from\n        # 'path/to/file.gyp:target_name'.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "SetGeneratorGlobals",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def SetGeneratorGlobals(generator_input_info):\n    # Set up path_sections and non_configuration_keys with the default data plus\n    # the generator-specific data.\n    global path_sections\n    path_sections = set(base_path_sections)\n    path_sections.update(generator_input_info[\"path_sections\"])\n    global non_configuration_keys\n    non_configuration_keys = base_non_configuration_keys[:]\n    non_configuration_keys.extend(generator_input_info[\"non_configuration_keys\"])\n    global multiple_toolsets",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "Load",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "def Load(\n    build_files,\n    variables,\n    includes,\n    depth,\n    generator_input_info,\n    check,\n    circular_check,\n    parallel,\n    root_targets,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "linkable_types",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "linkable_types = [\n    \"executable\",\n    \"shared_library\",\n    \"loadable_module\",\n    \"mac_kernel_extension\",\n    \"windows_driver\",\n]\n# A list of sections that contain links to other targets.\ndependency_sections = [\"dependencies\", \"export_dependent_settings\"]\n# base_path_sections is a list of sections defined by GYP that contain",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "dependency_sections",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "dependency_sections = [\"dependencies\", \"export_dependent_settings\"]\n# base_path_sections is a list of sections defined by GYP that contain\n# pathnames.  The generators can provide more keys, the two lists are merged\n# into path_sections, but you should call IsPathSection instead of using either\n# list directly.\nbase_path_sections = [\n    \"destination\",\n    \"files\",\n    \"include_dirs\",\n    \"inputs\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "base_path_sections",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "base_path_sections = [\n    \"destination\",\n    \"files\",\n    \"include_dirs\",\n    \"inputs\",\n    \"libraries\",\n    \"outputs\",\n    \"sources\",\n]\npath_sections = set()",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "path_sections",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "path_sections = set()\n# These per-process dictionaries are used to cache build file data when loading\n# in parallel mode.\nper_process_data = {}\nper_process_aux_data = {}\ndef IsPathSection(section):\n    # If section ends in one of the '=+?!' characters, it's applied to a section\n    # without the trailing characters.  '/' is notably absent from this list,\n    # because there's no way for a regular expression to be treated as a path.\n    while section and section[-1:] in \"=+?!\":",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "per_process_data",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "per_process_data = {}\nper_process_aux_data = {}\ndef IsPathSection(section):\n    # If section ends in one of the '=+?!' characters, it's applied to a section\n    # without the trailing characters.  '/' is notably absent from this list,\n    # because there's no way for a regular expression to be treated as a path.\n    while section and section[-1:] in \"=+?!\":\n        section = section[:-1]\n    if section in path_sections:\n        return True",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "per_process_aux_data",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "per_process_aux_data = {}\ndef IsPathSection(section):\n    # If section ends in one of the '=+?!' characters, it's applied to a section\n    # without the trailing characters.  '/' is notably absent from this list,\n    # because there's no way for a regular expression to be treated as a path.\n    while section and section[-1:] in \"=+?!\":\n        section = section[:-1]\n    if section in path_sections:\n        return True\n    # Sections matching the regexp '_(dir|file|path)s?$' are also",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "base_non_configuration_keys",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "base_non_configuration_keys = [\n    # Sections that must exist inside targets and not configurations.\n    \"actions\",\n    \"configurations\",\n    \"copies\",\n    \"default_configuration\",\n    \"dependencies\",\n    \"dependencies_original\",\n    \"libraries\",\n    \"postbuilds\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "non_configuration_keys",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "non_configuration_keys = []\n# Keys that do not belong inside a configuration dictionary.\ninvalid_configuration_keys = [\n    \"actions\",\n    \"all_dependent_settings\",\n    \"configurations\",\n    \"dependencies\",\n    \"direct_dependent_settings\",\n    \"libraries\",\n    \"link_settings\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "invalid_configuration_keys",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "invalid_configuration_keys = [\n    \"actions\",\n    \"all_dependent_settings\",\n    \"configurations\",\n    \"dependencies\",\n    \"direct_dependent_settings\",\n    \"libraries\",\n    \"link_settings\",\n    \"sources\",\n    \"standalone_static_library\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "multiple_toolsets",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "multiple_toolsets = False\n# Paths for converting filelist paths to output paths: {\n#   toplevel,\n#   qualified_output_dir,\n# }\ngenerator_filelist_paths = None\ndef GetIncludedBuildFiles(build_file_path, aux_data, included=None):\n    \"\"\"Return a list of all build files included into build_file_path.\n    The returned list will contain build_file_path as well as all other files\n    that it included, either directly or indirectly.  Note that the list may",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "generator_filelist_paths",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "generator_filelist_paths = None\ndef GetIncludedBuildFiles(build_file_path, aux_data, included=None):\n    \"\"\"Return a list of all build files included into build_file_path.\n    The returned list will contain build_file_path as well as all other files\n    that it included, either directly or indirectly.  Note that the list may\n    contain files that were included into a conditional section that evaluated\n    to false and was not merged into build_file_path's dict.\n    aux_data is a dict containing a key for each build file or included build\n    file.  Those keys provide access to dicts whose \"included\" keys contain\n    lists of all other files included by the build file.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "LBRACKETS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "LBRACKETS = set(\"{[(\")\nBRACKETS = {\"}\": \"{\", \"]\": \"[\", \")\": \"(\"}\ndef FindEnclosingBracketGroup(input_str):\n    stack = []\n    start = -1\n    for index, char in enumerate(input_str):\n        if char in LBRACKETS:\n            stack.append(char)\n            if start == -1:\n                start = index",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "BRACKETS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "BRACKETS = {\"}\": \"{\", \"]\": \"[\", \")\": \"(\"}\ndef FindEnclosingBracketGroup(input_str):\n    stack = []\n    start = -1\n    for index, char in enumerate(input_str):\n        if char in LBRACKETS:\n            stack.append(char)\n            if start == -1:\n                start = index\n        elif char in BRACKETS:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "early_variable_re",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "early_variable_re = re.compile(\n    r\"(?P<replace>(?P<type><(?:(?:!?@?)|\\|)?)\"\n    r\"(?P<command_string>[-a-zA-Z0-9_.]+)?\"\n    r\"\\((?P<is_array>\\s*\\[?)\"\n    r\"(?P<content>.*?)(\\]?)\\))\"\n)\n# This matches the same as early_variable_re, but with '>' instead of '<'.\nlate_variable_re = re.compile(\n    r\"(?P<replace>(?P<type>>(?:(?:!?@?)|\\|)?)\"\n    r\"(?P<command_string>[-a-zA-Z0-9_.]+)?\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "late_variable_re",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "late_variable_re = re.compile(\n    r\"(?P<replace>(?P<type>>(?:(?:!?@?)|\\|)?)\"\n    r\"(?P<command_string>[-a-zA-Z0-9_.]+)?\"\n    r\"\\((?P<is_array>\\s*\\[?)\"\n    r\"(?P<content>.*?)(\\]?)\\))\"\n)\n# This matches the same as early_variable_re, but with '^' instead of '<'.\nlatelate_variable_re = re.compile(\n    r\"(?P<replace>(?P<type>[\\^](?:(?:!?@?)|\\|)?)\"\n    r\"(?P<command_string>[-a-zA-Z0-9_.]+)?\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "latelate_variable_re",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "latelate_variable_re = re.compile(\n    r\"(?P<replace>(?P<type>[\\^](?:(?:!?@?)|\\|)?)\"\n    r\"(?P<command_string>[-a-zA-Z0-9_.]+)?\"\n    r\"\\((?P<is_array>\\s*\\[?)\"\n    r\"(?P<content>.*?)(\\]?)\\))\"\n)\n# Global cache of results from running commands so they don't have to be run\n# more then once.\ncached_command_results = {}\ndef FixupPlatformCommand(cmd):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "cached_command_results",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "cached_command_results = {}\ndef FixupPlatformCommand(cmd):\n    if sys.platform == \"win32\":\n        if isinstance(cmd, list):\n            cmd = [re.sub(\"^cat \", \"type \", cmd[0])] + cmd[1:]\n        else:\n            cmd = re.sub(\"^cat \", \"type \", cmd)\n    return cmd\nPHASE_EARLY = 0\nPHASE_LATE = 1",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "PHASE_EARLY",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "PHASE_EARLY = 0\nPHASE_LATE = 1\nPHASE_LATELATE = 2\ndef ExpandVariables(input, phase, variables, build_file):\n    # Look for the pattern that gets expanded into variables\n    if phase == PHASE_EARLY:\n        variable_re = early_variable_re\n        expansion_symbol = \"<\"\n    elif phase == PHASE_LATE:\n        variable_re = late_variable_re",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "PHASE_LATE",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "PHASE_LATE = 1\nPHASE_LATELATE = 2\ndef ExpandVariables(input, phase, variables, build_file):\n    # Look for the pattern that gets expanded into variables\n    if phase == PHASE_EARLY:\n        variable_re = early_variable_re\n        expansion_symbol = \"<\"\n    elif phase == PHASE_LATE:\n        variable_re = late_variable_re\n        expansion_symbol = \">\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "PHASE_LATELATE",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "PHASE_LATELATE = 2\ndef ExpandVariables(input, phase, variables, build_file):\n    # Look for the pattern that gets expanded into variables\n    if phase == PHASE_EARLY:\n        variable_re = early_variable_re\n        expansion_symbol = \"<\"\n    elif phase == PHASE_LATE:\n        variable_re = late_variable_re\n        expansion_symbol = \">\"\n    elif phase == PHASE_LATELATE:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "cached_conditions_asts",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "cached_conditions_asts = {}\ndef EvalCondition(condition, conditions_key, phase, variables, build_file):\n    \"\"\"Returns the dict that should be used or None if the result was\n    that nothing should be used.\"\"\"\n    if not isinstance(condition, list):\n        raise GypError(conditions_key + \" must be a list\")\n    if len(condition) < 2:\n        # It's possible that condition[0] won't work in which case this\n        # attempt will raise its own IndexError.  That's probably fine.\n        raise GypError(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "exception_re",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "peekOfCode": "exception_re = re.compile(r\"\"\"[\"']?[-/$<>^]\"\"\")\ndef MakePathRelative(to_file, fro_file, item):\n    # If item is a relative path, it's relative to the build file dict that it's\n    # coming from.  Fix it up to make it relative to the build file dict that\n    # it's going into.\n    # Exception: any |item| that begins with these special characters is\n    # returned without modification.\n    #   /   Used when a path is already absolute (shortcut optimization;\n    #       such paths would be returned as absolute anyway)\n    #   $   Used for build environment variables",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input",
        "documentation": {}
    },
    {
        "label": "TestFindCycles",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input_test",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input_test",
        "peekOfCode": "class TestFindCycles(unittest.TestCase):\n    def setUp(self):\n        self.nodes = {}\n        for x in (\"a\", \"b\", \"c\", \"d\", \"e\"):\n            self.nodes[x] = gyp.input.DependencyGraphNode(x)\n    def _create_dependency(self, dependent, dependency):\n        dependent.dependencies.append(dependency)\n        dependency.dependents.append(dependent)\n    def test_no_cycle_empty_graph(self):\n        for label, node in self.nodes.items():",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.input_test",
        "documentation": {}
    },
    {
        "label": "MacTool",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "peekOfCode": "class MacTool:\n    \"\"\"This class performs all the Mac tooling steps. The methods can either be\n    executed directly, or dispatched from an argument list.\"\"\"\n    def Dispatch(self, args):\n        \"\"\"Dispatches a string command to a method.\"\"\"\n        if len(args) < 1:\n            raise Exception(\"Not enough arguments\")\n        method = \"Exec%s\" % self._CommandifyName(args[0])\n        return getattr(self, method)(*args[1:])\n    def _CommandifyName(self, name_string):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "peekOfCode": "def main(args):\n    executor = MacTool()\n    if (exit_code := executor.Dispatch(args)) is not None:\n        sys.exit(exit_code)\nclass MacTool:\n    \"\"\"This class performs all the Mac tooling steps. The methods can either be\n    executed directly, or dispatched from an argument list.\"\"\"\n    def Dispatch(self, args):\n        \"\"\"Dispatches a string command to a method.\"\"\"\n        if len(args) < 1:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "documentation": {}
    },
    {
        "label": "NextGreaterPowerOf2",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "peekOfCode": "def NextGreaterPowerOf2(x):\n    return 2 ** (x).bit_length()\ndef WriteHmap(output_name, filelist):\n    \"\"\"Generates a header map based on |filelist|.\n    Per Mark Mentovai:\n      A header map is structured essentially as a hash table, keyed by names used\n      in #includes, and providing pathnames to the actual files.\n    The implementation below and the comment above comes from inspecting:\n      http://www.opensource.apple.com/source/distcc/distcc-2503/distcc_dist/include_server/headermap.py?txt\n    while also looking at the implementation in clang in:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "documentation": {}
    },
    {
        "label": "WriteHmap",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "peekOfCode": "def WriteHmap(output_name, filelist):\n    \"\"\"Generates a header map based on |filelist|.\n    Per Mark Mentovai:\n      A header map is structured essentially as a hash table, keyed by names used\n      in #includes, and providing pathnames to the actual files.\n    The implementation below and the comment above comes from inspecting:\n      http://www.opensource.apple.com/source/distcc/distcc-2503/distcc_dist/include_server/headermap.py?txt\n    while also looking at the implementation in clang in:\n      https://llvm.org/svn/llvm-project/cfe/trunk/lib/Lex/HeaderMap.cpp\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.mac_tool",
        "documentation": {}
    },
    {
        "label": "MsvsSettings",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "class MsvsSettings:\n    \"\"\"A class that understands the gyp 'msvs_...' values (especially the\n    msvs_settings field). They largely correpond to the VS2008 IDE DOM. This\n    class helps map those settings to command line options.\"\"\"\n    def __init__(self, spec, generator_flags):\n        self.spec = spec\n        self.vs_version = GetVSVersion(generator_flags)\n        supported_fields = [\n            (\"msvs_configuration_attributes\", dict),\n            (\"msvs_settings\", dict),",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "PrecompiledHeader",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "class PrecompiledHeader:\n    \"\"\"Helper to generate dependencies and build rules to handle generation of\n    precompiled headers. Interface matches the GCH handler in xcode_emulation.py.\n    \"\"\"\n    def __init__(\n        self, settings, config, gyp_to_build_path, gyp_to_unique_output, obj_ext\n    ):\n        self.settings = settings\n        self.config = config\n        pch_source = self.settings.msvs_precompiled_source[self.config]",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "QuoteForRspFile",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "def QuoteForRspFile(arg, quote_cmd=True):\n    \"\"\"Quote a command line argument so that it appears as one argument when\n    processed via cmd.exe and parsed by CommandLineToArgvW (as is typical for\n    Windows programs).\"\"\"\n    # See http://goo.gl/cuFbX and http://goo.gl/dhPnp including the comment\n    # threads. This is actually the quoting rules for CommandLineToArgvW, not\n    # for the shell, because the shell doesn't do anything in Windows. This\n    # works more or less because most programs (including the compiler, etc.)\n    # use that function to handle command line arguments.\n    # Use a heuristic to try to find args that are paths, and normalize them",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "EncodeRspFileList",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "def EncodeRspFileList(args, quote_cmd):\n    \"\"\"Process a list of arguments using QuoteCmdExeArgument.\"\"\"\n    # Note that the first argument is assumed to be the command. Don't add\n    # quotes around it because then built-ins like 'echo', etc. won't work.\n    # Take care to normpath only the path in the case of 'call ../x.bat' because\n    # otherwise the whole thing is incorrectly interpreted as a path and not\n    # normalized correctly.\n    if not args:\n        return \"\"\n    if args[0].startswith(\"call \"):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "GetGlobalVSMacroEnv",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "def GetGlobalVSMacroEnv(vs_version):\n    \"\"\"Get a dict of variables mapping internal VS macro names to their gyp\n    equivalents. Returns all variables that are independent of the target.\"\"\"\n    env = {}\n    # '$(VSInstallDir)' and '$(VCInstallDir)' are available when and only when\n    # Visual Studio is actually installed.\n    if vs_version.Path():\n        env[\"$(VSInstallDir)\"] = vs_version.Path()\n        env[\"$(VCInstallDir)\"] = os.path.join(vs_version.Path(), \"VC\") + \"\\\\\"\n    # Chromium uses DXSDK_DIR in include/lib paths, but it may or may not be",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "ExtractSharedMSVSSystemIncludes",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "def ExtractSharedMSVSSystemIncludes(configs, generator_flags):\n    \"\"\"Finds msvs_system_include_dirs that are common to all targets, removes\n    them from all targets, and returns an OrderedSet containing them.\"\"\"\n    all_system_includes = OrderedSet(configs[0].get(\"msvs_system_include_dirs\", []))\n    for config in configs[1:]:\n        system_includes = config.get(\"msvs_system_include_dirs\", [])\n        all_system_includes = all_system_includes & OrderedSet(system_includes)\n    if not all_system_includes:\n        return None\n    # Expand macros in all_system_includes.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "GetVSVersion",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "def GetVSVersion(generator_flags):\n    global vs_version\n    if not vs_version:\n        vs_version = gyp.MSVSVersion.SelectVisualStudioVersion(\n            generator_flags.get(\"msvs_version\", \"auto\"), allow_fallback=False\n        )\n    return vs_version\ndef _GetVsvarsSetupArgs(generator_flags, arch):\n    vs = GetVSVersion(generator_flags)\n    return vs.SetupScript()",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "ExpandMacros",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "def ExpandMacros(string, expansions):\n    \"\"\"Expand $(Variable) per expansions dict. See MsvsSettings.GetVSMacroEnv\n    for the canonical way to retrieve a suitable dict.\"\"\"\n    if \"$\" in string:\n        for old, new in expansions.items():\n            assert \"$(\" not in new, new\n            string = string.replace(old, new)\n    return string\ndef _ExtractImportantEnvironment(output_of_set):\n    \"\"\"Extracts environment variables required for the toolchain to run from",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "GenerateEnvironmentFiles",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "def GenerateEnvironmentFiles(\n    toplevel_build_dir, generator_flags, system_includes, open_out\n):\n    \"\"\"It's not sufficient to have the absolute path to the compiler, linker,\n    etc. on Windows, as those tools rely on .dlls being in the PATH. We also\n    need to support both x86 and x64 compilers within the same build (to support\n    msvs_target_platform hackery). Different architectures require a different\n    compiler binary, and different supporting environment variables (INCLUDE,\n    LIB, LIBPATH). So, we extract the environment here, wrap all invocations\n    of compiler tools (cl, link, lib, rc, midl, etc.) via win_tool.py which",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "VerifyMissingSources",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "def VerifyMissingSources(sources, build_dir, generator_flags, gyp_to_ninja):\n    \"\"\"Emulate behavior of msvs_error_on_missing_sources present in the msvs\n    generator: Check that all regular source files, i.e. not created at run time,\n    exist on disk. Missing files cause needless recompilation when building via\n    VS, and we want this check to match for people/bots that build using ninja,\n    so they're not surprised when the VS build fails.\"\"\"\n    if int(generator_flags.get(\"msvs_error_on_missing_sources\", 0)):\n        no_specials = filter(lambda x: \"$\" not in x, sources)\n        relative = [os.path.join(build_dir, gyp_to_ninja(s)) for s in no_specials]\n        missing = [x for x in relative if not os.path.exists(x)]",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "CalculateCommonVariables",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "def CalculateCommonVariables(default_variables, params):\n    generator_flags = params.get(\"generator_flags\", {})\n    # Set a variable so conditions can be based on msvs_version.\n    msvs_version = gyp.msvs_emulation.GetVSVersion(generator_flags)\n    default_variables[\"MSVS_VERSION\"] = msvs_version.ShortName()\n    # To determine processor word size on Windows, in addition to checking\n    # PROCESSOR_ARCHITECTURE (which reflects the word size of the current\n    # process), it is also necessary to check PROCESSOR_ARCHITEW6432 (which\n    # contains the actual word size of the system when running thru WOW64).\n    if \"64\" in os.environ.get(\"PROCESSOR_ARCHITECTURE\", \"\") or \"64\" in os.environ.get(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "windows_quoter_regex",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "windows_quoter_regex = re.compile(r'(\\\\*)\"')\ndef QuoteForRspFile(arg, quote_cmd=True):\n    \"\"\"Quote a command line argument so that it appears as one argument when\n    processed via cmd.exe and parsed by CommandLineToArgvW (as is typical for\n    Windows programs).\"\"\"\n    # See http://goo.gl/cuFbX and http://goo.gl/dhPnp including the comment\n    # threads. This is actually the quoting rules for CommandLineToArgvW, not\n    # for the shell, because the shell doesn't do anything in Windows. This\n    # works more or less because most programs (including the compiler, etc.)\n    # use that function to handle command line arguments.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "vs_version",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "peekOfCode": "vs_version = None\ndef GetVSVersion(generator_flags):\n    global vs_version\n    if not vs_version:\n        vs_version = gyp.MSVSVersion.SelectVisualStudioVersion(\n            generator_flags.get(\"msvs_version\", \"auto\"), allow_fallback=False\n        )\n    return vs_version\ndef _GetVsvarsSetupArgs(generator_flags, arch):\n    vs = GetVSVersion(generator_flags)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.msvs_emulation",
        "documentation": {}
    },
    {
        "label": "Writer",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.ninja_syntax",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.ninja_syntax",
        "peekOfCode": "class Writer:\n    def __init__(self, output, width=78):\n        self.output = output\n        self.width = width\n    def newline(self):\n        self.output.write(\"\\n\")\n    def comment(self, text):\n        for line in textwrap.wrap(text, self.width - 2):\n            self.output.write(\"# \" + line + \"\\n\")\n    def variable(self, key, value, indent=0):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.ninja_syntax",
        "documentation": {}
    },
    {
        "label": "escape_path",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.ninja_syntax",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.ninja_syntax",
        "peekOfCode": "def escape_path(word):\n    return word.replace(\"$ \", \"$$ \").replace(\" \", \"$ \").replace(\":\", \"$:\")\nclass Writer:\n    def __init__(self, output, width=78):\n        self.output = output\n        self.width = width\n    def newline(self):\n        self.output.write(\"\\n\")\n    def comment(self, text):\n        for line in textwrap.wrap(text, self.width - 2):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.ninja_syntax",
        "documentation": {}
    },
    {
        "label": "escape",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.ninja_syntax",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.ninja_syntax",
        "peekOfCode": "def escape(string):\n    \"\"\"Escape a string such that it can be embedded into a Ninja file without\n    further interpretation.\"\"\"\n    assert \"\\n\" not in string, \"Ninja syntax does not allow newlines\"\n    # We only have one special metacharacter: '$'.\n    return string.replace(\"$\", \"$$\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.ninja_syntax",
        "documentation": {}
    },
    {
        "label": "Error",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "peekOfCode": "class Error(Exception):\n    pass\n__all__ = [\"Error\", \"deepcopy\"]\ndef deepcopy(x):\n    \"\"\"Deep copy operation on gyp objects such as strings, ints, dicts\n    and lists. More than twice as fast as copy.deepcopy but much less\n    generic.\"\"\"\n    try:\n        return _deepcopy_dispatch[type(x)](x)\n    except KeyError:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "peekOfCode": "def deepcopy(x):\n    \"\"\"Deep copy operation on gyp objects such as strings, ints, dicts\n    and lists. More than twice as fast as copy.deepcopy but much less\n    generic.\"\"\"\n    try:\n        return _deepcopy_dispatch[type(x)](x)\n    except KeyError:\n        raise Error(\n            \"Unsupported type %s for deepcopy. Use copy.deepcopy \"\n            + \"or expand simple_copy support.\" % type(x)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "peekOfCode": "__all__ = [\"Error\", \"deepcopy\"]\ndef deepcopy(x):\n    \"\"\"Deep copy operation on gyp objects such as strings, ints, dicts\n    and lists. More than twice as fast as copy.deepcopy but much less\n    generic.\"\"\"\n    try:\n        return _deepcopy_dispatch[type(x)](x)\n    except KeyError:\n        raise Error(\n            \"Unsupported type %s for deepcopy. Use copy.deepcopy \"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "documentation": {}
    },
    {
        "label": "_deepcopy_dispatch",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "peekOfCode": "_deepcopy_dispatch = d = {}\ndef _deepcopy_atomic(x):\n    return x\ntypes = bool, float, int, str, type, type(None)\nfor x in types:\n    d[x] = _deepcopy_atomic\ndef _deepcopy_list(x):\n    return [deepcopy(a) for a in x]\nd[list] = _deepcopy_list",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "documentation": {}
    },
    {
        "label": "types",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "peekOfCode": "types = bool, float, int, str, type, type(None)\nfor x in types:\n    d[x] = _deepcopy_atomic\ndef _deepcopy_list(x):\n    return [deepcopy(a) for a in x]\nd[list] = _deepcopy_list\ndef _deepcopy_dict(x):\n    y = {}\n    for key, value in x.items():\n        y[deepcopy(key)] = deepcopy(value)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "documentation": {}
    },
    {
        "label": "d[list]",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "peekOfCode": "d[list] = _deepcopy_list\ndef _deepcopy_dict(x):\n    y = {}\n    for key, value in x.items():\n        y[deepcopy(key)] = deepcopy(value)\n    return y\nd[dict] = _deepcopy_dict\ndel d",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "documentation": {}
    },
    {
        "label": "d[dict]",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "peekOfCode": "d[dict] = _deepcopy_dict\ndel d",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.simple_copy",
        "documentation": {}
    },
    {
        "label": "WinTool",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "peekOfCode": "class WinTool:\n    \"\"\"This class performs all the Windows tooling steps. The methods can either\n    be executed directly, or dispatched from an argument list.\"\"\"\n    def _UseSeparateMspdbsrv(self, env, args):\n        \"\"\"Allows to use a unique instance of mspdbsrv.exe per linker instead of a\n        shared one.\"\"\"\n        if len(args) < 1:\n            raise Exception(\"Not enough arguments\")\n        if args[0] != \"link.exe\":\n            return",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "peekOfCode": "def main(args):\n    executor = WinTool()\n    if (exit_code := executor.Dispatch(args)) is not None:\n        sys.exit(exit_code)\nclass WinTool:\n    \"\"\"This class performs all the Windows tooling steps. The methods can either\n    be executed directly, or dispatched from an argument list.\"\"\"\n    def _UseSeparateMspdbsrv(self, env, args):\n        \"\"\"Allows to use a unique instance of mspdbsrv.exe per linker instead of a\n        shared one.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "peekOfCode": "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n# A regex matching an argument corresponding to the output filename passed to\n# link.exe.\n_LINK_EXE_OUT_ARG = re.compile(\"/OUT:(?P<out>.+)$\", re.IGNORECASE)\ndef main(args):\n    executor = WinTool()\n    if (exit_code := executor.Dispatch(args)) is not None:\n        sys.exit(exit_code)\nclass WinTool:\n    \"\"\"This class performs all the Windows tooling steps. The methods can either",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "documentation": {}
    },
    {
        "label": "_LINK_EXE_OUT_ARG",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "peekOfCode": "_LINK_EXE_OUT_ARG = re.compile(\"/OUT:(?P<out>.+)$\", re.IGNORECASE)\ndef main(args):\n    executor = WinTool()\n    if (exit_code := executor.Dispatch(args)) is not None:\n        sys.exit(exit_code)\nclass WinTool:\n    \"\"\"This class performs all the Windows tooling steps. The methods can either\n    be executed directly, or dispatched from an argument list.\"\"\"\n    def _UseSeparateMspdbsrv(self, env, args):\n        \"\"\"Allows to use a unique instance of mspdbsrv.exe per linker instead of a",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.win_tool",
        "documentation": {}
    },
    {
        "label": "XcodeArchsDefault",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "class XcodeArchsDefault:\n    \"\"\"A class to resolve ARCHS variable from xcode_settings, resolving Xcode\n    macros and implementing filtering by VALID_ARCHS. The expansion of macros\n    depends on the SDKROOT used (\"macosx\", \"iphoneos\", \"iphonesimulator\") and\n    on the version of Xcode.\n    \"\"\"\n    # Match variable like $(ARCHS_STANDARD).\n    variable_pattern = re.compile(r\"\\$\\([a-zA-Z_][a-zA-Z0-9_]*\\)$\")\n    def __init__(self, default, mac, iphonesimulator, iphoneos):\n        self._default = (default,)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "XcodeSettings",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "class XcodeSettings:\n    \"\"\"A class that understands the gyp 'xcode_settings' object.\"\"\"\n    # Populated lazily by _SdkPath(). Shared by all XcodeSettings, so cached\n    # at class-level for efficiency.\n    _sdk_path_cache = {}\n    _platform_path_cache = {}\n    _sdk_root_cache = {}\n    # Populated lazily by GetExtraPlistItems(). Shared by all XcodeSettings, so\n    # cached at class-level for efficiency.\n    _plist_cache = {}",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "MacPrefixHeader",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "class MacPrefixHeader:\n    \"\"\"A class that helps with emulating Xcode's GCC_PREFIX_HEADER feature.\n    This feature consists of several pieces:\n    * If GCC_PREFIX_HEADER is present, all compilations in that project get an\n      additional |-include path_to_prefix_header| cflag.\n    * If GCC_PRECOMPILE_PREFIX_HEADER is present too, then the prefix header is\n      instead compiled, and all other compilations in the project get an\n      additional |-include path_to_compiled_header| instead.\n      + Compiled prefix headers have the extension gch. There is one gch file for\n        every language used in the project (c, cc, m, mm), since gch files for",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "XcodeArchsVariableMapping",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def XcodeArchsVariableMapping(archs, archs_including_64_bit=None):\n    \"\"\"Constructs a dictionary with expansion for $(ARCHS_STANDARD) variable,\n    and optionally for $(ARCHS_STANDARD_INCLUDING_64_BIT).\"\"\"\n    mapping = {\"$(ARCHS_STANDARD)\": archs}\n    if archs_including_64_bit:\n        mapping[\"$(ARCHS_STANDARD_INCLUDING_64_BIT)\"] = archs_including_64_bit\n    return mapping\nclass XcodeArchsDefault:\n    \"\"\"A class to resolve ARCHS variable from xcode_settings, resolving Xcode\n    macros and implementing filtering by VALID_ARCHS. The expansion of macros",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "GetXcodeArchsDefault",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def GetXcodeArchsDefault():\n    \"\"\"Returns the |XcodeArchsDefault| object to use to expand ARCHS for the\n    installed version of Xcode. The default values used by Xcode for ARCHS\n    and the expansion of the variables depends on the version of Xcode used.\n    For all version anterior to Xcode 5.0 or posterior to Xcode 5.1 included\n    uses $(ARCHS_STANDARD) if ARCHS is unset, while Xcode 5.0 to 5.0.2 uses\n    $(ARCHS_STANDARD_INCLUDING_64_BIT). This variable was added to Xcode 5.0\n    and deprecated with Xcode 5.1.\n    For \"macosx\" SDKROOT, all version starting with Xcode 5.0 includes 64-bit\n    architecture as part of $(ARCHS_STANDARD) and default to only building it.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "XcodeVersion",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def XcodeVersion():\n    \"\"\"Returns a tuple of version and build version of installed Xcode.\"\"\"\n    # `xcodebuild -version` output looks like\n    #    Xcode 4.6.3\n    #    Build version 4H1503\n    # or like\n    #    Xcode 3.2.6\n    #    Component versions: DevToolsCore-1809.0; DevToolsSupport-1806.0\n    #    BuildVersion: 10M2518\n    # Convert that to ('0463', '4H1503') or ('0326', '10M2518').",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "CLTVersion",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def CLTVersion():\n    \"\"\"Returns the version of command-line tools from pkgutil.\"\"\"\n    # pkgutil output looks like\n    #   package-id: com.apple.pkg.CLTools_Executables\n    #   version: 5.0.1.0.1.1382131676\n    #   volume: /\n    #   location: /\n    #   install-time: 1382544035\n    #   groups: com.apple.FindSystemFiles.pkg-group\n    #           com.apple.DevToolsBoth.pkg-group",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "GetStdoutQuiet",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def GetStdoutQuiet(cmdlist):\n    \"\"\"Returns the content of standard output returned by invoking |cmdlist|.\n    Ignores the stderr.\n    Raises |GypError| if the command return with a non-zero return code.\"\"\"\n    job = subprocess.Popen(cmdlist, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out = job.communicate()[0].decode(\"utf-8\")\n    if job.returncode != 0:\n        raise GypError(\"Error %d running %s\" % (job.returncode, cmdlist[0]))\n    return out.rstrip(\"\\n\")\ndef GetStdout(cmdlist):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "GetStdout",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def GetStdout(cmdlist):\n    \"\"\"Returns the content of standard output returned by invoking |cmdlist|.\n    Raises |GypError| if the command return with a non-zero return code.\"\"\"\n    job = subprocess.Popen(cmdlist, stdout=subprocess.PIPE)\n    out = job.communicate()[0].decode(\"utf-8\")\n    if job.returncode != 0:\n        sys.stderr.write(out + \"\\n\")\n        raise GypError(\"Error %d running %s\" % (job.returncode, cmdlist[0]))\n    return out.rstrip(\"\\n\")\ndef MergeGlobalXcodeSettingsToSpec(global_dict, spec):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "MergeGlobalXcodeSettingsToSpec",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def MergeGlobalXcodeSettingsToSpec(global_dict, spec):\n    \"\"\"Merges the global xcode_settings dictionary into each configuration of the\n    target represented by spec. For keys that are both in the global and the local\n    xcode_settings dict, the local key gets precedence.\n    \"\"\"\n    # The xcode generator special-cases global xcode_settings and does something\n    # that amounts to merging in the global xcode_settings into each local\n    # xcode_settings dict.\n    global_xcode_settings = global_dict.get(\"xcode_settings\", {})\n    for config in spec[\"configurations\"].values():",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "IsMacBundle",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def IsMacBundle(flavor, spec):\n    \"\"\"Returns if |spec| should be treated as a bundle.\n    Bundles are directories with a certain subdirectory structure, instead of\n    just a single file. Bundle rules do not produce a binary but also package\n    resources into that directory.\"\"\"\n    is_mac_bundle = (\n        int(spec.get(\"mac_xctest_bundle\", 0)) != 0\n        or int(spec.get(\"mac_xcuitest_bundle\", 0)) != 0\n        or (int(spec.get(\"mac_bundle\", 0)) != 0 and flavor == \"mac\")\n    )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "GetMacBundleResources",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def GetMacBundleResources(product_dir, xcode_settings, resources):\n    \"\"\"Yields (output, resource) pairs for every resource in |resources|.\n    Only call this for mac bundle targets.\n    Args:\n        product_dir: Path to the directory containing the output bundle,\n            relative to the build directory.\n        xcode_settings: The XcodeSettings of the current target.\n        resources: A list of bundle resources, relative to the build directory.\n    \"\"\"\n    dest = os.path.join(product_dir, xcode_settings.GetBundleResourceFolder())",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "GetMacInfoPlist",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def GetMacInfoPlist(product_dir, xcode_settings, gyp_path_to_build_path):\n    \"\"\"Returns (info_plist, dest_plist, defines, extra_env), where:\n    * |info_plist| is the source plist path, relative to the\n      build directory,\n    * |dest_plist| is the destination plist path, relative to the\n      build directory,\n    * |defines| is a list of preprocessor defines (empty if the plist\n      shouldn't be preprocessed,\n    * |extra_env| is a dict of env variables that should be exported when\n      invoking |mac_tool copy-info-plist|.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "ExpandEnvVars",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def ExpandEnvVars(string, expansions):\n    \"\"\"Expands ${VARIABLES}, $(VARIABLES), and $VARIABLES in string per the\n    expansions list. If the variable expands to something that references\n    another variable, this variable is expanded as well if it's in env --\n    until no variables present in env are left.\"\"\"\n    for k, v in reversed(expansions):\n        string = string.replace(\"${\" + k + \"}\", v)\n        string = string.replace(\"$(\" + k + \")\", v)\n        string = string.replace(\"$\" + k, v)\n    return string",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "GetSortedXcodeEnv",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def GetSortedXcodeEnv(\n    xcode_settings, built_products_dir, srcroot, configuration, additional_settings=None\n):\n    env = _GetXcodeEnv(\n        xcode_settings, built_products_dir, srcroot, configuration, additional_settings\n    )\n    return [(key, env[key]) for key in _TopologicallySortedEnvVarKeys(env)]\ndef GetSpecPostbuildCommands(spec, quiet=False):\n    \"\"\"Returns the list of postbuilds explicitly defined on |spec|, in a form\n    executable by a shell.\"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "GetSpecPostbuildCommands",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def GetSpecPostbuildCommands(spec, quiet=False):\n    \"\"\"Returns the list of postbuilds explicitly defined on |spec|, in a form\n    executable by a shell.\"\"\"\n    postbuilds = []\n    for postbuild in spec.get(\"postbuilds\", []):\n        if not quiet:\n            postbuilds.append(\n                \"echo POSTBUILD\\\\(%s\\\\) %s\"\n                % (spec[\"target_name\"], postbuild[\"postbuild_name\"])\n            )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "CloneConfigurationForDeviceAndEmulator",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "def CloneConfigurationForDeviceAndEmulator(target_dicts):\n    \"\"\"If |target_dicts| contains any iOS targets, automatically create -iphoneos\n    targets for iOS device builds.\"\"\"\n    if _HasIOSTarget(target_dicts):\n        return _AddIOSDeviceConfigurations(target_dicts)\n    return target_dicts",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "XCODE_VERSION_CACHE",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "XCODE_VERSION_CACHE = None\n# Populated lazily by GetXcodeArchsDefault, to an |XcodeArchsDefault| instance\n# corresponding to the installed version of Xcode.\nXCODE_ARCHS_DEFAULT_CACHE = None\ndef XcodeArchsVariableMapping(archs, archs_including_64_bit=None):\n    \"\"\"Constructs a dictionary with expansion for $(ARCHS_STANDARD) variable,\n    and optionally for $(ARCHS_STANDARD_INCLUDING_64_BIT).\"\"\"\n    mapping = {\"$(ARCHS_STANDARD)\": archs}\n    if archs_including_64_bit:\n        mapping[\"$(ARCHS_STANDARD_INCLUDING_64_BIT)\"] = archs_including_64_bit",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "XCODE_ARCHS_DEFAULT_CACHE",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "peekOfCode": "XCODE_ARCHS_DEFAULT_CACHE = None\ndef XcodeArchsVariableMapping(archs, archs_including_64_bit=None):\n    \"\"\"Constructs a dictionary with expansion for $(ARCHS_STANDARD) variable,\n    and optionally for $(ARCHS_STANDARD_INCLUDING_64_BIT).\"\"\"\n    mapping = {\"$(ARCHS_STANDARD)\": archs}\n    if archs_including_64_bit:\n        mapping[\"$(ARCHS_STANDARD_INCLUDING_64_BIT)\"] = archs_including_64_bit\n    return mapping\nclass XcodeArchsDefault:\n    \"\"\"A class to resolve ARCHS variable from xcode_settings, resolving Xcode",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation",
        "documentation": {}
    },
    {
        "label": "TestXcodeSettings",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation_test",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation_test",
        "peekOfCode": "class TestXcodeSettings(unittest.TestCase):\n    def setUp(self):\n        if sys.platform != \"darwin\":\n            self.skipTest(\"This test only runs on macOS\")\n    def test_GetCflags(self):\n        target = {\n            \"type\": \"static_library\",\n            \"configurations\": {\n                \"Release\": {},\n            },",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_emulation_test",
        "documentation": {}
    },
    {
        "label": "IsValidTargetForWrapper",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_ninja",
        "peekOfCode": "def IsValidTargetForWrapper(target_extras, executable_target_pattern, spec):\n    \"\"\"Limit targets for Xcode wrapper.\n    Xcode sometimes performs poorly with too many targets, so only include\n    proper executable targets, with filters to customize.\n    Arguments:\n      target_extras: Regular expression to always add, matching any target.\n      executable_target_pattern: Regular expression limiting executable targets.\n      spec: Specifications for target.\n    \"\"\"\n    target_name = spec.get(\"target_name\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_ninja",
        "documentation": {}
    },
    {
        "label": "CreateWrapper",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_ninja",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_ninja",
        "peekOfCode": "def CreateWrapper(target_list, target_dicts, data, params):\n    \"\"\"Initialize targets for the ninja wrapper.\n    This sets up the necessary variables in the targets to generate Xcode projects\n    that use ninja as an external builder.\n    Arguments:\n      target_list: List of target pairs: 'base/base.gyp:base'.\n      target_dicts: Dict of target properties keyed on target pair.\n      data: Dict of flattened build files keyed on gyp path.\n      params: Dict of global options for gyp.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcode_ninja",
        "documentation": {}
    },
    {
        "label": "XCObject",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class XCObject:\n    \"\"\"The abstract base of all class types used in Xcode project files.\n    Class variables:\n      _schema: A dictionary defining the properties of this class.  The keys to\n               _schema are string property keys as used in project files.  Values\n               are a list of four or five elements:\n               [ is_list, property_type, is_strong, is_required, default ]\n               is_list: True if the property described is a list, as opposed\n                        to a single element.\n               property_type: The type to use as the value of the property,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "XCHierarchicalElement",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class XCHierarchicalElement(XCObject):\n    \"\"\"Abstract base for PBXGroup and PBXFileReference.  Not represented in a\n    project file.\"\"\"\n    # TODO(mark): Do name and path belong here?  Probably so.\n    # If path is set and name is not, name may have a default value.  Name will\n    # be set to the basename of path, if the basename of path is different from\n    # the full value of path.  If path is already just a leaf name, name will\n    # not be set.\n    _schema = XCObject._schema.copy()\n    _schema.update(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXGroup",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXGroup(XCHierarchicalElement):\n    \"\"\"\n    Attributes:\n      _children_by_path: Maps pathnames of children of this PBXGroup to the\n        actual child XCHierarchicalElement objects.\n      _variant_children_by_name_and_path: Maps (name, path) tuples of\n        PBXVariantGroup children to the actual child PBXVariantGroup objects.\n    \"\"\"\n    _schema = XCHierarchicalElement._schema.copy()\n    _schema.update(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "XCFileLikeElement",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class XCFileLikeElement(XCHierarchicalElement):\n    # Abstract base for objects that can be used as the fileRef property of\n    # PBXBuildFile.\n    def PathHashables(self):\n        # A PBXBuildFile that refers to this object will call this method to\n        # obtain additional hashables specific to this XCFileLikeElement.  Don't\n        # just use this object's hashables, they're not specific and unique enough\n        # on their own (without access to the parent hashables.)  Instead, provide\n        # hashables that identify this object by path by getting its hashables as\n        # well as the hashables of ancestor XCHierarchicalElement objects.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "XCContainerPortal",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class XCContainerPortal(XCObject):\n    # Abstract base for objects that can be used as the containerPortal property\n    # of PBXContainerItemProxy.\n    pass\nclass XCRemoteObject(XCObject):\n    # Abstract base for objects that can be used as the remoteGlobalIDString\n    # property of PBXContainerItemProxy.\n    pass\nclass PBXFileReference(XCFileLikeElement, XCContainerPortal, XCRemoteObject):\n    _schema = XCFileLikeElement._schema.copy()",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "XCRemoteObject",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class XCRemoteObject(XCObject):\n    # Abstract base for objects that can be used as the remoteGlobalIDString\n    # property of PBXContainerItemProxy.\n    pass\nclass PBXFileReference(XCFileLikeElement, XCContainerPortal, XCRemoteObject):\n    _schema = XCFileLikeElement._schema.copy()\n    _schema.update(\n        {\n            \"explicitFileType\": [0, str, 0, 0],\n            \"lastKnownFileType\": [0, str, 0, 0],",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXFileReference",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXFileReference(XCFileLikeElement, XCContainerPortal, XCRemoteObject):\n    _schema = XCFileLikeElement._schema.copy()\n    _schema.update(\n        {\n            \"explicitFileType\": [0, str, 0, 0],\n            \"lastKnownFileType\": [0, str, 0, 0],\n            \"name\": [0, str, 0, 0],\n            \"path\": [0, str, 0, 1],\n        }\n    )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXVariantGroup",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXVariantGroup(PBXGroup, XCFileLikeElement):\n    \"\"\"PBXVariantGroup is used by Xcode to represent localizations.\"\"\"\n    # No additions to the schema relative to PBXGroup.\n# PBXReferenceProxy is also an XCFileLikeElement subclass.  It is defined below\n# because it uses PBXContainerItemProxy, defined below.\nclass XCBuildConfiguration(XCObject):\n    _schema = XCObject._schema.copy()\n    _schema.update(\n        {\n            \"baseConfigurationReference\": [0, PBXFileReference, 0, 0],",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "XCBuildConfiguration",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class XCBuildConfiguration(XCObject):\n    _schema = XCObject._schema.copy()\n    _schema.update(\n        {\n            \"baseConfigurationReference\": [0, PBXFileReference, 0, 0],\n            \"buildSettings\": [0, dict, 0, 1, {}],\n            \"name\": [0, str, 0, 1],\n        }\n    )\n    def HasBuildSetting(self, key):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "XCConfigurationList",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class XCConfigurationList(XCObject):\n    # _configs is the default list of configurations.\n    _configs = [\n        XCBuildConfiguration({\"name\": \"Debug\"}),\n        XCBuildConfiguration({\"name\": \"Release\"}),\n    ]\n    _schema = XCObject._schema.copy()\n    _schema.update(\n        {\n            \"buildConfigurations\": [1, XCBuildConfiguration, 1, 1, _configs],",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXBuildFile",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXBuildFile(XCObject):\n    _schema = XCObject._schema.copy()\n    _schema.update(\n        {\n            \"fileRef\": [0, XCFileLikeElement, 0, 1],\n            \"settings\": [0, str, 0, 0],  # hack, it's a dict\n        }\n    )\n    # Weird output rules for PBXBuildFile.\n    _should_print_single_line = True",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "XCBuildPhase",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class XCBuildPhase(XCObject):\n    \"\"\"Abstract base for build phase classes.  Not represented in a project\n    file.\n    Attributes:\n      _files_by_path: A dict mapping each path of a child in the files list by\n        path (keys) to the corresponding PBXBuildFile children (values).\n      _files_by_xcfilelikeelement: A dict mapping each XCFileLikeElement (keys)\n        to the corresponding PBXBuildFile children (values).\n    \"\"\"\n    # TODO(mark): Some build phase types, like PBXShellScriptBuildPhase, don't",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXHeadersBuildPhase",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXHeadersBuildPhase(XCBuildPhase):\n    # No additions to the schema relative to XCBuildPhase.\n    def Name(self):\n        return \"Headers\"\n    def FileGroup(self, path):\n        return self.PBXProjectAncestor().RootGroupForPath(path)\nclass PBXResourcesBuildPhase(XCBuildPhase):\n    # No additions to the schema relative to XCBuildPhase.\n    def Name(self):\n        return \"Resources\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXResourcesBuildPhase",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXResourcesBuildPhase(XCBuildPhase):\n    # No additions to the schema relative to XCBuildPhase.\n    def Name(self):\n        return \"Resources\"\n    def FileGroup(self, path):\n        return self.PBXProjectAncestor().RootGroupForPath(path)\nclass PBXSourcesBuildPhase(XCBuildPhase):\n    # No additions to the schema relative to XCBuildPhase.\n    def Name(self):\n        return \"Sources\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXSourcesBuildPhase",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXSourcesBuildPhase(XCBuildPhase):\n    # No additions to the schema relative to XCBuildPhase.\n    def Name(self):\n        return \"Sources\"\n    def FileGroup(self, path):\n        return self.PBXProjectAncestor().RootGroupForPath(path)\nclass PBXFrameworksBuildPhase(XCBuildPhase):\n    # No additions to the schema relative to XCBuildPhase.\n    def Name(self):\n        return \"Frameworks\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXFrameworksBuildPhase",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXFrameworksBuildPhase(XCBuildPhase):\n    # No additions to the schema relative to XCBuildPhase.\n    def Name(self):\n        return \"Frameworks\"\n    def FileGroup(self, path):\n        (_root, ext) = posixpath.splitext(path)\n        if ext != \"\":\n            ext = ext[1:].lower()\n        if ext == \"o\":\n            # .o files are added to Xcode Frameworks phases, but conceptually aren't",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXShellScriptBuildPhase",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXShellScriptBuildPhase(XCBuildPhase):\n    _schema = XCBuildPhase._schema.copy()\n    _schema.update(\n        {\n            \"inputPaths\": [1, str, 0, 1, []],\n            \"name\": [0, str, 0, 0],\n            \"outputPaths\": [1, str, 0, 1, []],\n            \"shellPath\": [0, str, 0, 1, \"/bin/sh\"],\n            \"shellScript\": [0, str, 0, 1],\n            \"showEnvVarsInLog\": [0, int, 0, 0],",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXCopyFilesBuildPhase",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXCopyFilesBuildPhase(XCBuildPhase):\n    _schema = XCBuildPhase._schema.copy()\n    _schema.update(\n        {\n            \"dstPath\": [0, str, 0, 1],\n            \"dstSubfolderSpec\": [0, int, 0, 1],\n            \"name\": [0, str, 0, 0],\n        }\n    )\n    # path_tree_re matches \"$(DIR)/path\", \"$(DIR)/$(DIR2)/path\" or just \"$(DIR)\".",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXBuildRule",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXBuildRule(XCObject):\n    _schema = XCObject._schema.copy()\n    _schema.update(\n        {\n            \"compilerSpec\": [0, str, 0, 1],\n            \"filePatterns\": [0, str, 0, 0],\n            \"fileType\": [0, str, 0, 1],\n            \"isEditable\": [0, int, 0, 1, 1],\n            \"outputFiles\": [1, str, 0, 1, []],\n            \"script\": [0, str, 0, 0],",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXContainerItemProxy",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXContainerItemProxy(XCObject):\n    # When referencing an item in this project file, containerPortal is the\n    # PBXProject root object of this project file.  When referencing an item in\n    # another project file, containerPortal is a PBXFileReference identifying\n    # the other project file.\n    #\n    # When serving as a proxy to an XCTarget (in this project file or another),\n    # proxyType is 1.  When serving as a proxy to a PBXFileReference (in another\n    # project file), proxyType is 2.  Type 2 is used for references to the\n    # producs of the other project file's targets.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXTargetDependency",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXTargetDependency(XCObject):\n    # The \"target\" property accepts an XCTarget object, and obviously not\n    # NoneType.  But XCTarget is defined below, so it can't be put into the\n    # schema yet.  The definition of PBXTargetDependency can't be moved below\n    # XCTarget because XCTarget's own schema references PBXTargetDependency.\n    # Python doesn't deal well with this circular relationship, and doesn't have\n    # a real way to do forward declarations.  To work around, the type of\n    # the \"target\" property is reset below, after XCTarget is defined.\n    #\n    # At least one of \"name\" and \"target\" is required.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXReferenceProxy",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXReferenceProxy(XCFileLikeElement):\n    _schema = XCFileLikeElement._schema.copy()\n    _schema.update(\n        {\n            \"fileType\": [0, str, 0, 1],\n            \"path\": [0, str, 0, 1],\n            \"remoteRef\": [0, PBXContainerItemProxy, 1, 1],\n        }\n    )\nclass XCTarget(XCRemoteObject):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "XCTarget",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class XCTarget(XCRemoteObject):\n    # An XCTarget is really just an XCObject, the XCRemoteObject thing is just\n    # to allow PBXProject to be used in the remoteGlobalIDString property of\n    # PBXContainerItemProxy.\n    #\n    # Setting a \"name\" property at instantiation may also affect \"productName\",\n    # which may in turn affect the \"PRODUCT_NAME\" build setting in children of\n    # \"buildConfigurationList\".  See __init__ below.\n    _schema = XCRemoteObject._schema.copy()\n    _schema.update(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXNativeTarget",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXNativeTarget(XCTarget):\n    # buildPhases is overridden in the schema to be able to set defaults.\n    #\n    # NOTE: Contrary to most objects, it is advisable to set parent when\n    # constructing PBXNativeTarget.  A parent of an XCTarget must be a PBXProject\n    # object.  A parent reference is required for a PBXNativeTarget during\n    # construction to be able to set up the target defaults for productReference,\n    # because a PBXBuildFile object must be created for the target and it must\n    # be added to the PBXProject's mainGroup hierarchy.\n    _schema = XCTarget._schema.copy()",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXAggregateTarget",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXAggregateTarget(XCTarget):\n    pass\nclass PBXProject(XCContainerPortal):\n    # A PBXProject is really just an XCObject, the XCContainerPortal thing is\n    # just to allow PBXProject to be used in the containerPortal property of\n    # PBXContainerItemProxy.\n    \"\"\"\n    Attributes:\n      path: \"sample.xcodeproj\".  TODO(mark) Document me!\n      _other_pbxprojects: A dictionary, keyed by other PBXProject objects.  Each",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXProject",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class PBXProject(XCContainerPortal):\n    # A PBXProject is really just an XCObject, the XCContainerPortal thing is\n    # just to allow PBXProject to be used in the containerPortal property of\n    # PBXContainerItemProxy.\n    \"\"\"\n    Attributes:\n      path: \"sample.xcodeproj\".  TODO(mark) Document me!\n      _other_pbxprojects: A dictionary, keyed by other PBXProject objects.  Each\n                          value is a reference to the dict in the\n                          projectReferences list associated with the keyed",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "XCProjectFile",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "class XCProjectFile(XCObject):\n    _schema = XCObject._schema.copy()\n    _schema.update(\n        {\n            \"archiveVersion\": [0, int, 0, 1, 1],\n            \"classes\": [0, dict, 0, 1, {}],\n            \"objectVersion\": [0, int, 0, 1, 46],\n            \"rootObject\": [0, PBXProject, 1, 1],\n        }\n    )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "cmp",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "def cmp(x, y):\n    return (x > y) - (x < y)\n# See XCObject._EncodeString.  This pattern is used to determine when a string\n# can be printed unquoted.  Strings that match this pattern may be printed\n# unquoted.  Strings that do not match must be quoted and may be further\n# transformed to be properly encoded.  Note that this expression matches the\n# characters listed with \"+\", for 1 or more occurrences: if a string is empty,\n# it must not match this pattern, because it needs to be encoded as \"\".\n_unquoted = re.compile(\"^[A-Za-z0-9$./_]+$\")\n# Strings that match this pattern are quoted regardless of what _unquoted says.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "SourceTreeAndPathFromPath",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "def SourceTreeAndPathFromPath(input_path):\n    \"\"\"Given input_path, returns a tuple with sourceTree and path values.\n    Examples:\n      input_path     (source_tree, output_path)\n      '$(VAR)/path'  ('VAR', 'path')\n      '$(VAR)'       ('VAR', None)\n      'path'         (None, 'path')\n    \"\"\"\n    if source_group_match := _path_leading_variable.match(input_path):\n        source_tree = source_group_match.group(1)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "ConvertVariablesToShellSyntax",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "def ConvertVariablesToShellSyntax(input_string):\n    return re.sub(r\"\\$\\((.*?)\\)\", \"${\\\\1}\", input_string)\nclass XCObject:\n    \"\"\"The abstract base of all class types used in Xcode project files.\n    Class variables:\n      _schema: A dictionary defining the properties of this class.  The keys to\n               _schema are string property keys as used in project files.  Values\n               are a list of four or five elements:\n               [ is_list, property_type, is_strong, is_required, default ]\n               is_list: True if the property described is a list, as opposed",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "_unquoted",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "_unquoted = re.compile(\"^[A-Za-z0-9$./_]+$\")\n# Strings that match this pattern are quoted regardless of what _unquoted says.\n# Oddly, Xcode will quote any string with a run of three or more underscores.\n_quoted = re.compile(\"___\")\n# This pattern should match any character that needs to be escaped by\n# XCObject._EncodeString.  See that function.\n_escaped = re.compile('[\\\\\\\\\"]|[\\x00-\\x1f]')\n# Used by SourceTreeAndPathFromPath\n_path_leading_variable = re.compile(r\"^\\$\\((.*?)\\)(/(.*))?$\")\ndef SourceTreeAndPathFromPath(input_path):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "_quoted",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "_quoted = re.compile(\"___\")\n# This pattern should match any character that needs to be escaped by\n# XCObject._EncodeString.  See that function.\n_escaped = re.compile('[\\\\\\\\\"]|[\\x00-\\x1f]')\n# Used by SourceTreeAndPathFromPath\n_path_leading_variable = re.compile(r\"^\\$\\((.*?)\\)(/(.*))?$\")\ndef SourceTreeAndPathFromPath(input_path):\n    \"\"\"Given input_path, returns a tuple with sourceTree and path values.\n    Examples:\n      input_path     (source_tree, output_path)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "_escaped",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "_escaped = re.compile('[\\\\\\\\\"]|[\\x00-\\x1f]')\n# Used by SourceTreeAndPathFromPath\n_path_leading_variable = re.compile(r\"^\\$\\((.*?)\\)(/(.*))?$\")\ndef SourceTreeAndPathFromPath(input_path):\n    \"\"\"Given input_path, returns a tuple with sourceTree and path values.\n    Examples:\n      input_path     (source_tree, output_path)\n      '$(VAR)/path'  ('VAR', 'path')\n      '$(VAR)'       ('VAR', None)\n      'path'         (None, 'path')",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "_path_leading_variable",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "_path_leading_variable = re.compile(r\"^\\$\\((.*?)\\)(/(.*))?$\")\ndef SourceTreeAndPathFromPath(input_path):\n    \"\"\"Given input_path, returns a tuple with sourceTree and path values.\n    Examples:\n      input_path     (source_tree, output_path)\n      '$(VAR)/path'  ('VAR', 'path')\n      '$(VAR)'       ('VAR', None)\n      'path'         (None, 'path')\n    \"\"\"\n    if source_group_match := _path_leading_variable.match(input_path):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "PBXTargetDependency._schema[\"target\"][1]",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "peekOfCode": "PBXTargetDependency._schema[\"target\"][1] = XCTarget\nclass PBXNativeTarget(XCTarget):\n    # buildPhases is overridden in the schema to be able to set defaults.\n    #\n    # NOTE: Contrary to most objects, it is advisable to set parent when\n    # constructing PBXNativeTarget.  A parent of an XCTarget must be a PBXProject\n    # object.  A parent reference is required for a PBXNativeTarget during\n    # construction to be able to set up the target defaults for productReference,\n    # because a PBXBuildFile object must be created for the target and it must\n    # be added to the PBXProject's mainGroup hierarchy.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xcodeproj_file",
        "documentation": {}
    },
    {
        "label": "XmlFix",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xml_fix",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xml_fix",
        "peekOfCode": "class XmlFix:\n    \"\"\"Object to manage temporary patching of xml.dom.minidom.\"\"\"\n    def __init__(self):\n        # Preserve current xml.dom.minidom functions.\n        self.write_data = xml.dom.minidom._write_data\n        self.writexml = xml.dom.minidom.Element.writexml\n        # Inject replacement versions of a function and a method.\n        xml.dom.minidom._write_data = _Replacement_write_data\n        xml.dom.minidom.Element.writexml = _Replacement_writexml\n    def Cleanup(self):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.gyp.xml_fix",
        "documentation": {}
    },
    {
        "label": "ELFInvalid",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "peekOfCode": "class ELFInvalid(ValueError):\n    pass\nclass EIClass(enum.IntEnum):\n    C32 = 1\n    C64 = 2\nclass EIData(enum.IntEnum):\n    Lsb = 1\n    Msb = 2\nclass EMachine(enum.IntEnum):\n    I386 = 3",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "documentation": {}
    },
    {
        "label": "EIClass",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "peekOfCode": "class EIClass(enum.IntEnum):\n    C32 = 1\n    C64 = 2\nclass EIData(enum.IntEnum):\n    Lsb = 1\n    Msb = 2\nclass EMachine(enum.IntEnum):\n    I386 = 3\n    S390 = 22\n    Arm = 40",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "documentation": {}
    },
    {
        "label": "EIData",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "peekOfCode": "class EIData(enum.IntEnum):\n    Lsb = 1\n    Msb = 2\nclass EMachine(enum.IntEnum):\n    I386 = 3\n    S390 = 22\n    Arm = 40\n    X8664 = 62\n    AArc64 = 183\nclass ELFFile:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "documentation": {}
    },
    {
        "label": "EMachine",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "peekOfCode": "class EMachine(enum.IntEnum):\n    I386 = 3\n    S390 = 22\n    Arm = 40\n    X8664 = 62\n    AArc64 = 183\nclass ELFFile:\n    \"\"\"\n    Representation of an ELF executable.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "documentation": {}
    },
    {
        "label": "ELFFile",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "peekOfCode": "class ELFFile:\n    \"\"\"\n    Representation of an ELF executable.\n    \"\"\"\n    def __init__(self, f: IO[bytes]) -> None:\n        self._f = f\n        try:\n            ident = self._read(\"16B\")\n        except struct.error:\n            raise ELFInvalid(\"unable to parse identification\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._elffile",
        "documentation": {}
    },
    {
        "label": "_GLibCVersion",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "peekOfCode": "class _GLibCVersion(NamedTuple):\n    major: int\n    minor: int\ndef _glibc_version_string_confstr() -> Optional[str]:\n    \"\"\"\n    Primary implementation of glibc_version_string using os.confstr.\n    \"\"\"\n    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely\n    # to be broken or missing. This strategy is used in the standard library\n    # platform module.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "documentation": {}
    },
    {
        "label": "platform_tags",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "peekOfCode": "def platform_tags(archs: Sequence[str]) -> Iterator[str]:\n    \"\"\"Generate manylinux tags compatible to the current platform.\n    :param archs: Sequence of compatible architectures.\n        The first one shall be the closest to the actual architecture and be the part of\n        platform tag after the ``linux_`` prefix, e.g. ``x86_64``.\n        The ``linux_`` prefix is assumed as a prerequisite for the current platform to\n        be manylinux-compatible.\n    :returns: An iterator of compatible manylinux tags.\n    \"\"\"\n    if not _have_compatible_abi(sys.executable, archs):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "documentation": {}
    },
    {
        "label": "EF_ARM_ABIMASK",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "peekOfCode": "EF_ARM_ABIMASK = 0xFF000000\nEF_ARM_ABI_VER5 = 0x05000000\nEF_ARM_ABI_FLOAT_HARD = 0x00000400\n# `os.PathLike` not a generic type until Python 3.9, so sticking with `str`\n# as the type for `path` until then.\n@contextlib.contextmanager\ndef _parse_elf(path: str) -> Generator[Optional[ELFFile], None, None]:\n    try:\n        with open(path, \"rb\") as f:\n            yield ELFFile(f)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "documentation": {}
    },
    {
        "label": "EF_ARM_ABI_VER5",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "peekOfCode": "EF_ARM_ABI_VER5 = 0x05000000\nEF_ARM_ABI_FLOAT_HARD = 0x00000400\n# `os.PathLike` not a generic type until Python 3.9, so sticking with `str`\n# as the type for `path` until then.\n@contextlib.contextmanager\ndef _parse_elf(path: str) -> Generator[Optional[ELFFile], None, None]:\n    try:\n        with open(path, \"rb\") as f:\n            yield ELFFile(f)\n    except (OSError, TypeError, ValueError):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "documentation": {}
    },
    {
        "label": "EF_ARM_ABI_FLOAT_HARD",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "peekOfCode": "EF_ARM_ABI_FLOAT_HARD = 0x00000400\n# `os.PathLike` not a generic type until Python 3.9, so sticking with `str`\n# as the type for `path` until then.\n@contextlib.contextmanager\ndef _parse_elf(path: str) -> Generator[Optional[ELFFile], None, None]:\n    try:\n        with open(path, \"rb\") as f:\n            yield ELFFile(f)\n    except (OSError, TypeError, ValueError):\n        yield None",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "documentation": {}
    },
    {
        "label": "_LEGACY_MANYLINUX_MAP",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "peekOfCode": "_LEGACY_MANYLINUX_MAP = {\n    # CentOS 7 w/ glibc 2.17 (PEP 599)\n    (2, 17): \"manylinux2014\",\n    # CentOS 6 w/ glibc 2.12 (PEP 571)\n    (2, 12): \"manylinux2010\",\n    # CentOS 5 w/ glibc 2.5 (PEP 513)\n    (2, 5): \"manylinux1\",\n}\ndef platform_tags(archs: Sequence[str]) -> Iterator[str]:\n    \"\"\"Generate manylinux tags compatible to the current platform.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._manylinux",
        "documentation": {}
    },
    {
        "label": "_MuslVersion",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._musllinux",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._musllinux",
        "peekOfCode": "class _MuslVersion(NamedTuple):\n    major: int\n    minor: int\ndef _parse_musl_version(output: str) -> Optional[_MuslVersion]:\n    lines = [n for n in (n.strip() for n in output.splitlines()) if n]\n    if len(lines) < 2 or lines[0][:4] != \"musl\":\n        return None\n    m = re.match(r\"Version (\\d+)\\.(\\d+)\", lines[1])\n    if not m:\n        return None",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._musllinux",
        "documentation": {}
    },
    {
        "label": "platform_tags",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._musllinux",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._musllinux",
        "peekOfCode": "def platform_tags(archs: Sequence[str]) -> Iterator[str]:\n    \"\"\"Generate musllinux tags compatible to the current platform.\n    :param archs: Sequence of compatible architectures.\n        The first one shall be the closest to the actual architecture and be the part of\n        platform tag after the ``linux_`` prefix, e.g. ``x86_64``.\n        The ``linux_`` prefix is assumed as a prerequisite for the current platform to\n        be musllinux-compatible.\n    :returns: An iterator of compatible musllinux tags.\n    \"\"\"\n    sys_musl = _get_musl_version(sys.executable)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._musllinux",
        "documentation": {}
    },
    {
        "label": "Node",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "class Node:\n    def __init__(self, value: str) -> None:\n        self.value = value\n    def __str__(self) -> str:\n        return self.value\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}('{self}')>\"\n    def serialize(self) -> str:\n        raise NotImplementedError\nclass Variable(Node):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "Variable",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "class Variable(Node):\n    def serialize(self) -> str:\n        return str(self)\nclass Value(Node):\n    def serialize(self) -> str:\n        return f'\"{self}\"'\nclass Op(Node):\n    def serialize(self) -> str:\n        return str(self)\nMarkerVar = Union[Variable, Value]",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "Value",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "class Value(Node):\n    def serialize(self) -> str:\n        return f'\"{self}\"'\nclass Op(Node):\n    def serialize(self) -> str:\n        return str(self)\nMarkerVar = Union[Variable, Value]\nMarkerItem = Tuple[MarkerVar, Op, MarkerVar]\n# MarkerAtom = Union[MarkerItem, List[\"MarkerAtom\"]]\n# MarkerList = List[Union[\"MarkerList\", MarkerAtom, str]]",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "Op",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "class Op(Node):\n    def serialize(self) -> str:\n        return str(self)\nMarkerVar = Union[Variable, Value]\nMarkerItem = Tuple[MarkerVar, Op, MarkerVar]\n# MarkerAtom = Union[MarkerItem, List[\"MarkerAtom\"]]\n# MarkerList = List[Union[\"MarkerList\", MarkerAtom, str]]\n# mypy does not support recursive type definition\n# https://github.com/python/mypy/issues/731\nMarkerAtom = Any",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "ParsedRequirement",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "class ParsedRequirement(NamedTuple):\n    name: str\n    url: str\n    extras: List[str]\n    specifier: str\n    marker: Optional[MarkerList]\n# --------------------------------------------------------------------------------------\n# Recursive descent parser for dependency specifier\n# --------------------------------------------------------------------------------------\ndef parse_requirement(source: str) -> ParsedRequirement:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "parse_requirement",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "def parse_requirement(source: str) -> ParsedRequirement:\n    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\ndef _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:\n    \"\"\"\n    requirement = WS? IDENTIFIER WS? extras WS? requirement_details\n    \"\"\"\n    tokenizer.consume(\"WS\")\n    name_token = tokenizer.expect(\n        \"IDENTIFIER\", expected=\"package name at the start of dependency specifier\"\n    )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "parse_marker",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "def parse_marker(source: str) -> MarkerList:\n    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\ndef _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:\n    retval = _parse_marker(tokenizer)\n    tokenizer.expect(\"END\", expected=\"end of marker expression\")\n    return retval\ndef _parse_marker(tokenizer: Tokenizer) -> MarkerList:\n    \"\"\"\n    marker = marker_atom (BOOLOP marker_atom)+\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "process_env_var",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "def process_env_var(env_var: str) -> Variable:\n    if (\n        env_var == \"platform_python_implementation\"\n        or env_var == \"python_implementation\"\n    ):\n        return Variable(\"platform_python_implementation\")\n    else:\n        return Variable(env_var)\ndef process_python_str(python_str: str) -> Value:\n    value = ast.literal_eval(python_str)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "process_python_str",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "def process_python_str(python_str: str) -> Value:\n    value = ast.literal_eval(python_str)\n    return Value(str(value))\ndef _parse_marker_op(tokenizer: Tokenizer) -> Op:\n    \"\"\"\n    marker_op = IN | NOT IN | OP\n    \"\"\"\n    if tokenizer.check(\"IN\"):\n        tokenizer.read()\n        return Op(\"in\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "MarkerVar",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "MarkerVar = Union[Variable, Value]\nMarkerItem = Tuple[MarkerVar, Op, MarkerVar]\n# MarkerAtom = Union[MarkerItem, List[\"MarkerAtom\"]]\n# MarkerList = List[Union[\"MarkerList\", MarkerAtom, str]]\n# mypy does not support recursive type definition\n# https://github.com/python/mypy/issues/731\nMarkerAtom = Any\nMarkerList = List[Any]\nclass ParsedRequirement(NamedTuple):\n    name: str",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "MarkerItem",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "MarkerItem = Tuple[MarkerVar, Op, MarkerVar]\n# MarkerAtom = Union[MarkerItem, List[\"MarkerAtom\"]]\n# MarkerList = List[Union[\"MarkerList\", MarkerAtom, str]]\n# mypy does not support recursive type definition\n# https://github.com/python/mypy/issues/731\nMarkerAtom = Any\nMarkerList = List[Any]\nclass ParsedRequirement(NamedTuple):\n    name: str\n    url: str",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "MarkerAtom",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "MarkerAtom = Any\nMarkerList = List[Any]\nclass ParsedRequirement(NamedTuple):\n    name: str\n    url: str\n    extras: List[str]\n    specifier: str\n    marker: Optional[MarkerList]\n# --------------------------------------------------------------------------------------\n# Recursive descent parser for dependency specifier",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "MarkerList",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "peekOfCode": "MarkerList = List[Any]\nclass ParsedRequirement(NamedTuple):\n    name: str\n    url: str\n    extras: List[str]\n    specifier: str\n    marker: Optional[MarkerList]\n# --------------------------------------------------------------------------------------\n# Recursive descent parser for dependency specifier\n# --------------------------------------------------------------------------------------",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._parser",
        "documentation": {}
    },
    {
        "label": "InfinityType",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "peekOfCode": "class InfinityType:\n    def __repr__(self) -> str:\n        return \"Infinity\"\n    def __hash__(self) -> int:\n        return hash(repr(self))\n    def __lt__(self, other: object) -> bool:\n        return False\n    def __le__(self, other: object) -> bool:\n        return False\n    def __eq__(self, other: object) -> bool:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "documentation": {}
    },
    {
        "label": "NegativeInfinityType",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "peekOfCode": "class NegativeInfinityType:\n    def __repr__(self) -> str:\n        return \"-Infinity\"\n    def __hash__(self) -> int:\n        return hash(repr(self))\n    def __lt__(self, other: object) -> bool:\n        return True\n    def __le__(self, other: object) -> bool:\n        return True\n    def __eq__(self, other: object) -> bool:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "documentation": {}
    },
    {
        "label": "Infinity",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "peekOfCode": "Infinity = InfinityType()\nclass NegativeInfinityType:\n    def __repr__(self) -> str:\n        return \"-Infinity\"\n    def __hash__(self) -> int:\n        return hash(repr(self))\n    def __lt__(self, other: object) -> bool:\n        return True\n    def __le__(self, other: object) -> bool:\n        return True",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "documentation": {}
    },
    {
        "label": "NegativeInfinity",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "peekOfCode": "NegativeInfinity = NegativeInfinityType()",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._structures",
        "documentation": {}
    },
    {
        "label": "Token",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._tokenizer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._tokenizer",
        "peekOfCode": "class Token:\n    name: str\n    text: str\n    position: int\nclass ParserSyntaxError(Exception):\n    \"\"\"The provided source text could not be parsed correctly.\"\"\"\n    def __init__(\n        self,\n        message: str,\n        *,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._tokenizer",
        "documentation": {}
    },
    {
        "label": "ParserSyntaxError",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._tokenizer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._tokenizer",
        "peekOfCode": "class ParserSyntaxError(Exception):\n    \"\"\"The provided source text could not be parsed correctly.\"\"\"\n    def __init__(\n        self,\n        message: str,\n        *,\n        source: str,\n        span: Tuple[int, int],\n    ) -> None:\n        self.span = span",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._tokenizer",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._tokenizer",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._tokenizer",
        "peekOfCode": "class Tokenizer:\n    \"\"\"Context-sensitive token parsing.\n    Provides methods to examine the input stream to check whether the next token\n    matches.\n    \"\"\"\n    def __init__(\n        self,\n        source: str,\n        *,\n        rules: \"Dict[str, Union[str, re.Pattern[str]]]\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging._tokenizer",
        "documentation": {}
    },
    {
        "label": "InvalidMarker",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "peekOfCode": "class InvalidMarker(ValueError):\n    \"\"\"\n    An invalid marker was found, users should refer to PEP 508.\n    \"\"\"\nclass UndefinedComparison(ValueError):\n    \"\"\"\n    An invalid operation was attempted on a value that doesn't support it.\n    \"\"\"\nclass UndefinedEnvironmentName(ValueError):\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "documentation": {}
    },
    {
        "label": "UndefinedComparison",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "peekOfCode": "class UndefinedComparison(ValueError):\n    \"\"\"\n    An invalid operation was attempted on a value that doesn't support it.\n    \"\"\"\nclass UndefinedEnvironmentName(ValueError):\n    \"\"\"\n    A name was attempted to be used that does not exist inside of the\n    environment.\n    \"\"\"\ndef _normalize_extra_values(results: Any) -> Any:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "documentation": {}
    },
    {
        "label": "UndefinedEnvironmentName",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "peekOfCode": "class UndefinedEnvironmentName(ValueError):\n    \"\"\"\n    A name was attempted to be used that does not exist inside of the\n    environment.\n    \"\"\"\ndef _normalize_extra_values(results: Any) -> Any:\n    \"\"\"\n    Normalize extra values.\n    \"\"\"\n    if isinstance(results[0], tuple):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "documentation": {}
    },
    {
        "label": "Marker",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "peekOfCode": "class Marker:\n    def __init__(self, marker: str) -> None:\n        # Note: We create a Marker object without calling this constructor in\n        #       packaging.requirements.Requirement. If any additional logic is\n        #       added here, make sure to mirror/adapt Requirement.\n        try:\n            self._markers = _normalize_extra_values(_parse_marker(marker))\n            # The attribute `_markers` can be described in terms of a recursive type:\n            # MarkerList = List[Union[Tuple[Node, ...], str, MarkerList]]\n            #",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "documentation": {}
    },
    {
        "label": "format_full_version",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "peekOfCode": "def format_full_version(info: \"sys._version_info\") -> str:\n    version = \"{0.major}.{0.minor}.{0.micro}\".format(info)\n    if (kind := info.releaselevel) != \"final\":\n        version += kind[0] + str(info.serial)\n    return version\ndef default_environment() -> Dict[str, str]:\n    iver = format_full_version(sys.implementation.version)\n    implementation_name = sys.implementation.name\n    return {\n        \"implementation_name\": implementation_name,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "documentation": {}
    },
    {
        "label": "default_environment",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "peekOfCode": "def default_environment() -> Dict[str, str]:\n    iver = format_full_version(sys.implementation.version)\n    implementation_name = sys.implementation.name\n    return {\n        \"implementation_name\": implementation_name,\n        \"implementation_version\": iver,\n        \"os_name\": os.name,\n        \"platform_machine\": platform.machine(),\n        \"platform_release\": platform.release(),\n        \"platform_system\": platform.system(),",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "peekOfCode": "__all__ = [\n    \"InvalidMarker\",\n    \"UndefinedComparison\",\n    \"UndefinedEnvironmentName\",\n    \"Marker\",\n    \"default_environment\",\n]\nOperator = Callable[[str, str], bool]\nclass InvalidMarker(ValueError):\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "documentation": {}
    },
    {
        "label": "Operator",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "peekOfCode": "Operator = Callable[[str, str], bool]\nclass InvalidMarker(ValueError):\n    \"\"\"\n    An invalid marker was found, users should refer to PEP 508.\n    \"\"\"\nclass UndefinedComparison(ValueError):\n    \"\"\"\n    An invalid operation was attempted on a value that doesn't support it.\n    \"\"\"\nclass UndefinedEnvironmentName(ValueError):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.markers",
        "documentation": {}
    },
    {
        "label": "InvalidMetadata",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "class InvalidMetadata(ValueError):\n    \"\"\"A metadata field contains invalid data.\"\"\"\n    field: str\n    \"\"\"The name of the field that contains invalid data.\"\"\"\n    def __init__(self, field: str, message: str) -> None:\n        self.field = field\n        super().__init__(message)\n# The RawMetadata class attempts to make as few assumptions about the underlying\n# serialization formats as possible. The idea is that as long as a serialization\n# formats offer some very basic primitives in *some* way then we can support",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "RawMetadata",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "class RawMetadata(TypedDict, total=False):\n    \"\"\"A dictionary of raw core metadata.\n    Each field in core metadata maps to a key of this dictionary (when data is\n    provided). The key is lower-case and underscores are used instead of dashes\n    compared to the equivalent core metadata field. Any core metadata field that\n    can be specified multiple times or can hold multiple values in a single\n    field have a key with a plural name. See :class:`Metadata` whose attributes\n    match the keys of this dictionary.\n    Core metadata fields that can be specified multiple times are stored as a\n    list or dict depending on which is appropriate for the field. Any fields",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "_Validator",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "class _Validator(Generic[T]):\n    \"\"\"Validate a metadata field.\n    All _process_*() methods correspond to a core metadata field. The method is\n    called with the field's raw value. If the raw value is valid it is returned\n    in its \"enriched\" form (e.g. ``version.Version`` for the ``Version`` field).\n    If the raw value is invalid, :exc:`InvalidMetadata` is raised (with a cause\n    as appropriate).\n    \"\"\"\n    name: str\n    raw_name: str",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "Metadata",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "class Metadata:\n    \"\"\"Representation of distribution metadata.\n    Compared to :class:`RawMetadata`, this class provides objects representing\n    metadata fields instead of only using built-in types. Any invalid metadata\n    will cause :exc:`InvalidMetadata` to be raised (with a\n    :py:attr:`~BaseException.__cause__` attribute as appropriate).\n    \"\"\"\n    _raw: RawMetadata\n    @classmethod\n    def from_raw(cls, data: RawMetadata, *, validate: bool = True) -> \"Metadata\":",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "parse_email",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "def parse_email(data: Union[bytes, str]) -> Tuple[RawMetadata, Dict[str, List[str]]]:\n    \"\"\"Parse a distribution's metadata stored as email headers (e.g. from ``METADATA``).\n    This function returns a two-item tuple of dicts. The first dict is of\n    recognized fields from the core metadata specification. Fields that can be\n    parsed and translated into Python's built-in types are converted\n    appropriately. All other fields are left as-is. Fields that are allowed to\n    appear multiple times are stored as lists.\n    The second dict contains all other fields from the metadata. This includes\n    any unrecognized fields. It also includes any fields which are expected to\n    be parsed into a built-in type but were not formatted appropriately. Finally,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "T = typing.TypeVar(\"T\")\nif sys.version_info[:2] >= (3, 8):  # pragma: no cover\n    from typing import Literal, TypedDict\nelse:  # pragma: no cover\n    if typing.TYPE_CHECKING:\n        from typing_extensions import Literal, TypedDict\n    else:\n        try:\n            from typing_extensions import Literal, TypedDict\n        except ImportError:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "_STRING_FIELDS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "_STRING_FIELDS = {\n    \"author\",\n    \"author_email\",\n    \"description\",\n    \"description_content_type\",\n    \"download_url\",\n    \"home_page\",\n    \"license\",\n    \"maintainer\",\n    \"maintainer_email\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "_LIST_FIELDS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "_LIST_FIELDS = {\n    \"classifiers\",\n    \"dynamic\",\n    \"obsoletes\",\n    \"obsoletes_dist\",\n    \"platforms\",\n    \"provides\",\n    \"provides_dist\",\n    \"provides_extra\",\n    \"requires\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "_DICT_FIELDS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "_DICT_FIELDS = {\n    \"project_urls\",\n}\ndef _parse_keywords(data: str) -> List[str]:\n    \"\"\"Split a string of comma-separate keyboards into a list of keywords.\"\"\"\n    return [k.strip() for k in data.split(\",\")]\ndef _parse_project_urls(data: List[str]) -> Dict[str, str]:\n    \"\"\"Parse a list of label/URL string pairings separated by a comma.\"\"\"\n    urls = {}\n    for pair in data:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "_EMAIL_TO_RAW_MAPPING",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "_EMAIL_TO_RAW_MAPPING = {\n    \"author\": \"author\",\n    \"author-email\": \"author_email\",\n    \"classifier\": \"classifiers\",\n    \"description\": \"description\",\n    \"description-content-type\": \"description_content_type\",\n    \"download-url\": \"download_url\",\n    \"dynamic\": \"dynamic\",\n    \"home-page\": \"home_page\",\n    \"keywords\": \"keywords\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "_RAW_TO_EMAIL_MAPPING",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "_RAW_TO_EMAIL_MAPPING = {raw: email for email, raw in _EMAIL_TO_RAW_MAPPING.items()}\ndef parse_email(data: Union[bytes, str]) -> Tuple[RawMetadata, Dict[str, List[str]]]:\n    \"\"\"Parse a distribution's metadata stored as email headers (e.g. from ``METADATA``).\n    This function returns a two-item tuple of dicts. The first dict is of\n    recognized fields from the core metadata specification. Fields that can be\n    parsed and translated into Python's built-in types are converted\n    appropriately. All other fields are left as-is. Fields that are allowed to\n    appear multiple times are stored as lists.\n    The second dict contains all other fields from the metadata. This includes\n    any unrecognized fields. It also includes any fields which are expected to",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "_NOT_FOUND",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "_NOT_FOUND = object()\n# Keep the two values in sync.\n_VALID_METADATA_VERSIONS = [\"1.0\", \"1.1\", \"1.2\", \"2.1\", \"2.2\", \"2.3\"]\n_MetadataVersion = Literal[\"1.0\", \"1.1\", \"1.2\", \"2.1\", \"2.2\", \"2.3\"]\n_REQUIRED_ATTRS = frozenset([\"metadata_version\", \"name\", \"version\"])\nclass _Validator(Generic[T]):\n    \"\"\"Validate a metadata field.\n    All _process_*() methods correspond to a core metadata field. The method is\n    called with the field's raw value. If the raw value is valid it is returned\n    in its \"enriched\" form (e.g. ``version.Version`` for the ``Version`` field).",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "_VALID_METADATA_VERSIONS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "_VALID_METADATA_VERSIONS = [\"1.0\", \"1.1\", \"1.2\", \"2.1\", \"2.2\", \"2.3\"]\n_MetadataVersion = Literal[\"1.0\", \"1.1\", \"1.2\", \"2.1\", \"2.2\", \"2.3\"]\n_REQUIRED_ATTRS = frozenset([\"metadata_version\", \"name\", \"version\"])\nclass _Validator(Generic[T]):\n    \"\"\"Validate a metadata field.\n    All _process_*() methods correspond to a core metadata field. The method is\n    called with the field's raw value. If the raw value is valid it is returned\n    in its \"enriched\" form (e.g. ``version.Version`` for the ``Version`` field).\n    If the raw value is invalid, :exc:`InvalidMetadata` is raised (with a cause\n    as appropriate).",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "_MetadataVersion",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "_MetadataVersion = Literal[\"1.0\", \"1.1\", \"1.2\", \"2.1\", \"2.2\", \"2.3\"]\n_REQUIRED_ATTRS = frozenset([\"metadata_version\", \"name\", \"version\"])\nclass _Validator(Generic[T]):\n    \"\"\"Validate a metadata field.\n    All _process_*() methods correspond to a core metadata field. The method is\n    called with the field's raw value. If the raw value is valid it is returned\n    in its \"enriched\" form (e.g. ``version.Version`` for the ``Version`` field).\n    If the raw value is invalid, :exc:`InvalidMetadata` is raised (with a cause\n    as appropriate).\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "_REQUIRED_ATTRS",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "peekOfCode": "_REQUIRED_ATTRS = frozenset([\"metadata_version\", \"name\", \"version\"])\nclass _Validator(Generic[T]):\n    \"\"\"Validate a metadata field.\n    All _process_*() methods correspond to a core metadata field. The method is\n    called with the field's raw value. If the raw value is valid it is returned\n    in its \"enriched\" form (e.g. ``version.Version`` for the ``Version`` field).\n    If the raw value is invalid, :exc:`InvalidMetadata` is raised (with a cause\n    as appropriate).\n    \"\"\"\n    name: str",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.metadata",
        "documentation": {}
    },
    {
        "label": "InvalidRequirement",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.requirements",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.requirements",
        "peekOfCode": "class InvalidRequirement(ValueError):\n    \"\"\"\n    An invalid requirement was found, users should refer to PEP 508.\n    \"\"\"\nclass Requirement:\n    \"\"\"Parse a requirement.\n    Parse a given requirement string into its parts, such as name, specifier,\n    URL, and extras. Raises InvalidRequirement on a badly-formed requirement\n    string.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.requirements",
        "documentation": {}
    },
    {
        "label": "Requirement",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.requirements",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.requirements",
        "peekOfCode": "class Requirement:\n    \"\"\"Parse a requirement.\n    Parse a given requirement string into its parts, such as name, specifier,\n    URL, and extras. Raises InvalidRequirement on a badly-formed requirement\n    string.\n    \"\"\"\n    # TODO: Can we test whether something is contained within a requirement?\n    #       If so how do we do that? Do we need to test against the _name_ of\n    #       the thing as well as the version? What about the markers?\n    # TODO: Can we normalize the name and extra name?",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.requirements",
        "documentation": {}
    },
    {
        "label": "InvalidSpecifier",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "peekOfCode": "class InvalidSpecifier(ValueError):\n    \"\"\"\n    Raised when attempting to create a :class:`Specifier` with a specifier\n    string that is invalid.\n    >>> Specifier(\"lolwat\")\n    Traceback (most recent call last):\n        ...\n    packaging.specifiers.InvalidSpecifier: Invalid specifier: 'lolwat'\n    \"\"\"\nclass BaseSpecifier(metaclass=abc.ABCMeta):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "documentation": {}
    },
    {
        "label": "BaseSpecifier",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "peekOfCode": "class BaseSpecifier(metaclass=abc.ABCMeta):\n    @abc.abstractmethod\n    def __str__(self) -> str:\n        \"\"\"\n        Returns the str representation of this Specifier-like object. This\n        should be representative of the Specifier itself.\n        \"\"\"\n    @abc.abstractmethod\n    def __hash__(self) -> int:\n        \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "documentation": {}
    },
    {
        "label": "Specifier",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "peekOfCode": "class Specifier(BaseSpecifier):\n    \"\"\"This class abstracts handling of version specifiers.\n    .. tip::\n        It is generally not required to instantiate this manually. You should instead\n        prefer to work with :class:`SpecifierSet` instead, which can parse\n        comma-separated version specifiers (which is what package metadata contains).\n    \"\"\"\n    _operator_regex_str = r\"\"\"\n        (?P<operator>(~=|==|!=|<=|>=|<|>|===))\n        \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "documentation": {}
    },
    {
        "label": "SpecifierSet",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "peekOfCode": "class SpecifierSet(BaseSpecifier):\n    \"\"\"This class abstracts handling of a set of version specifiers.\n    It can be passed a single specifier (``>=3.0``), a comma-separated list of\n    specifiers (``>=3.0,!=3.1``), or no specifier at all.\n    \"\"\"\n    def __init__(\n        self, specifiers: str = \"\", prereleases: Optional[bool] = None\n    ) -> None:\n        \"\"\"Initialize a SpecifierSet instance.\n        :param specifiers:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "documentation": {}
    },
    {
        "label": "UnparsedVersion",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "peekOfCode": "UnparsedVersion = Union[Version, str]\nUnparsedVersionVar = TypeVar(\"UnparsedVersionVar\", bound=UnparsedVersion)\nCallableOperator = Callable[[Version, str], bool]\ndef _coerce_version(version: UnparsedVersion) -> Version:\n    if not isinstance(version, Version):\n        version = Version(version)\n    return version\nclass InvalidSpecifier(ValueError):\n    \"\"\"\n    Raised when attempting to create a :class:`Specifier` with a specifier",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "documentation": {}
    },
    {
        "label": "UnparsedVersionVar",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "peekOfCode": "UnparsedVersionVar = TypeVar(\"UnparsedVersionVar\", bound=UnparsedVersion)\nCallableOperator = Callable[[Version, str], bool]\ndef _coerce_version(version: UnparsedVersion) -> Version:\n    if not isinstance(version, Version):\n        version = Version(version)\n    return version\nclass InvalidSpecifier(ValueError):\n    \"\"\"\n    Raised when attempting to create a :class:`Specifier` with a specifier\n    string that is invalid.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "documentation": {}
    },
    {
        "label": "CallableOperator",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "peekOfCode": "CallableOperator = Callable[[Version, str], bool]\ndef _coerce_version(version: UnparsedVersion) -> Version:\n    if not isinstance(version, Version):\n        version = Version(version)\n    return version\nclass InvalidSpecifier(ValueError):\n    \"\"\"\n    Raised when attempting to create a :class:`Specifier` with a specifier\n    string that is invalid.\n    >>> Specifier(\"lolwat\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "documentation": {}
    },
    {
        "label": "_prefix_regex",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "peekOfCode": "_prefix_regex = re.compile(r\"^([0-9]+)((?:a|b|c|rc)[0-9]+)$\")\ndef _version_split(version: str) -> List[str]:\n    \"\"\"Split version into components.\n    The split components are intended for version comparison. The logic does\n    not attempt to retain the original version string, so joining the\n    components back with :func:`_version_join` may not produce the original\n    version string.\n    \"\"\"\n    result: List[str] = []\n    epoch, _, rest = version.rpartition(\"!\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.specifiers",
        "documentation": {}
    },
    {
        "label": "Tag",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "class Tag:\n    \"\"\"\n    A representation of the tag triple for a wheel.\n    Instances are considered immutable and thus are hashable. Equality checking\n    is also supported.\n    \"\"\"\n    __slots__ = [\"_interpreter\", \"_abi\", \"_platform\", \"_hash\"]\n    def __init__(self, interpreter: str, abi: str, platform: str) -> None:\n        self._interpreter = interpreter.lower()\n        self._abi = abi.lower()",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "parse_tag",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "def parse_tag(tag: str) -> FrozenSet[Tag]:\n    \"\"\"\n    Parses the provided tag (e.g. `py3-none-any`) into a frozenset of Tag instances.\n    Returning a set is required due to the possibility that the tag is a\n    compressed tag set.\n    \"\"\"\n    tags = set()\n    interpreters, abis, platforms = tag.split(\"-\")\n    for interpreter in interpreters.split(\".\"):\n        for abi in abis.split(\".\"):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "cpython_tags",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "def cpython_tags(\n    python_version: Optional[PythonVersion] = None,\n    abis: Optional[Iterable[str]] = None,\n    platforms: Optional[Iterable[str]] = None,\n    *,\n    warn: bool = False,\n) -> Iterator[Tag]:\n    \"\"\"\n    Yields the tags for a CPython interpreter.\n    The tags consist of:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "generic_tags",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "def generic_tags(\n    interpreter: Optional[str] = None,\n    abis: Optional[Iterable[str]] = None,\n    platforms: Optional[Iterable[str]] = None,\n    *,\n    warn: bool = False,\n) -> Iterator[Tag]:\n    \"\"\"\n    Yields the tags for a generic interpreter.\n    The tags consist of:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "compatible_tags",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "def compatible_tags(\n    python_version: Optional[PythonVersion] = None,\n    interpreter: Optional[str] = None,\n    platforms: Optional[Iterable[str]] = None,\n) -> Iterator[Tag]:\n    \"\"\"\n    Yields the sequence of tags that are compatible with a specific version of Python.\n    The tags consist of:\n    - py*-none-<platform>\n    - <interpreter>-none-any  # ... if `interpreter` is provided.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "mac_platforms",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "def mac_platforms(\n    version: Optional[MacVersion] = None, arch: Optional[str] = None\n) -> Iterator[str]:\n    \"\"\"\n    Yields the platform tags for a macOS system.\n    The `version` parameter is a two-item tuple specifying the macOS version to\n    generate platform tags for. The `arch` parameter is the CPU architecture to\n    generate platform tags for. Both parameters default to the appropriate value\n    for the current system.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "platform_tags",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "def platform_tags() -> Iterator[str]:\n    \"\"\"\n    Provides the platform tags for this installation.\n    \"\"\"\n    if platform.system() == \"Darwin\":\n        return mac_platforms()\n    elif platform.system() == \"Linux\":\n        return _linux_platforms()\n    else:\n        return _generic_platforms()",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "interpreter_name",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "def interpreter_name() -> str:\n    \"\"\"\n    Returns the name of the running interpreter.\n    Some implementations have a reserved, two-letter abbreviation which will\n    be returned when appropriate.\n    \"\"\"\n    name = sys.implementation.name\n    return INTERPRETER_SHORT_NAMES.get(name) or name\ndef interpreter_version(*, warn: bool = False) -> str:\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "interpreter_version",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "def interpreter_version(*, warn: bool = False) -> str:\n    \"\"\"\n    Returns the version of the running interpreter.\n    \"\"\"\n    version = _get_config_var(\"py_version_nodot\", warn=warn)\n    if version:\n        version = str(version)\n    else:\n        version = _version_nodot(sys.version_info[:2])\n    return version",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "sys_tags",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "def sys_tags(*, warn: bool = False) -> Iterator[Tag]:\n    \"\"\"\n    Returns the sequence of tag triples for the running interpreter.\n    The order of the sequence corresponds to priority order for the\n    interpreter, from most to least important.\n    \"\"\"\n    interp_name = interpreter_name()\n    if interp_name == \"cp\":\n        yield from cpython_tags(warn=warn)\n    else:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "logger = logging.getLogger(__name__)\nPythonVersion = Sequence[int]\nMacVersion = Tuple[int, int]\nINTERPRETER_SHORT_NAMES: Dict[str, str] = {\n    \"python\": \"py\",  # Generic.\n    \"cpython\": \"cp\",\n    \"pypy\": \"pp\",\n    \"ironpython\": \"ip\",\n    \"jython\": \"jy\",\n}",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "PythonVersion",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "PythonVersion = Sequence[int]\nMacVersion = Tuple[int, int]\nINTERPRETER_SHORT_NAMES: Dict[str, str] = {\n    \"python\": \"py\",  # Generic.\n    \"cpython\": \"cp\",\n    \"pypy\": \"pp\",\n    \"ironpython\": \"ip\",\n    \"jython\": \"jy\",\n}\n_32_BIT_INTERPRETER = struct.calcsize(\"P\") == 4",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "MacVersion",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "MacVersion = Tuple[int, int]\nINTERPRETER_SHORT_NAMES: Dict[str, str] = {\n    \"python\": \"py\",  # Generic.\n    \"cpython\": \"cp\",\n    \"pypy\": \"pp\",\n    \"ironpython\": \"ip\",\n    \"jython\": \"jy\",\n}\n_32_BIT_INTERPRETER = struct.calcsize(\"P\") == 4\nclass Tag:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "_32_BIT_INTERPRETER",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "peekOfCode": "_32_BIT_INTERPRETER = struct.calcsize(\"P\") == 4\nclass Tag:\n    \"\"\"\n    A representation of the tag triple for a wheel.\n    Instances are considered immutable and thus are hashable. Equality checking\n    is also supported.\n    \"\"\"\n    __slots__ = [\"_interpreter\", \"_abi\", \"_platform\", \"_hash\"]\n    def __init__(self, interpreter: str, abi: str, platform: str) -> None:\n        self._interpreter = interpreter.lower()",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.tags",
        "documentation": {}
    },
    {
        "label": "InvalidName",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "class InvalidName(ValueError):\n    \"\"\"\n    An invalid distribution name; users should refer to the packaging user guide.\n    \"\"\"\nclass InvalidWheelFilename(ValueError):\n    \"\"\"\n    An invalid wheel filename was found, users should refer to PEP 427.\n    \"\"\"\nclass InvalidSdistFilename(ValueError):\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "InvalidWheelFilename",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "class InvalidWheelFilename(ValueError):\n    \"\"\"\n    An invalid wheel filename was found, users should refer to PEP 427.\n    \"\"\"\nclass InvalidSdistFilename(ValueError):\n    \"\"\"\n    An invalid sdist filename was found, users should refer to the packaging user guide.\n    \"\"\"\n# Core metadata spec for `Name`\n_validate_regex = re.compile(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "InvalidSdistFilename",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "class InvalidSdistFilename(ValueError):\n    \"\"\"\n    An invalid sdist filename was found, users should refer to the packaging user guide.\n    \"\"\"\n# Core metadata spec for `Name`\n_validate_regex = re.compile(\n    r\"^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$\", re.IGNORECASE\n)\n_canonicalize_regex = re.compile(r\"[-_.]+\")\n_normalized_regex = re.compile(r\"^([a-z0-9]|[a-z0-9]([a-z0-9-](?!--))*[a-z0-9])$\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "canonicalize_name",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "def canonicalize_name(name: str, *, validate: bool = False) -> NormalizedName:\n    if validate and not _validate_regex.match(name):\n        raise InvalidName(f\"name is invalid: {name!r}\")\n    # This is taken from PEP 503.\n    value = _canonicalize_regex.sub(\"-\", name).lower()\n    return cast(NormalizedName, value)\ndef is_normalized_name(name: str) -> bool:\n    return _normalized_regex.match(name) is not None\ndef canonicalize_version(\n    version: Union[Version, str], *, strip_trailing_zero: bool = True",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "is_normalized_name",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "def is_normalized_name(name: str) -> bool:\n    return _normalized_regex.match(name) is not None\ndef canonicalize_version(\n    version: Union[Version, str], *, strip_trailing_zero: bool = True\n) -> str:\n    \"\"\"\n    This is very similar to Version.__str__, but has one subtle difference\n    with the way it handles the release segment.\n    \"\"\"\n    if isinstance(version, str):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "canonicalize_version",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "def canonicalize_version(\n    version: Union[Version, str], *, strip_trailing_zero: bool = True\n) -> str:\n    \"\"\"\n    This is very similar to Version.__str__, but has one subtle difference\n    with the way it handles the release segment.\n    \"\"\"\n    if isinstance(version, str):\n        try:\n            parsed = Version(version)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "parse_wheel_filename",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "def parse_wheel_filename(\n    filename: str,\n) -> Tuple[NormalizedName, Version, BuildTag, FrozenSet[Tag]]:\n    if not filename.endswith(\".whl\"):\n        raise InvalidWheelFilename(\n            f\"Invalid wheel filename (extension must be '.whl'): {filename}\"\n        )\n    filename = filename[:-4]\n    dashes = filename.count(\"-\")\n    if dashes not in (4, 5):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "parse_sdist_filename",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "def parse_sdist_filename(filename: str) -> Tuple[NormalizedName, Version]:\n    if filename.endswith(\".tar.gz\"):\n        file_stem = filename[: -len(\".tar.gz\")]\n    elif filename.endswith(\".zip\"):\n        file_stem = filename[: -len(\".zip\")]\n    else:\n        raise InvalidSdistFilename(\n            f\"Invalid sdist filename (extension must be '.tar.gz' or '.zip'):\"\n            f\" {filename}\"\n        )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "BuildTag",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "BuildTag = Union[Tuple[()], Tuple[int, str]]\nNormalizedName = NewType(\"NormalizedName\", str)\nclass InvalidName(ValueError):\n    \"\"\"\n    An invalid distribution name; users should refer to the packaging user guide.\n    \"\"\"\nclass InvalidWheelFilename(ValueError):\n    \"\"\"\n    An invalid wheel filename was found, users should refer to PEP 427.\n    \"\"\"",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "NormalizedName",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "NormalizedName = NewType(\"NormalizedName\", str)\nclass InvalidName(ValueError):\n    \"\"\"\n    An invalid distribution name; users should refer to the packaging user guide.\n    \"\"\"\nclass InvalidWheelFilename(ValueError):\n    \"\"\"\n    An invalid wheel filename was found, users should refer to PEP 427.\n    \"\"\"\nclass InvalidSdistFilename(ValueError):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "_validate_regex",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "_validate_regex = re.compile(\n    r\"^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$\", re.IGNORECASE\n)\n_canonicalize_regex = re.compile(r\"[-_.]+\")\n_normalized_regex = re.compile(r\"^([a-z0-9]|[a-z0-9]([a-z0-9-](?!--))*[a-z0-9])$\")\n# PEP 427: The build number must start with a digit.\n_build_tag_regex = re.compile(r\"(\\d+)(.*)\")\ndef canonicalize_name(name: str, *, validate: bool = False) -> NormalizedName:\n    if validate and not _validate_regex.match(name):\n        raise InvalidName(f\"name is invalid: {name!r}\")",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "_canonicalize_regex",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "_canonicalize_regex = re.compile(r\"[-_.]+\")\n_normalized_regex = re.compile(r\"^([a-z0-9]|[a-z0-9]([a-z0-9-](?!--))*[a-z0-9])$\")\n# PEP 427: The build number must start with a digit.\n_build_tag_regex = re.compile(r\"(\\d+)(.*)\")\ndef canonicalize_name(name: str, *, validate: bool = False) -> NormalizedName:\n    if validate and not _validate_regex.match(name):\n        raise InvalidName(f\"name is invalid: {name!r}\")\n    # This is taken from PEP 503.\n    value = _canonicalize_regex.sub(\"-\", name).lower()\n    return cast(NormalizedName, value)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "_normalized_regex",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "_normalized_regex = re.compile(r\"^([a-z0-9]|[a-z0-9]([a-z0-9-](?!--))*[a-z0-9])$\")\n# PEP 427: The build number must start with a digit.\n_build_tag_regex = re.compile(r\"(\\d+)(.*)\")\ndef canonicalize_name(name: str, *, validate: bool = False) -> NormalizedName:\n    if validate and not _validate_regex.match(name):\n        raise InvalidName(f\"name is invalid: {name!r}\")\n    # This is taken from PEP 503.\n    value = _canonicalize_regex.sub(\"-\", name).lower()\n    return cast(NormalizedName, value)\ndef is_normalized_name(name: str) -> bool:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "_build_tag_regex",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "peekOfCode": "_build_tag_regex = re.compile(r\"(\\d+)(.*)\")\ndef canonicalize_name(name: str, *, validate: bool = False) -> NormalizedName:\n    if validate and not _validate_regex.match(name):\n        raise InvalidName(f\"name is invalid: {name!r}\")\n    # This is taken from PEP 503.\n    value = _canonicalize_regex.sub(\"-\", name).lower()\n    return cast(NormalizedName, value)\ndef is_normalized_name(name: str) -> bool:\n    return _normalized_regex.match(name) is not None\ndef canonicalize_version(",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.utils",
        "documentation": {}
    },
    {
        "label": "_Version",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "class _Version(NamedTuple):\n    epoch: int\n    release: Tuple[int, ...]\n    dev: Optional[Tuple[str, int]]\n    pre: Optional[Tuple[str, int]]\n    post: Optional[Tuple[str, int]]\n    local: Optional[LocalType]\ndef parse(version: str) -> \"Version\":\n    \"\"\"Parse the given version string.\n    >>> parse('1.0.dev1')",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "InvalidVersion",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "class InvalidVersion(ValueError):\n    \"\"\"Raised when a version string is not a valid version.\n    >>> Version(\"invalid\")\n    Traceback (most recent call last):\n        ...\n    packaging.version.InvalidVersion: Invalid version: 'invalid'\n    \"\"\"\nclass _BaseVersion:\n    _key: Tuple[Any, ...]\n    def __hash__(self) -> int:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "_BaseVersion",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "class _BaseVersion:\n    _key: Tuple[Any, ...]\n    def __hash__(self) -> int:\n        return hash(self._key)\n    # Please keep the duplicated `isinstance` check\n    # in the six comparisons hereunder\n    # unless you find a way to avoid adding overhead function calls.\n    def __lt__(self, other: \"_BaseVersion\") -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "class Version(_BaseVersion):\n    \"\"\"This class abstracts handling of a project's versions.\n    A :class:`Version` instance is comparison aware and can be compared and\n    sorted using the standard Python interfaces.\n    >>> v1 = Version(\"1.0a5\")\n    >>> v2 = Version(\"1.0\")\n    >>> v1\n    <Version('1.0a5')>\n    >>> v2\n    <Version('1.0')>",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "def parse(version: str) -> \"Version\":\n    \"\"\"Parse the given version string.\n    >>> parse('1.0.dev1')\n    <Version('1.0.dev1')>\n    :param version: The version string to parse.\n    :raises InvalidVersion: When the version string is not a valid version.\n    \"\"\"\n    return Version(version)\nclass InvalidVersion(ValueError):\n    \"\"\"Raised when a version string is not a valid version.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "__all__ = [\"VERSION_PATTERN\", \"parse\", \"Version\", \"InvalidVersion\"]\nLocalType = Tuple[Union[int, str], ...]\nCmpPrePostDevType = Union[InfinityType, NegativeInfinityType, Tuple[str, int]]\nCmpLocalType = Union[\n    NegativeInfinityType,\n    Tuple[Union[Tuple[int, str], Tuple[NegativeInfinityType, Union[int, str]]], ...],\n]\nCmpKey = Tuple[\n    int,\n    Tuple[int, ...],",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "LocalType",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "LocalType = Tuple[Union[int, str], ...]\nCmpPrePostDevType = Union[InfinityType, NegativeInfinityType, Tuple[str, int]]\nCmpLocalType = Union[\n    NegativeInfinityType,\n    Tuple[Union[Tuple[int, str], Tuple[NegativeInfinityType, Union[int, str]]], ...],\n]\nCmpKey = Tuple[\n    int,\n    Tuple[int, ...],\n    CmpPrePostDevType,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "CmpPrePostDevType",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "CmpPrePostDevType = Union[InfinityType, NegativeInfinityType, Tuple[str, int]]\nCmpLocalType = Union[\n    NegativeInfinityType,\n    Tuple[Union[Tuple[int, str], Tuple[NegativeInfinityType, Union[int, str]]], ...],\n]\nCmpKey = Tuple[\n    int,\n    Tuple[int, ...],\n    CmpPrePostDevType,\n    CmpPrePostDevType,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "CmpLocalType",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "CmpLocalType = Union[\n    NegativeInfinityType,\n    Tuple[Union[Tuple[int, str], Tuple[NegativeInfinityType, Union[int, str]]], ...],\n]\nCmpKey = Tuple[\n    int,\n    Tuple[int, ...],\n    CmpPrePostDevType,\n    CmpPrePostDevType,\n    CmpPrePostDevType,",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "CmpKey",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "CmpKey = Tuple[\n    int,\n    Tuple[int, ...],\n    CmpPrePostDevType,\n    CmpPrePostDevType,\n    CmpPrePostDevType,\n    CmpLocalType,\n]\nVersionComparisonMethod = Callable[[CmpKey, CmpKey], bool]\nclass _Version(NamedTuple):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "VersionComparisonMethod",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "VersionComparisonMethod = Callable[[CmpKey, CmpKey], bool]\nclass _Version(NamedTuple):\n    epoch: int\n    release: Tuple[int, ...]\n    dev: Optional[Tuple[str, int]]\n    pre: Optional[Tuple[str, int]]\n    post: Optional[Tuple[str, int]]\n    local: Optional[LocalType]\ndef parse(version: str) -> \"Version\":\n    \"\"\"Parse the given version string.",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "_VERSION_PATTERN",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "_VERSION_PATTERN = r\"\"\"\n    v?\n    (?:\n        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n        (?P<pre>                                          # pre-release\n            [-_\\.]?\n            (?P<pre_l>alpha|a|beta|b|preview|pre|c|rc)\n            [-_\\.]?\n            (?P<pre_n>[0-9]+)?",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "VERSION_PATTERN",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "VERSION_PATTERN = _VERSION_PATTERN\n\"\"\"\nA string containing the regular expression used to match a valid version.\nThe pattern is not anchored at either end, and is intended for embedding in larger\nexpressions (for example, matching a version number as part of a file name). The\nregular expression should be compiled with the ``re.VERBOSE`` and ``re.IGNORECASE``\nflags set.\n:meta hide-value:\n\"\"\"\nclass Version(_BaseVersion):",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "_local_version_separators",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "peekOfCode": "_local_version_separators = re.compile(r\"[\\._-]\")\ndef _parse_local_version(local: Optional[str]) -> Optional[LocalType]:\n    \"\"\"\n    Takes a string like abc.1.twelve and turns it into (\"abc\", 1, \"twelve\").\n    \"\"\"\n    if local is not None:\n        return tuple(\n            part.lower() if not part.isdigit() else int(part)\n            for part in _local_version_separators.split(local)\n        )",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.pylib.packaging.version",
        "documentation": {}
    },
    {
        "label": "IsCygwin",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.gyp_main",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.gyp_main",
        "peekOfCode": "def IsCygwin():\n    # Function copied from pylib/gyp/common.py\n    try:\n        out = subprocess.Popen(\n            \"uname\", stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n        )\n        stdout, _ = out.communicate()\n        return \"CYGWIN\" in stdout.decode(\"utf-8\")\n    except Exception:\n        return False",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.gyp_main",
        "documentation": {}
    },
    {
        "label": "UnixifyPath",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.gyp_main",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.gyp_main",
        "peekOfCode": "def UnixifyPath(path):\n    try:\n        if not IsCygwin():\n            return path\n        out = subprocess.Popen(\n            [\"cygpath\", \"-u\", path], stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n        )\n        stdout, _ = out.communicate()\n        return stdout.decode(\"utf-8\")\n    except Exception:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.gyp_main",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.gyp_main",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.gyp_main",
        "peekOfCode": "path = UnixifyPath(sys.argv[0])\nsys.path.insert(0, os.path.join(os.path.dirname(path), \"pylib\"))\nimport gyp  # noqa: E402\nif __name__ == \"__main__\":\n    sys.exit(gyp.script_main())",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.gyp_main",
        "documentation": {}
    },
    {
        "label": "Runner",
        "kind": 6,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "peekOfCode": "class Runner:\n    def __init__(self, formats, tests, gyp_options, verbose):\n        self.formats = formats\n        self.tests = tests\n        self.verbose = verbose\n        self.gyp_options = gyp_options\n        self.failures = []\n        self.num_tests = len(formats) * len(tests)\n        num_digits = len(str(self.num_tests))\n        self.fmt_str = \"[%%%dd/%%%dd] (%%s) %%s\" % (num_digits, num_digits)",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "documentation": {}
    },
    {
        "label": "is_test_name",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "peekOfCode": "def is_test_name(f):\n    return f.startswith(\"gyptest\") and f.endswith(\".py\")\ndef find_all_gyptest_files(directory):\n    result = []\n    for root, dirs, files in os.walk(directory):\n        result.extend([os.path.join(root, f) for f in files if is_test_name(f)])\n    result.sort()\n    return result\ndef main(argv=None):\n    if argv is None:",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "documentation": {}
    },
    {
        "label": "find_all_gyptest_files",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "peekOfCode": "def find_all_gyptest_files(directory):\n    result = []\n    for root, dirs, files in os.walk(directory):\n        result.extend([os.path.join(root, f) for f in files if is_test_name(f)])\n    result.sort()\n    return result\ndef main(argv=None):\n    if argv is None:\n        argv = sys.argv\n    parser = argparse.ArgumentParser()",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "peekOfCode": "def main(argv=None):\n    if argv is None:\n        argv = sys.argv\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-a\", \"--all\", action=\"store_true\", help=\"run all tests\")\n    parser.add_argument(\"-C\", \"--chdir\", action=\"store\", help=\"change to directory\")\n    parser.add_argument(\n        \"-f\",\n        \"--format\",\n        action=\"store\",",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "documentation": {}
    },
    {
        "label": "print_configuration_info",
        "kind": 2,
        "importPath": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "description": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "peekOfCode": "def print_configuration_info():\n    print(\"Test configuration:\")\n    if sys.platform == \"darwin\":\n        sys.path.append(os.path.abspath(\"test/lib\"))\n        import TestMac  # noqa: PLC0415\n        print(f\"  Mac {platform.mac_ver()[0]} {platform.mac_ver()[2]}\")\n        print(f\"  Xcode {TestMac.Xcode.Version()}\")\n    elif sys.platform == \"win32\":\n        sys.path.append(os.path.abspath(\"pylib\"))\n        import gyp.MSVSVersion  # noqa: PLC0415",
        "detail": ".yarn.unplugged.node-gyp-npm-12.1.0-0690767ce9.node_modules.node-gyp.gyp.test_gyp",
        "documentation": {}
    },
    {
        "label": "quick_logits_check",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.compare-logits",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.compare-logits",
        "peekOfCode": "def quick_logits_check(pytorch_file, llamacpp_file):\n    \"\"\"Lightweight sanity check before NMSE\"\"\"\n    try:\n        pytorch_logits = np.fromfile(pytorch_file, dtype=np.float32)\n        llamacpp_logits = np.fromfile(llamacpp_file, dtype=np.float32)\n    except Exception as e:\n        print(f\" NOK: Failed to load files - {e}\")\n        return False\n    # Check shapes match\n    if pytorch_logits.shape != llamacpp_logits.shape:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.compare-logits",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.compare-logits",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.compare-logits",
        "peekOfCode": "def main():\n    model_path = os.environ.get('MODEL_PATH')\n    model_name = get_model_name_from_env_path('MODEL_PATH')\n    data_dir = Path(\"data\")\n    pytorch_file = data_dir / f\"pytorch-{model_name}.bin\"\n    llamacpp_model_name = get_model_name_from_env_path('CONVERTED_MODEL')\n    print(f\"Using converted model: {llamacpp_model_name}\")\n    llamacpp_file = data_dir / f\"llamacpp-{llamacpp_model_name}.bin\"\n    if not pytorch_file.exists():\n        print(f\"Error: PyTorch logits file not found: {pytorch_file}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.compare-logits",
        "documentation": {}
    },
    {
        "label": "unreleased_model_name",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "peekOfCode": "unreleased_model_name = os.getenv('UNRELEASED_MODEL_NAME')\nparser = argparse.ArgumentParser(description='Process model with specified path')\nparser.add_argument('--model-path', '-m', help='Path to the model')\nargs = parser.parse_args()\nmodel_path = os.environ.get('MODEL_PATH', args.model_path)\nif model_path is None:\n    parser.error(\"Model path must be specified either via --model-path argument or MODEL_PATH environment variable\")\nconfig = AutoConfig.from_pretrained(model_path)\nprint(\"Model type:       \", config.model_type)\nprint(\"Vocab size:       \", config.vocab_size)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Process model with specified path')\nparser.add_argument('--model-path', '-m', help='Path to the model')\nargs = parser.parse_args()\nmodel_path = os.environ.get('MODEL_PATH', args.model_path)\nif model_path is None:\n    parser.error(\"Model path must be specified either via --model-path argument or MODEL_PATH environment variable\")\nconfig = AutoConfig.from_pretrained(model_path)\nprint(\"Model type:       \", config.model_type)\nprint(\"Vocab size:       \", config.vocab_size)\nprint(\"Hidden size:      \", config.hidden_size)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "peekOfCode": "args = parser.parse_args()\nmodel_path = os.environ.get('MODEL_PATH', args.model_path)\nif model_path is None:\n    parser.error(\"Model path must be specified either via --model-path argument or MODEL_PATH environment variable\")\nconfig = AutoConfig.from_pretrained(model_path)\nprint(\"Model type:       \", config.model_type)\nprint(\"Vocab size:       \", config.vocab_size)\nprint(\"Hidden size:      \", config.hidden_size)\nprint(\"Number of layers: \", config.num_hidden_layers)\nprint(\"BOS token id:     \", config.bos_token_id)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "peekOfCode": "model_path = os.environ.get('MODEL_PATH', args.model_path)\nif model_path is None:\n    parser.error(\"Model path must be specified either via --model-path argument or MODEL_PATH environment variable\")\nconfig = AutoConfig.from_pretrained(model_path)\nprint(\"Model type:       \", config.model_type)\nprint(\"Vocab size:       \", config.vocab_size)\nprint(\"Hidden size:      \", config.hidden_size)\nprint(\"Number of layers: \", config.num_hidden_layers)\nprint(\"BOS token id:     \", config.bos_token_id)\nprint(\"EOS token id:     \", config.eos_token_id)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "peekOfCode": "config = AutoConfig.from_pretrained(model_path)\nprint(\"Model type:       \", config.model_type)\nprint(\"Vocab size:       \", config.vocab_size)\nprint(\"Hidden size:      \", config.hidden_size)\nprint(\"Number of layers: \", config.num_hidden_layers)\nprint(\"BOS token id:     \", config.bos_token_id)\nprint(\"EOS token id:     \", config.eos_token_id)\nprint(\"Loading model and tokenizer using AutoTokenizer:\", model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nif unreleased_model_name:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(model_path)\nif unreleased_model_name:\n    model_name_lower = unreleased_model_name.lower()\n    unreleased_module_path = f\"transformers.models.{model_name_lower}.modular_{model_name_lower}\"\n    class_name = f\"{unreleased_model_name}ForCausalLM\"\n    print(f\"Importing unreleased model module: {unreleased_module_path}\")\n    try:\n        model_class = getattr(importlib.import_module(unreleased_module_path), class_name)\n        model = model_class.from_pretrained(model_path)\n    except (ImportError, AttributeError) as e:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "peekOfCode": "model_name = os.path.basename(model_path)\nprint(f\"Model name: {model_name}\")\nprompt = \"Hello world today\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\nprint(f\"Input tokens: {input_ids}\")\nprint(f\"Input text: {repr(prompt)}\")\nprint(f\"Tokenized: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\nwith torch.no_grad():\n    outputs = model(input_ids, output_hidden_states=True)\n    # Extract hidden states from the last layer",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "peekOfCode": "prompt = \"Hello world today\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\nprint(f\"Input tokens: {input_ids}\")\nprint(f\"Input text: {repr(prompt)}\")\nprint(f\"Tokenized: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\nwith torch.no_grad():\n    outputs = model(input_ids, output_hidden_states=True)\n    # Extract hidden states from the last layer\n    # outputs.hidden_states is a tuple of (num_layers + 1) tensors\n    # Index -1 gets the last layer, shape: [batch_size, seq_len, hidden_size]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "documentation": {}
    },
    {
        "label": "input_ids",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "peekOfCode": "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\nprint(f\"Input tokens: {input_ids}\")\nprint(f\"Input text: {repr(prompt)}\")\nprint(f\"Tokenized: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\nwith torch.no_grad():\n    outputs = model(input_ids, output_hidden_states=True)\n    # Extract hidden states from the last layer\n    # outputs.hidden_states is a tuple of (num_layers + 1) tensors\n    # Index -1 gets the last layer, shape: [batch_size, seq_len, hidden_size]\n    last_hidden_states = outputs.hidden_states[-1]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-casual-gen-embeddings-org",
        "documentation": {}
    },
    {
        "label": "parse_arguments",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "peekOfCode": "def parse_arguments():\n    parser = argparse.ArgumentParser(description=\"Process model with specified path\")\n    parser.add_argument(\"--model-path\", \"-m\", help=\"Path to the model\")\n    parser.add_argument(\"--prompt-file\", \"-f\", help=\"Optional prompt file\", required=False)\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Enable verbose debug output\")\n    parser.add_argument(\"--device\", \"-d\", help=\"Device to use (cpu, cuda, mps, auto)\", default=\"auto\")\n    return parser.parse_args()\ndef load_model_and_tokenizer(model_path, device=\"auto\"):\n    print(\"Loading model and tokenizer using AutoTokenizer:\", model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "documentation": {}
    },
    {
        "label": "load_model_and_tokenizer",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "peekOfCode": "def load_model_and_tokenizer(model_path, device=\"auto\"):\n    print(\"Loading model and tokenizer using AutoTokenizer:\", model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n    multimodal = False\n    full_config = config\n    # Determine device_map based on device argument\n    if device == \"cpu\":\n        device_map = {\"\": \"cpu\"}\n        print(\"Forcing CPU usage\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "documentation": {}
    },
    {
        "label": "enable_torch_debugging",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "peekOfCode": "def enable_torch_debugging(model):\n        for name, module in model.named_modules():\n            if len(list(module.children())) == 0:  # only leaf modules\n                module.register_forward_hook(debug_hook(name))\ndef get_prompt(args):\n    if args.prompt_file:\n        with open(args.prompt_file, encoding='utf-8') as f:\n            return f.read()\n    elif os.getenv(\"MODEL_TESTING_PROMPT\"):\n        return os.getenv(\"MODEL_TESTING_PROMPT\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "documentation": {}
    },
    {
        "label": "get_prompt",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "peekOfCode": "def get_prompt(args):\n    if args.prompt_file:\n        with open(args.prompt_file, encoding='utf-8') as f:\n            return f.read()\n    elif os.getenv(\"MODEL_TESTING_PROMPT\"):\n        return os.getenv(\"MODEL_TESTING_PROMPT\")\n    else:\n        return \"Hello, my name is\"\ndef main():\n    args = parse_arguments()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "peekOfCode": "def main():\n    args = parse_arguments()\n    model_path = os.environ.get(\"MODEL_PATH\", args.model_path)\n    if model_path is None:\n        print(\"Error: Model path must be specified either via --model-path argument or MODEL_PATH environment variable\")\n        sys.exit(1)\n    model, tokenizer, config = load_model_and_tokenizer(model_path, args.device)\n    if args.verbose:\n        enable_torch_debugging(model)\n    model_name = os.path.basename(model_path)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.causal.run-org-model",
        "documentation": {}
    },
    {
        "label": "parse_arguments",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "peekOfCode": "def parse_arguments():\n    parser = argparse.ArgumentParser(description='Run original embedding model')\n    parser.add_argument(\n        '--model-path',\n        '-m',\n        help='Path to the model'\n    )\n    parser.add_argument(\n        '--prompts-file',\n        '-p',",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "documentation": {}
    },
    {
        "label": "load_model_and_tokenizer",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "peekOfCode": "def load_model_and_tokenizer(model_path, use_sentence_transformers=False, device=\"auto\"):\n    if device == \"cpu\":\n        device_map = {\"\": \"cpu\"}\n        print(\"Forcing CPU usage\")\n    elif device == \"auto\":\n        # On Mac, \"auto\" device_map can cause issues with accelerate\n        # So we detect the best device manually\n        if torch.cuda.is_available():\n            device_map = {\"\": \"cuda\"}\n            print(\"Using CUDA\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "documentation": {}
    },
    {
        "label": "get_prompt",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "peekOfCode": "def get_prompt(args):\n    if args.prompts_file:\n        try:\n            with open(args.prompts_file, 'r', encoding='utf-8') as f:\n                return f.read().strip()\n        except FileNotFoundError:\n            print(f\"Error: Prompts file '{args.prompts_file}' not found\")\n            sys.exit(1)\n        except Exception as e:\n            print(f\"Error reading prompts file: {e}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "peekOfCode": "def main():\n    args = parse_arguments()\n    model_path = os.environ.get('EMBEDDING_MODEL_PATH', args.model_path)\n    if model_path is None:\n        print(\"Error: Model path must be specified either via --model-path argument \"\n              \"or EMBEDDING_MODEL_PATH environment variable\")\n        sys.exit(1)\n    # Determine if we should use SentenceTransformer\n    use_st = (\n        args.use_sentence_transformers or os.environ.get('USE_SENTENCE_TRANSFORMERS', '').lower() in ('1', 'true', 'yes')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.embedding.run-original-model",
        "documentation": {}
    },
    {
        "label": "calculate_nmse",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "peekOfCode": "def calculate_nmse(reference, test):\n    mse = np.mean((test - reference) ** 2)\n    ref_var = np.var(reference)\n    if ref_var == 0:\n        nmse = float('inf') if mse > 0 else 0.0\n        return mse, mse, ref_var\n    nmse = mse / ref_var\n    return nmse, mse, ref_var\ndef load_logits(file_path):\n    if not os.path.exists(file_path):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "documentation": {}
    },
    {
        "label": "load_logits",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "peekOfCode": "def load_logits(file_path):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n    if file_path.suffix == '.npy':\n        return np.load(file_path)\n    elif file_path.suffix == '.bin':\n        return np.fromfile(file_path, dtype=np.float32)\n    else:\n        # Try to load as text file\n        try:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "documentation": {}
    },
    {
        "label": "interpret_nmse",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "peekOfCode": "def interpret_nmse(nmse):\n    \"\"\"Provide interpretation of NMSE value\"\"\"\n    if nmse == 0:\n        return \"Perfect match\", \"\"\n    elif nmse < 1e-6:\n        return \"Essentially identical\", \"\"\n    elif nmse < 1e-4:\n        return \"Excellent match\", \"\"\n    elif nmse < 1e-3:\n        return \"Very good match\", \"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description='Validate model logits')\n    parser.add_argument('-m', '--model-path', required=True,  help='Path to the model directory')\n    args = parser.parse_args()\n    model_name = get_model_name_from_env_path('MODEL_PATH')\n    data_dir = Path(\"data\")\n    pytorch_file = data_dir / f\"pytorch-{model_name}.bin\"\n    llamacpp_model_name = get_model_name_from_env_path('CONVERTED_MODEL')\n    llamacpp_file = data_dir / f\"llamacpp-{llamacpp_model_name}.bin\"\n    print(f\"Model name: {model_name}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.check-nmse",
        "documentation": {}
    },
    {
        "label": "get_model_name_from_env_path",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "peekOfCode": "def get_model_name_from_env_path(env_path_name):\n    model_path = os.getenv(env_path_name)\n    if not model_path:\n        print(f\"Error: {env_path_name} environment variable not set\")\n        sys.exit(1)\n    if not os.path.exists(model_path):\n        print(f\"Error: Model file not found: {model_path}\")\n        sys.exit(1)\n    name = os.path.basename(os.path.normpath(model_path))\n    if name.endswith(\".gguf\"):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "documentation": {}
    },
    {
        "label": "summarize",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "peekOfCode": "def summarize(tensor: torch.Tensor, name: str, max_seq: int = 3, max_vals: int = 3):\n    \"\"\"\n    Print a tensor in llama.cpp debug style.\n    Supports:\n    - 2D tensors (seq, hidden)\n    - 3D tensors (batch, seq, hidden)\n    - 4D tensors (batch, seq, heads, dim_per_head) via flattening heads  dim_per_head\n    Shows first and last max_vals of each vector per sequence position.\n    \"\"\"\n    t = tensor.detach().to(torch.float32).cpu()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "documentation": {}
    },
    {
        "label": "debug_hook",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "peekOfCode": "def debug_hook(name):\n    def fn(_m, input, output):\n        if isinstance(input, torch.Tensor):\n            summarize(input, name + \"_in\")\n        elif isinstance(input, (tuple, list)) and len(input) > 0 and isinstance(input[0], torch.Tensor):\n            summarize(input[0], name + \"_in\")\n        if isinstance(output, torch.Tensor):\n            summarize(output, name + \"_out\")\n        elif isinstance(output, (tuple, list)) and len(output) > 0 and isinstance(output[0], torch.Tensor):\n            summarize(output[0], name + \"_out\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "documentation": {}
    },
    {
        "label": "setup_rope_debug",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "peekOfCode": "def setup_rope_debug(model_module_path: str, function_name: str = \"apply_rotary_pos_emb\"):\n    \"\"\"\n    Apply monkey patch to dump RoPE activations for debugging.\n    Args:\n        model_module_path: Path to the model module (e.g., \"transformers.models.apertus.modeling_apertus\")\n        function_name: Name of the RoPE function to patch (default: \"apply_rotary_pos_emb\")\n    Example:\n        from utils.common import setup_rope_debug\n        setup_rope_debug(\"transformers.models.apertus.modeling_apertus\")\n    \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "documentation": {}
    },
    {
        "label": "save_output_data",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "peekOfCode": "def save_output_data(data, tokens, prompt, model_name, type_suffix=\"\", output_dir=\"data\"):\n    \"\"\"\n    Save output data (logits/embeddings), tokens, and prompt to files.\n    Args:\n        data:        numpy array of floats (logits or embeddings)\n        tokens:      list or array of token IDs\n        prompt:      string containing the input prompt\n        model_name:  name of the model\n        type_suffix: optional suffix like \"-embeddings\" (default: \"\")\n        output_dir:  directory to save files (default: \"data\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "documentation": {}
    },
    {
        "label": "compare_tokens",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "peekOfCode": "def compare_tokens(original, converted, type_suffix=\"\", output_dir=\"data\"):\n    data_dir = Path(output_dir)\n    # Read tokens from both models\n    tokens1_file = data_dir / f\"{original}{type_suffix}-tokens.bin\"\n    tokens2_file = data_dir / f\"{converted}{type_suffix}-tokens.bin\"\n    if not tokens1_file.exists():\n        print(f\"Error: Token file not found: {tokens1_file}\")\n        return False\n    if not tokens2_file.exists():\n        print(f\"Error: Token file not found: {tokens2_file}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "documentation": {}
    },
    {
        "label": "show_version_warning",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "peekOfCode": "def show_version_warning(current_version, model_version):\n    if not model_version:\n        return False\n    try:\n        from packaging.version import parse, InvalidVersion\n        try:\n            return parse(current_version) < parse(model_version)\n        except InvalidVersion:\n            return current_version != model_version\n    except ImportError:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "documentation": {}
    },
    {
        "label": "get_model_transformers_version",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "peekOfCode": "def get_model_transformers_version(model_path):\n    if not model_path:\n        return None\n    config_path = Path(model_path) / \"config.json\"\n    if not config_path.is_file():\n        return None\n    try:\n        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config = json.load(f)\n        return config.get(\"transformers_version\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "documentation": {}
    },
    {
        "label": "exit_with_warning",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "peekOfCode": "def exit_with_warning(message, model_path):\n    print(message)\n    if model_path and transformers is not None:\n        model_transformers_version = get_model_transformers_version(model_path)\n        transformers_version       = transformers.__version__\n        if show_version_warning(transformers_version, model_transformers_version):\n            warning_message = f\"\"\"\n                =====================================================================\n                Verification failure might be due to a transformers version mismatch:\n                Current transformers version: {transformers_version}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.common",
        "documentation": {}
    },
    {
        "label": "parse_arguments",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.compare_tokens",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.compare_tokens",
        "peekOfCode": "def parse_arguments():\n    parser = argparse.ArgumentParser(\n        description='Compare tokens between two models',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  %(prog)s pytorch-gemma-3-270m-it llamacpp-gemma-3-270m-it-bf16\n        \"\"\"\n    )\n    parser.add_argument(",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.compare_tokens",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.compare_tokens",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.compare_tokens",
        "peekOfCode": "def main():\n    args = parse_arguments()\n    if args.verbose:\n        from pathlib import Path\n        data_dir = Path(args.data_dir)\n        prompt1_file = data_dir / f\"{args.original}{args.suffix}-prompt.txt\"\n        prompt2_file = data_dir / f\"{args.converted}{args.suffix}-prompt.txt\"\n        if prompt1_file.exists():\n            print(f\"\\nOriginal model prompt ({args.original}):\")\n            print(f\"  {prompt1_file.read_text().strip()}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.compare_tokens",
        "documentation": {}
    },
    {
        "label": "add_model_to_collection",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-add-model-to-collection",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-add-model-to-collection",
        "peekOfCode": "def add_model_to_collection(collection_slug, model_id, note=\"\"):\n    \"\"\"\n    Add a model to an existing collection\n    Args:\n        collection_slug: The slug of the collection (e.g., \"username/collection-name-12345\")\n        model_id: The model repository ID (e.g., \"username/model-name\")\n        note: Optional note about the model\n    Returns:\n        True if successful, False if failed\n    \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-add-model-to-collection",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-add-model-to-collection",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-add-model-to-collection",
        "peekOfCode": "def main():\n    # This script requires that the environment variable HF_TOKEN is set with your\n    # Hugging Face API token.\n    api = HfApi()\n    parser = argparse.ArgumentParser(description='Add model to a Huggingface Collection')\n    parser.add_argument('--collection', '-c', help='The collection slug username/collection-hash', required=True)\n    parser.add_argument('--model', '-m', help='The model to add to the Collection', required=True)\n    parser.add_argument('--note', '-n', help='An optional note/description', required=False)\n    args = parser.parse_args()\n    collection = args.collection",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-add-model-to-collection",
        "documentation": {}
    },
    {
        "label": "create_collection",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-collection",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-collection",
        "peekOfCode": "def create_collection(title, description, private=False, namespace=None, return_slug=False):\n    \"\"\"\n    Create a new collection on Hugging Face\n    Args:\n        title: Collection title\n        description: Collection description\n        private: Whether the collection should be private (default: False)\n        namespace: Optional namespace (defaults to your username)\n    Returns:\n        Collection object if successful, None if failed",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-collection",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-collection",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-collection",
        "peekOfCode": "def main():\n    # This script requires that the environment variable HF_TOKEN is set with your\n    # Hugging Face API token.\n    api = HfApi()\n    parser = argparse.ArgumentParser(description='Create a Huggingface Collection')\n    parser.add_argument('--name', '-n', help='The name/title of the Collection', required=True)\n    parser.add_argument('--description', '-d', help='The description for the Collection', required=True)\n    parser.add_argument('--namespace', '-ns', help='The namespace to add the Collection to', required=True)\n    parser.add_argument('--private', '-p', help='Create a private Collection', action='store_true')  # Fixed\n    parser.add_argument('--return-slug', '-s', help='Only output the collection slug', action='store_true')  # Fixed",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-collection",
        "documentation": {}
    },
    {
        "label": "load_template_and_substitute",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "peekOfCode": "def load_template_and_substitute(template_path, **kwargs):\n    try:\n        with open(template_path, 'r', encoding='utf-8') as f:\n            template_content = f.read()\n        return template_content.format(**kwargs)\n    except FileNotFoundError:\n        print(f\"Template file '{template_path}' not found!\")\n        return None\n    except KeyError as e:\n        print(f\"Missing template variable: {e}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "documentation": {}
    },
    {
        "label": "api",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "peekOfCode": "api = HfApi()\ndef load_template_and_substitute(template_path, **kwargs):\n    try:\n        with open(template_path, 'r', encoding='utf-8') as f:\n            template_content = f.read()\n        return template_content.format(**kwargs)\n    except FileNotFoundError:\n        print(f\"Template file '{template_path}' not found!\")\n        return None\n    except KeyError as e:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Create a new Hugging Face model repository')\nparser.add_argument('--model-name', '-m', help='Name for the model', required=True)\nparser.add_argument('--namespace', '-ns', help='Namespace to add the model to', required=True)\nparser.add_argument('--org-base-model', '-b', help='Original Base model name', default=\"\")\nparser.add_argument('--no-card', action='store_true', help='Skip creating model card')\nparser.add_argument('--private', '-p', action='store_true', help='Create private model')\nparser.add_argument('--embedding', '-e', action='store_true', help='Use embedding model card template')\nparser.add_argument('--dry-run', '-d', action='store_true', help='Print repository info and template without creating repository')\nargs = parser.parse_args()\nrepo_id = f\"{args.namespace}/{args.model_name}-GGUF\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "peekOfCode": "args = parser.parse_args()\nrepo_id = f\"{args.namespace}/{args.model_name}-GGUF\"\nprint(\"Repository ID: \", repo_id)\nrepo_url = None\nif not args.dry_run:\n    repo_url = api.create_repo(\n        repo_id=repo_id,\n        repo_type=\"model\",\n        private=args.private,\n        exist_ok=False",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "documentation": {}
    },
    {
        "label": "repo_id",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "peekOfCode": "repo_id = f\"{args.namespace}/{args.model_name}-GGUF\"\nprint(\"Repository ID: \", repo_id)\nrepo_url = None\nif not args.dry_run:\n    repo_url = api.create_repo(\n        repo_id=repo_id,\n        repo_type=\"model\",\n        private=args.private,\n        exist_ok=False\n    )",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "documentation": {}
    },
    {
        "label": "repo_url",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "peekOfCode": "repo_url = None\nif not args.dry_run:\n    repo_url = api.create_repo(\n        repo_id=repo_id,\n        repo_type=\"model\",\n        private=args.private,\n        exist_ok=False\n    )\nif not args.no_card:\n    if args.embedding:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-create-model",
        "documentation": {}
    },
    {
        "label": "upload_gguf_file",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "peekOfCode": "def upload_gguf_file(local_file_path, repo_id, filename_in_repo=None):\n    \"\"\"\n    Upload a GGUF file to a Hugging Face model repository\n    Args:\n        local_file_path: Path to your local GGUF file\n        repo_id: Your repository ID (e.g., \"username/model-name\")\n        filename_in_repo: Optional custom name for the file in the repo\n    \"\"\"\n    if not os.path.exists(local_file_path):\n        print(f\" File not found: {local_file_path}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "documentation": {}
    },
    {
        "label": "api",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "peekOfCode": "api = HfApi()\nparser = argparse.ArgumentParser(description='Upload a GGUF model to a Huggingface model repository')\nparser.add_argument('--gguf-model-path', '-m', help='The GGUF model file to upload', required=True)\nparser.add_argument('--repo-id', '-r', help='The repository to upload to', required=True)\nparser.add_argument('--name', '-o', help='The name in the model repository', required=False)\nargs = parser.parse_args()\nupload_gguf_file(args.gguf_model_path, args.repo_id, args.name)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Upload a GGUF model to a Huggingface model repository')\nparser.add_argument('--gguf-model-path', '-m', help='The GGUF model file to upload', required=True)\nparser.add_argument('--repo-id', '-r', help='The repository to upload to', required=True)\nparser.add_argument('--name', '-o', help='The name in the model repository', required=False)\nargs = parser.parse_args()\nupload_gguf_file(args.gguf_model_path, args.repo_id, args.name)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "peekOfCode": "args = parser.parse_args()\nupload_gguf_file(args.gguf_model_path, args.repo_id, args.name)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.hf-upload-gguf-model",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Process model with specified path')\nparser.add_argument('--model-path', '-m', help='Path to the model')\nargs = parser.parse_args()\nmodel_path = os.environ.get('MODEL_PATH', args.model_path)\nif model_path is None:\n    parser.error(\"Model path must be specified either via --model-path argument or MODEL_PATH environment variable\")\n# Check if there's an index file (multi-file model)\nindex_path = os.path.join(model_path, \"model.safetensors.index.json\")\nsingle_file_path = os.path.join(model_path, \"model.safetensors\")\nif os.path.exists(index_path):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "peekOfCode": "args = parser.parse_args()\nmodel_path = os.environ.get('MODEL_PATH', args.model_path)\nif model_path is None:\n    parser.error(\"Model path must be specified either via --model-path argument or MODEL_PATH environment variable\")\n# Check if there's an index file (multi-file model)\nindex_path = os.path.join(model_path, \"model.safetensors.index.json\")\nsingle_file_path = os.path.join(model_path, \"model.safetensors\")\nif os.path.exists(index_path):\n    # Multi-file model\n    print(\"Multi-file model detected\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "peekOfCode": "model_path = os.environ.get('MODEL_PATH', args.model_path)\nif model_path is None:\n    parser.error(\"Model path must be specified either via --model-path argument or MODEL_PATH environment variable\")\n# Check if there's an index file (multi-file model)\nindex_path = os.path.join(model_path, \"model.safetensors.index.json\")\nsingle_file_path = os.path.join(model_path, \"model.safetensors\")\nif os.path.exists(index_path):\n    # Multi-file model\n    print(\"Multi-file model detected\")\n    with open(index_path, 'r') as f:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "documentation": {}
    },
    {
        "label": "index_path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "peekOfCode": "index_path = os.path.join(model_path, \"model.safetensors.index.json\")\nsingle_file_path = os.path.join(model_path, \"model.safetensors\")\nif os.path.exists(index_path):\n    # Multi-file model\n    print(\"Multi-file model detected\")\n    with open(index_path, 'r') as f:\n        index_data = json.load(f)\n    # Get the weight map (tensor_name -> file_name)\n    weight_map = index_data.get(\"weight_map\", {})\n    # Group tensors by file for efficient processing",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "documentation": {}
    },
    {
        "label": "single_file_path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "peekOfCode": "single_file_path = os.path.join(model_path, \"model.safetensors\")\nif os.path.exists(index_path):\n    # Multi-file model\n    print(\"Multi-file model detected\")\n    with open(index_path, 'r') as f:\n        index_data = json.load(f)\n    # Get the weight map (tensor_name -> file_name)\n    weight_map = index_data.get(\"weight_map\", {})\n    # Group tensors by file for efficient processing\n    file_tensors = defaultdict(list)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.inspect-org-model",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "peekOfCode": "def cosine_similarity(a, b=None):\n    a = np.asarray(a)\n    if b is None:\n        b = a\n    else:\n        b = np.asarray(b)\n    if a.ndim == 1:\n        a = a.reshape(1, -1)\n    if b.ndim == 1:\n        b = b.reshape(1, -1)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "documentation": {}
    },
    {
        "label": "load_embeddings_from_file",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "peekOfCode": "def load_embeddings_from_file(filename, n_tokens, n_embd):\n    embeddings = np.fromfile(filename, dtype=np.float32)\n    # Check if this is pooled (single embedding) or per-token embeddings\n    if len(embeddings) == n_embd:\n        return embeddings.reshape(1, n_embd)\n    else:\n        return embeddings.reshape(n_tokens, n_embd)\ndef test_single_prompt_similarity(python_emb, cpp_emb, tokens, prompt):\n    np.set_printoptions(suppress=True, precision=6)\n    print(\"pytorch embeddings:\");",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "documentation": {}
    },
    {
        "label": "test_single_prompt_similarity",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "peekOfCode": "def test_single_prompt_similarity(python_emb, cpp_emb, tokens, prompt):\n    np.set_printoptions(suppress=True, precision=6)\n    print(\"pytorch embeddings:\");\n    print(python_emb)\n    print(\"llama.cpp embeddings:\");\n    print(cpp_emb)\n    print(f\"\\n=== Prompt: '{prompt}' ===\")\n    print(f\"Tokens: {tokens}\")\n    print(f\"Embeddings shape: Python {python_emb.shape}, llama.cpp {cpp_emb.shape}\")\n    n_tokens = len(tokens)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "documentation": {}
    },
    {
        "label": "read_prompt_from_file",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "peekOfCode": "def read_prompt_from_file(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read().strip()\n    except FileNotFoundError:\n        print(f\"Error: Prompts file '{file_path}' not found\")\n        exit(1)\n    except Exception as e:\n        print(f\"Error reading prompts file: {e}\")\n        exit(1)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description='Test semantic similarity between Python and llama.cpp embeddings')\n    parser.add_argument('--model-path', '-m', required=True, help='Path to the original Python model')\n    parser.add_argument('--python-embeddings', '-pe', help='Path to pytorch embeddings \"logits\" binary file')\n    parser.add_argument('--cpp-embeddings', '-ce', help='Path to llama.cpp embeddings \"logits\" binary file')\n    parser.add_argument('--causal', '-c', default=False, help='if the model is causal (default: false)', action='store_true')\n    parser.add_argument('--prompt', '-p', default='Hello world today', help='Test prompt')\n    parser.add_argument('--prompts-file', '-pf', help='Path to file containing prompts')\n    args = parser.parse_args()\n    if args.prompts_file:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "documentation": {}
    },
    {
        "label": "unreleased_model_name",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "peekOfCode": "unreleased_model_name = os.getenv('UNRELEASED_MODEL_NAME')\ndef cosine_similarity(a, b=None):\n    a = np.asarray(a)\n    if b is None:\n        b = a\n    else:\n        b = np.asarray(b)\n    if a.ndim == 1:\n        a = a.reshape(1, -1)\n    if b.ndim == 1:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.model-conversion.scripts.utils.semantic_check",
        "documentation": {}
    },
    {
        "label": "DataType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class DataType:\n    name: str\n    dtype: np.dtype[Any]\n    valid_conversions: list[str]\n    def elements_to_bytes(self, n_elements: int) -> int:\n        return n_elements * self.dtype.itemsize\n@dataclass(frozen=True)\nclass UnquantizedDataType(DataType):\n    pass\nDT_F16  = UnquantizedDataType('F16',  dtype = np.dtype(np.float16), valid_conversions = ['F32', 'Q8_0'])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "UnquantizedDataType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class UnquantizedDataType(DataType):\n    pass\nDT_F16  = UnquantizedDataType('F16',  dtype = np.dtype(np.float16), valid_conversions = ['F32', 'Q8_0'])\nDT_F32  = UnquantizedDataType('F32',  dtype = np.dtype(np.float32), valid_conversions = ['F16', 'Q8_0'])\nDT_I32  = UnquantizedDataType('I32',  dtype = np.dtype(np.int16),   valid_conversions = [])\nDT_BF16 = UnquantizedDataType('BF16', dtype = np.dtype(np.uint16),  valid_conversions = ['F32', 'F16', 'Q8_0'])\n@dataclass(frozen=True)\nclass QuantizedDataType(DataType):\n    block_size: int\n    quantized_dtype: np.dtype[Any]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "QuantizedDataType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class QuantizedDataType(DataType):\n    block_size: int\n    quantized_dtype: np.dtype[Any]\n    ggml_type: gguf.GGMLQuantizationType\n    def quantize(self, arr: NDArray) -> NDArray:\n        raise NotImplementedError(f'Quantization for {self.name} not implemented')\n    def elements_to_bytes(self, n_elements: int) -> int:\n        assert n_elements % self.block_size == 0, f'Invalid number of elements {n_elements} for {self.name} with block size {self.block_size}'\n        return self.quantized_dtype.itemsize * (n_elements // self.block_size)\n@dataclass(frozen=True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "Q8_0QuantizedDataType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class Q8_0QuantizedDataType(QuantizedDataType):\n    # Mini Q8_0 quantization in Python!\n    def quantize(self, arr: NDArray) -> NDArray:\n        assert arr.size % self.block_size == 0 and arr.size != 0, f'Bad array size {arr.size}'\n        assert arr.dtype == np.float32, f'Bad array type {arr.dtype}'\n        n_blocks = arr.size // self.block_size\n        blocks = arr.reshape((n_blocks, self.block_size))\n        # Much faster implementation of block quantization contributed by @Cebtenzzre\n        def quantize_blocks_q8_0(blocks: NDArray) -> Iterable[tuple[Any, Any]]:\n            d = abs(blocks).max(axis = 1) / np.float32(127)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "GGMLFileType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class GGMLFileType(enum.IntEnum):\n    AllF32     = 0\n    MostlyF16  = 1  # except 1d tensors\n    MostlyQ8_0 = 7  # except 1d tensors\n    def type_for_tensor(self, name: str, tensor: LazyTensor) -> DataType:\n        dt = GGML_FILE_TYPE_TO_DATA_TYPE.get(self)\n        if dt is None:\n            raise ValueError(self)\n        # Convert all 1D tensors to F32.  Most of the codebase that takes in 1D tensors only handles F32 tensors, and most of the outputs tensors are F32.\n        #  Also The 1d tensors aren't much of a performance/size issue.  So instead of having to have separate F32 and F16 implementations of both, just convert everything to F32 for now.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "Params",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class Params:\n    n_vocab:        int\n    n_embd:         int\n    n_layer:        int\n    n_ctx:          int\n    n_ff:           int\n    n_head:         int\n    n_head_kv:      int\n    n_experts:      int | None = None\n    n_experts_used: int | None = None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class Tensor(ABC):\n    ndarray: NDArray\n    data_type: DataType\n    @abstractmethod\n    def astype(self, data_type: DataType) -> Self: ...\n    @abstractmethod\n    def permute(self, n_head: int, n_head_kv: int) -> Self: ...\n    @abstractmethod\n    def permute_part(self, n_part: int, n_head: int, n_head_kv: int) -> Self: ...\n    @abstractmethod",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "UnquantizedTensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class UnquantizedTensor(Tensor):\n    def __init__(self, ndarray: NDArray):\n        assert isinstance(ndarray, np.ndarray)\n        self.ndarray = ndarray\n        self.data_type = NUMPY_TYPE_TO_DATA_TYPE[ndarray.dtype]\n    def astype(self, data_type: DataType) -> UnquantizedTensor:\n        dtype = data_type.dtype\n        if self.data_type == DT_BF16:\n            self.ndarray = bf16_to_fp32(self.ndarray)\n        return UnquantizedTensor(self.ndarray.astype(dtype))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "LazyTensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class LazyTensor:\n    _load: Callable[[], Tensor]\n    shape: list[int]\n    data_type: DataType\n    description: str\n    def load(self) -> Tensor:\n        ret = self._load()\n        # Should be okay if it maps to the same numpy type?\n        assert ret.data_type == self.data_type or (self.data_type.dtype == ret.data_type.dtype), \\\n            (self.data_type, ret.data_type, self.description)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "ModelPlus",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class ModelPlus:\n    model: LazyModel\n    paths: list[Path]  # Where this was read from.\n    format: ModelFormat\n    vocab: BaseVocab | None  # For GGML models (which have vocab built in), the vocab.\ndef merge_sharded(models: list[LazyModel]) -> LazyModel:\n    # Original LLaMA models have each file contain one part of each tensor.\n    # Use a dict instead of a set to preserve order.\n    names = {name: None for model in models for name in model}\n    def convert(name: str) -> LazyTensor:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "LazyStorageKind",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class LazyStorageKind:\n    data_type: DataType\n@dataclass\nclass LazyStorage:\n    load: Callable[[int, int], NDArray]\n    kind: LazyStorageKind\n    description: str\nclass LazyUnpickler(pickle.Unpickler):\n    def __init__(self, fp: IO[bytes], data_base_path: str, zip_file: zipfile.ZipFile):\n        super().__init__(fp)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "LazyStorage",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class LazyStorage:\n    load: Callable[[int, int], NDArray]\n    kind: LazyStorageKind\n    description: str\nclass LazyUnpickler(pickle.Unpickler):\n    def __init__(self, fp: IO[bytes], data_base_path: str, zip_file: zipfile.ZipFile):\n        super().__init__(fp)\n        self.data_base_path = data_base_path\n        self.zip_file = zip_file\n    def persistent_load(self, pid: Any) -> Any:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "LazyUnpickler",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class LazyUnpickler(pickle.Unpickler):\n    def __init__(self, fp: IO[bytes], data_base_path: str, zip_file: zipfile.ZipFile):\n        super().__init__(fp)\n        self.data_base_path = data_base_path\n        self.zip_file = zip_file\n    def persistent_load(self, pid: Any) -> Any:\n        assert pid[0] == 'storage'\n        assert isinstance(pid[1], LazyStorageKind)\n        data_type = pid[1].data_type\n        filename_stem = pid[2]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "OutputFile",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class OutputFile:\n    def __init__(self, fname_out: Path, endianess:gguf.GGUFEndian = gguf.GGUFEndian.LITTLE):\n        self.gguf = gguf.GGUFWriter(fname_out, gguf.MODEL_ARCH_NAMES[ARCH], endianess=endianess)\n    def add_meta_model(self, params: Params, metadata: gguf.Metadata | None) -> None:\n        # Metadata About The Model And Its Provenence\n        name = \"LLaMA\"\n        if metadata is not None and metadata.name is not None:\n            name = metadata.name\n        elif params.path_model is not None:\n            name = params.path_model.name",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "VocabFactory",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "class VocabFactory:\n    _VOCAB_CLASSES: list[type[Vocab]] = [SentencePieceVocab, BpeVocab, LlamaHfVocab]\n    def __init__(self, path: Path):\n        self.path = path\n    def _create_special_vocab(self, vocab: BaseVocab, model_parent_path: Path) -> gguf.SpecialVocab:\n        load_merges = vocab.name == \"bpe\"\n        n_vocab = vocab.vocab_size if isinstance(vocab, Vocab) else None\n        return gguf.SpecialVocab(\n            model_parent_path,\n            load_merges=load_merges,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "permute",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def permute(weights: NDArray, n_head: int, n_head_kv: int) -> NDArray:\n    if n_head_kv is not None and n_head != n_head_kv:\n        n_head = n_head_kv\n    return (weights.reshape(n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:])\n            .swapaxes(1, 2)\n            .reshape(weights.shape))\nclass Tensor(ABC):\n    ndarray: NDArray\n    data_type: DataType\n    @abstractmethod",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "bf16_to_fp32",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def bf16_to_fp32(bf16_arr: np.ndarray[Any, np.dtype[np.uint16]]) -> NDArray:\n    assert bf16_arr.dtype == np.uint16, f\"Input array should be of dtype uint16, but got {bf16_arr.dtype}\"\n    fp32_arr = bf16_arr.astype(np.uint32) << 16\n    return fp32_arr.view(np.float32)\nclass UnquantizedTensor(Tensor):\n    def __init__(self, ndarray: NDArray):\n        assert isinstance(ndarray, np.ndarray)\n        self.ndarray = ndarray\n        self.data_type = NUMPY_TYPE_TO_DATA_TYPE[ndarray.dtype]\n    def astype(self, data_type: DataType) -> UnquantizedTensor:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "load_unquantized",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def load_unquantized(lazy_tensor: LazyTensor, expected_dtype: Any = None, convert: bool = False) -> NDArray:\n    tensor = lazy_tensor.load()\n    assert isinstance(tensor, UnquantizedTensor)\n    # double-check:\n    actual_shape = list(tensor.ndarray.shape)\n    assert actual_shape == lazy_tensor.shape, (actual_shape, lazy_tensor.shape)\n    if expected_dtype is not None and expected_dtype != tensor.ndarray.dtype:\n        if convert:\n            tensor.ndarray = tensor.ndarray.astype(expected_dtype)\n        else:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "merge_sharded",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def merge_sharded(models: list[LazyModel]) -> LazyModel:\n    # Original LLaMA models have each file contain one part of each tensor.\n    # Use a dict instead of a set to preserve order.\n    names = {name: None for model in models for name in model}\n    def convert(name: str) -> LazyTensor:\n        lazy_tensors = [model[name] for model in models]\n        if len(lazy_tensors) == 1:\n            # only one file; don't go through this procedure since there might\n            # be quantized tensors\n            return lazy_tensors[0]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "merge_multifile_models",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def merge_multifile_models(models_plus: list[ModelPlus]) -> ModelPlus:\n    formats: set[ModelFormat] = set(mp.format for mp in models_plus)\n    assert len(formats) == 1, \"different formats?\"\n    format = formats.pop()\n    paths = [path for mp in models_plus for path in mp.paths]\n    # Use the first non-None vocab, if any.\n    try:\n        vocab = next(mp.vocab for mp in models_plus if mp.vocab is not None)\n    except StopIteration:\n        vocab = None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "permute_lazy",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def permute_lazy(lazy_tensor: LazyTensor, n_head: int, n_head_kv: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().permute(n_head, n_head_kv)\n    return LazyTensor(load, lazy_tensor.shape, lazy_tensor.data_type, f'permute({n_head}, {n_head_kv}) ' + lazy_tensor.description)\ndef permute_part_lazy(lazy_tensor: LazyTensor, n_part: int, n_head: int, n_head_kv: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().permute_part(n_part, n_head, n_head_kv)\n    s = lazy_tensor.shape.copy()\n    s[0] = s[0] // 3\n    return LazyTensor(load, s, lazy_tensor.data_type, f'permute({n_head}, {n_head_kv}) ' + lazy_tensor.description)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "permute_part_lazy",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def permute_part_lazy(lazy_tensor: LazyTensor, n_part: int, n_head: int, n_head_kv: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().permute_part(n_part, n_head, n_head_kv)\n    s = lazy_tensor.shape.copy()\n    s[0] = s[0] // 3\n    return LazyTensor(load, s, lazy_tensor.data_type, f'permute({n_head}, {n_head_kv}) ' + lazy_tensor.description)\ndef part_lazy(lazy_tensor: LazyTensor, n_part: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().part(n_part)\n    s = lazy_tensor.shape.copy()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "part_lazy",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def part_lazy(lazy_tensor: LazyTensor, n_part: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().part(n_part)\n    s = lazy_tensor.shape.copy()\n    s[0] = s[0] // 3\n    return LazyTensor(load, s, lazy_tensor.data_type, 'part ' + lazy_tensor.description)\ndef pack_experts_lazy(lazy_tensors: list[LazyTensor]) -> LazyTensor:\n    def load() -> Tensor:\n        tensors = [lazy_tensor.load() for lazy_tensor in lazy_tensors]\n        return UnquantizedTensor(np.array([tensor.ndarray for tensor in tensors]))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "pack_experts_lazy",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def pack_experts_lazy(lazy_tensors: list[LazyTensor]) -> LazyTensor:\n    def load() -> Tensor:\n        tensors = [lazy_tensor.load() for lazy_tensor in lazy_tensors]\n        return UnquantizedTensor(np.array([tensor.ndarray for tensor in tensors]))\n    s = lazy_tensors[0].shape.copy()\n    s.insert(0, len(lazy_tensors))\n    return LazyTensor(load, s, lazy_tensors[0].data_type, 'pack_experts ' + ' | '.join(lt.description for lt in lazy_tensors))\n# Functionality that simulates `torch.load` but where individual tensors are\n# only loaded into memory on demand, not all at once.\n# PyTorch can't do this natively as of time of writing:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "lazy_load_torch_file",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def lazy_load_torch_file(outer_fp: IO[bytes], path: Path) -> ModelPlus:\n    zf = zipfile.ZipFile(outer_fp)\n    pickle_paths = [name for name in zf.namelist() if name.endswith('.pkl')]\n    assert len(pickle_paths) == 1, pickle_paths\n    pickle_fp = zf.open(pickle_paths[0], 'r')\n    unpickler = LazyUnpickler(pickle_fp,\n                              data_base_path=pickle_paths[0][:-4],\n                              zip_file=zf)\n    model = unpickler.load()\n    if 'model' in model: model = model['model']",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "lazy_load_safetensors_file",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def lazy_load_safetensors_file(fp: IO[bytes], path: Path) -> ModelPlus:\n    header_size, = struct.unpack('<Q', fp.read(8))\n    header: dict[str, dict[str, Any]] = json.loads(fp.read(header_size))\n    # Use mmap for the actual data to avoid race conditions with the file offset.\n    mapped = memoryview(mmap.mmap(fp.fileno(), 0, access=mmap.ACCESS_READ))\n    byte_buf = mapped[8 + header_size:]\n    def convert(info: dict[str, Any]) -> LazyTensor:\n        data_type = SAFETENSORS_DATA_TYPES[info['dtype']]\n        numpy_dtype = data_type.dtype\n        shape: list[int] = info['shape']",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "must_read",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def must_read(fp: IO[bytes], length: int) -> bytes:\n    ret = fp.read(length)\n    if len(ret) < length:\n        raise EOFError(\"unexpectedly reached end of file\")\n    return ret\n@functools.lru_cache(maxsize=None)\ndef lazy_load_file(path: Path) -> ModelPlus:\n    fp = open(path, 'rb')\n    first8 = fp.read(8)\n    fp.seek(0)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "lazy_load_file",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def lazy_load_file(path: Path) -> ModelPlus:\n    fp = open(path, 'rb')\n    first8 = fp.read(8)\n    fp.seek(0)\n    if first8[:2] == b'PK':\n        # A zip file, i.e. PyTorch format\n        return lazy_load_torch_file(fp, path)\n    elif struct.unpack('<Q', first8)[0] < 16 * 1024 * 1024:\n        # Probably safetensors\n        return lazy_load_safetensors_file(fp, path)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "bounded_parallel_map",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def bounded_parallel_map(func: Callable[[In], Out], iterable: Iterable[In], concurrency: int, max_workers: int | None = None, use_processpool_executor: bool = False) -> Iterable[Out]:\n    '''Parallel map, but with backpressure.  If the caller doesn't call `next`\n    fast enough, this will stop calling `func` at some point rather than\n    letting results pile up in memory.  Specifically, there is a max of one\n    output value buffered per thread.'''\n    if concurrency < 2:\n        yield from map(func, iterable)\n        # Not reached.\n    iterable = iter(iterable)\n    executor_class: type[ThreadPoolExecutor] | type[ProcessPoolExecutor]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "check_vocab_size",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def check_vocab_size(params: Params, vocab: BaseVocab, pad_vocab: bool = False) -> None:\n    # Handle special case where the model's vocab size is not set\n    if params.n_vocab == -1:\n        raise ValueError(\n            \"The model's vocab size is set to -1 in params.json. Please update it manually.\"\n            + (f\" Maybe {vocab.vocab_size}?\" if isinstance(vocab, Vocab) else \"\"),\n        )\n    if not isinstance(vocab, Vocab):\n        return  # model has no vocab\n    # Check for a vocab size mismatch",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "pick_output_type",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def pick_output_type(model: LazyModel, output_type_str: str | None) -> GGMLFileType:\n    wq_type = model[gguf.TENSOR_NAMES[gguf.MODEL_TENSOR.ATTN_Q].format(bid=0) + \".weight\"].data_type\n    if output_type_str == \"f32\" or (output_type_str is None and wq_type in (DT_F32, DT_BF16)):\n        return GGMLFileType.AllF32\n    if output_type_str == \"f16\" or (output_type_str is None and wq_type == DT_F16):\n        return GGMLFileType.MostlyF16\n    if output_type_str == \"q8_0\":\n        return GGMLFileType.MostlyQ8_0\n    name_to_type = {name: lazy_tensor.data_type for (name, lazy_tensor) in model.items()}\n    raise ValueError(f\"Unexpected combination of types: {name_to_type}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "per_model_weight_count_estimation",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def per_model_weight_count_estimation(tensors: Iterable[tuple[str, LazyTensor]]) -> tuple[int, int, int]:\n    total_params = 0\n    shared_params = 0\n    expert_params = 0\n    for name, lazy_tensor in tensors:\n        # We don't need these\n        if name.endswith((\".attention.masked_bias\", \".attention.bias\", \".rotary_emb.inv_freq\")):\n            continue\n        # Got A Tensor\n        sum_weights_in_tensor: int = 1",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "convert_to_output_type",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def convert_to_output_type(model: LazyModel, output_type: GGMLFileType) -> LazyModel:\n    return {name: tensor.astype(output_type.type_for_tensor(name, tensor))\n            for (name, tensor) in model.items()}\ndef convert_model_names(model: LazyModel, params: Params, skip_unknown: bool) -> LazyModel:\n    tmap = gguf.TensorNameMap(ARCH, params.n_layer)\n    should_skip = set(gguf.MODEL_TENSOR_SKIP.get(ARCH, []))\n    tmp = model\n    # merge experts into one tensor\n    if params.n_experts and params.n_experts > 0:\n        for i_l in range(params.n_layer):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "convert_model_names",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def convert_model_names(model: LazyModel, params: Params, skip_unknown: bool) -> LazyModel:\n    tmap = gguf.TensorNameMap(ARCH, params.n_layer)\n    should_skip = set(gguf.MODEL_TENSOR_SKIP.get(ARCH, []))\n    tmp = model\n    # merge experts into one tensor\n    if params.n_experts and params.n_experts > 0:\n        for i_l in range(params.n_layer):\n            for w in range(1, 4):\n                experts = []\n                for e in range(params.n_experts):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "nth_multifile_path",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def nth_multifile_path(path: Path, n: int) -> Path | None:\n    '''Given any path belonging to a multi-file model (e.g. foo.bin.1), return\n    the nth path in the model.\n    '''\n    # Support the following patterns:\n    patterns = [\n        # - x.00.pth, x.01.pth, etc.\n        (r'\\.[0-9]{2}\\.pth$', f'.{n:02}.pth'),\n        # - x-00001-of-00002.bin, x-00002-of-00002.bin, etc.\n        (r'-[0-9]{5}-of-(.*)$', fr'-{n:05}-of-\\1'),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "find_multifile_paths",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def find_multifile_paths(path: Path) -> list[Path]:\n    '''Given any path belonging to a multi-file model (e.g. foo.bin.1), return\n    the whole list of paths in the model.\n    '''\n    ret: list[Path] = []\n    for i in itertools.count():\n        nth_path = nth_multifile_path(path, i)\n        if nth_path is None:\n            break\n        ret.append(nth_path)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "load_some_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def load_some_model(path: Path) -> ModelPlus:\n    '''Load a model of any supported format.'''\n    # Be extra-friendly and accept either a file or a directory:\n    if path.is_dir():\n        # Check if it's a set of safetensors files first\n        globs = [\"model-00001-of-*.safetensors\", \"model.safetensors\", \"consolidated.safetensors\"]\n        files = [file for glob in globs for file in path.glob(glob)]\n        if not files:\n            # Try the PyTorch patterns too, with lower priority\n            globs = [\"consolidated.00.pth\", \"pytorch_model-00001-of-*.bin\", \"*.pt\", \"pytorch_model.bin\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "default_convention_outfile",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def default_convention_outfile(file_type: GGMLFileType, expert_count: int | None, model_params_count: tuple[int, int, int], metadata: gguf.Metadata) -> str:\n    name = metadata.name if metadata.name is not None else None\n    basename = metadata.basename if metadata.basename is not None else None\n    finetune = metadata.finetune if metadata.finetune is not None else None\n    version = metadata.version if metadata.version is not None else None\n    size_label = metadata.size_label if metadata.size_label is not None else gguf.size_label(*model_params_count, expert_count=expert_count or 0)\n    output_type = {\n        GGMLFileType.AllF32:    \"F32\",\n        GGMLFileType.MostlyF16: \"F16\",\n        GGMLFileType.MostlyQ8_0: \"Q8_0\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "default_outfile",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def default_outfile(model_paths: list[Path], file_type: GGMLFileType, expert_count: int | None, model_params_count: tuple[int, int, int], metadata: gguf.Metadata) -> Path:\n    default_filename = default_convention_outfile(file_type, expert_count, model_params_count, metadata)\n    ret = model_paths[0].parent / f\"{default_filename}.gguf\"\n    if ret in model_paths:\n        logger.error(\n            f\"Error: Default output path ({ret}) would overwrite the input. \"\n            \"Please explicitly specify a path using --outfile.\")\n        sys.exit(1)\n    return ret\ndef do_dump_model(model_plus: ModelPlus) -> None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "do_dump_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def do_dump_model(model_plus: ModelPlus) -> None:\n    print(f\"model_plus.paths = {model_plus.paths!r}\") # noqa: NP100\n    print(f\"model_plus.format = {model_plus.format!r}\") # noqa: NP100\n    print(f\"model_plus.vocab = {model_plus.vocab!r}\") # noqa: NP100\n    for name, lazy_tensor in model_plus.model.items():\n        print(f\"{name}: shape={lazy_tensor.shape} type={lazy_tensor.data_type}; {lazy_tensor.description}\") # noqa: NP100\ndef main(args_in: list[str] | None = None) -> None:\n    output_choices = [\"f32\", \"f16\"]\n    if np.uint32(1) == np.uint32(1).newbyteorder(\"<\"):\n        # We currently only support Q8_0 output on little endian systems.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "def main(args_in: list[str] | None = None) -> None:\n    output_choices = [\"f32\", \"f16\"]\n    if np.uint32(1) == np.uint32(1).newbyteorder(\"<\"):\n        # We currently only support Q8_0 output on little endian systems.\n        output_choices.append(\"q8_0\")\n    parser = argparse.ArgumentParser(description=\"Convert a LLaMA model to a GGML compatible file\")\n    parser.add_argument(\"--dump\",         action=\"store_true\",    help=\"don't convert, just show what's in the model\")\n    parser.add_argument(\"--dump-single\",  action=\"store_true\",    help=\"don't convert, just show what's in a single model file\")\n    parser.add_argument(\"--vocab-only\",   action=\"store_true\",    help=\"extract only the vocab\")\n    parser.add_argument(\"--no-vocab\",     action=\"store_true\",    help=\"store model without the vocab\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "logger = logging.getLogger(\"convert\")\nif hasattr(faulthandler, 'register') and hasattr(signal, 'SIGUSR1'):\n    faulthandler.register(signal.SIGUSR1)\nNDArray: TypeAlias = 'np.ndarray[Any, Any]'\nARCH = gguf.MODEL_ARCH.LLAMA\nDEFAULT_CONCURRENCY = 8\nADDED_TOKENS_FILE = 'added_tokens.json'\nFAST_TOKENIZER_FILE = 'tokenizer.json'\n#\n# data types",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "ARCH",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "ARCH = gguf.MODEL_ARCH.LLAMA\nDEFAULT_CONCURRENCY = 8\nADDED_TOKENS_FILE = 'added_tokens.json'\nFAST_TOKENIZER_FILE = 'tokenizer.json'\n#\n# data types\n#\n@dataclass(frozen=True)\nclass DataType:\n    name: str",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONCURRENCY",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "DEFAULT_CONCURRENCY = 8\nADDED_TOKENS_FILE = 'added_tokens.json'\nFAST_TOKENIZER_FILE = 'tokenizer.json'\n#\n# data types\n#\n@dataclass(frozen=True)\nclass DataType:\n    name: str\n    dtype: np.dtype[Any]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "ADDED_TOKENS_FILE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "ADDED_TOKENS_FILE = 'added_tokens.json'\nFAST_TOKENIZER_FILE = 'tokenizer.json'\n#\n# data types\n#\n@dataclass(frozen=True)\nclass DataType:\n    name: str\n    dtype: np.dtype[Any]\n    valid_conversions: list[str]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "FAST_TOKENIZER_FILE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "FAST_TOKENIZER_FILE = 'tokenizer.json'\n#\n# data types\n#\n@dataclass(frozen=True)\nclass DataType:\n    name: str\n    dtype: np.dtype[Any]\n    valid_conversions: list[str]\n    def elements_to_bytes(self, n_elements: int) -> int:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "DT_BF16",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "DT_BF16 = UnquantizedDataType('BF16', dtype = np.dtype(np.uint16),  valid_conversions = ['F32', 'F16', 'Q8_0'])\n@dataclass(frozen=True)\nclass QuantizedDataType(DataType):\n    block_size: int\n    quantized_dtype: np.dtype[Any]\n    ggml_type: gguf.GGMLQuantizationType\n    def quantize(self, arr: NDArray) -> NDArray:\n        raise NotImplementedError(f'Quantization for {self.name} not implemented')\n    def elements_to_bytes(self, n_elements: int) -> int:\n        assert n_elements % self.block_size == 0, f'Invalid number of elements {n_elements} for {self.name} with block size {self.block_size}'",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "DT_Q8_0",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "DT_Q8_0 = Q8_0QuantizedDataType('Q8_0',\n                                dtype = np.dtype(np.float32), valid_conversions = [],\n                                ggml_type = gguf.GGMLQuantizationType.Q8_0, block_size = 32,\n                                quantized_dtype = np.dtype([('d', '<f2'), ('qs', 'i1', (32,))]))\n# Quantized types skipped here because they may also map to np.float32\nNUMPY_TYPE_TO_DATA_TYPE: dict[np.dtype[Any], DataType] = {}\nfor dt in (DT_BF16, DT_F16, DT_F32, DT_I32):\n    if dt.dtype in NUMPY_TYPE_TO_DATA_TYPE:\n        raise ValueError(f'Invalid duplicate data type {dt}')\n    NUMPY_TYPE_TO_DATA_TYPE[dt.dtype] = dt",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "GGMLCompatibleTensor",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "GGMLCompatibleTensor = UnquantizedTensor\n@dataclass\nclass LazyTensor:\n    _load: Callable[[], Tensor]\n    shape: list[int]\n    data_type: DataType\n    description: str\n    def load(self) -> Tensor:\n        ret = self._load()\n        # Should be okay if it maps to the same numpy type?",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "In",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "In = TypeVar('In')\nOut = TypeVar('Out')\ndef bounded_parallel_map(func: Callable[[In], Out], iterable: Iterable[In], concurrency: int, max_workers: int | None = None, use_processpool_executor: bool = False) -> Iterable[Out]:\n    '''Parallel map, but with backpressure.  If the caller doesn't call `next`\n    fast enough, this will stop calling `func` at some point rather than\n    letting results pile up in memory.  Specifically, there is a max of one\n    output value buffered per thread.'''\n    if concurrency < 2:\n        yield from map(func, iterable)\n        # Not reached.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "Out",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "peekOfCode": "Out = TypeVar('Out')\ndef bounded_parallel_map(func: Callable[[In], Out], iterable: Iterable[In], concurrency: int, max_workers: int | None = None, use_processpool_executor: bool = False) -> Iterable[Out]:\n    '''Parallel map, but with backpressure.  If the caller doesn't call `next`\n    fast enough, this will stop calling `func` at some point rather than\n    letting results pile up in memory.  Specifically, there is a max of one\n    output value buffered per thread.'''\n    if concurrency < 2:\n        yield from map(func, iterable)\n        # Not reached.\n    iterable = iter(iterable)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.convert_legacy_llama",
        "documentation": {}
    },
    {
        "label": "BuiltinRule",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "class BuiltinRule:\n    def __init__(self, content: str, deps: list | None = None):\n        self.content = content\n        self.deps = deps or []\n# Constraining spaces to prevent model \"running away\".\nSPACE_RULE = '| \" \" | \"\\\\n\"{1,2} [ \\\\t]{0,20}'\nPRIMITIVE_RULES = {\n    'boolean'      : BuiltinRule('(\"true\" | \"false\") space', []),\n    'decimal-part' : BuiltinRule('[0-9]{1,16}', []),\n    'integral-part': BuiltinRule('[0] | [1-9] [0-9]{0,15}', []),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "SchemaConverter",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "class SchemaConverter:\n    def __init__(self, *, prop_order, allow_fetch, dotall, raw_pattern):\n        self._prop_order = prop_order\n        self._allow_fetch = allow_fetch\n        self._dotall = dotall\n        self._raw_pattern = raw_pattern\n        self._rules = {\n            'space': SPACE_RULE,\n        }\n        self._refs = {}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "def main(args_in = None):\n    parser = argparse.ArgumentParser(\n        description='''\n            Generates a grammar (suitable for use in ./llama-cli) that produces JSON conforming to a\n            given JSON schema. Only a subset of JSON schema features are supported; more may be\n            added in the future.\n        ''',\n    )\n    parser.add_argument(\n        '--prop-order',",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "SPACE_RULE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "SPACE_RULE = '| \" \" | \"\\\\n\"{1,2} [ \\\\t]{0,20}'\nPRIMITIVE_RULES = {\n    'boolean'      : BuiltinRule('(\"true\" | \"false\") space', []),\n    'decimal-part' : BuiltinRule('[0-9]{1,16}', []),\n    'integral-part': BuiltinRule('[0] | [1-9] [0-9]{0,15}', []),\n    'number'       : BuiltinRule('(\"-\"? integral-part) (\".\" decimal-part)? ([eE] [-+]? integral-part)? space', ['integral-part', 'decimal-part']),\n    'integer'      : BuiltinRule('(\"-\"? integral-part) space', ['integral-part']),\n    'value'        : BuiltinRule('object | array | string | number | boolean | null', ['object', 'array', 'string', 'number', 'boolean', 'null']),\n    'object'       : BuiltinRule('\"{\" space ( string \":\" space value (\",\" space string \":\" space value)* )? \"}\" space', ['string', 'value']),\n    'array'        : BuiltinRule('\"[\" space ( value (\",\" space value)* )? \"]\" space', ['value']),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "PRIMITIVE_RULES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "PRIMITIVE_RULES = {\n    'boolean'      : BuiltinRule('(\"true\" | \"false\") space', []),\n    'decimal-part' : BuiltinRule('[0-9]{1,16}', []),\n    'integral-part': BuiltinRule('[0] | [1-9] [0-9]{0,15}', []),\n    'number'       : BuiltinRule('(\"-\"? integral-part) (\".\" decimal-part)? ([eE] [-+]? integral-part)? space', ['integral-part', 'decimal-part']),\n    'integer'      : BuiltinRule('(\"-\"? integral-part) space', ['integral-part']),\n    'value'        : BuiltinRule('object | array | string | number | boolean | null', ['object', 'array', 'string', 'number', 'boolean', 'null']),\n    'object'       : BuiltinRule('\"{\" space ( string \":\" space value (\",\" space string \":\" space value)* )? \"}\" space', ['string', 'value']),\n    'array'        : BuiltinRule('\"[\" space ( value (\",\" space value)* )? \"]\" space', ['value']),\n    'uuid'         : BuiltinRule(r'\"\\\"\" [0-9a-fA-F]{8} \"-\" [0-9a-fA-F]{4} \"-\" [0-9a-fA-F]{4} \"-\" [0-9a-fA-F]{4} \"-\" [0-9a-fA-F]{12} \"\\\"\" space', []),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "STRING_FORMAT_RULES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "STRING_FORMAT_RULES = {\n    'date'            : BuiltinRule('[0-9]{4} \"-\" ( \"0\" [1-9] | \"1\" [0-2] ) \"-\" ( \\\"0\\\" [1-9] | [1-2] [0-9] | \"3\" [0-1] )', []),\n    'time'            : BuiltinRule('([01] [0-9] | \"2\" [0-3]) \":\" [0-5] [0-9] \":\" [0-5] [0-9] ( \".\" [0-9]{3} )? ( \"Z\" | ( \"+\" | \"-\" ) ( [01] [0-9] | \"2\" [0-3] ) \":\" [0-5] [0-9] )', []),\n    'date-time'       : BuiltinRule('date \"T\" time', ['date', 'time']),\n    'date-string'     : BuiltinRule('\"\\\\\"\" date \"\\\\\"\" space', ['date']),\n    'time-string'     : BuiltinRule('\"\\\\\"\" time \"\\\\\"\" space', ['time']),\n    'date-time-string': BuiltinRule('\"\\\\\"\" date-time \"\\\\\"\" space', ['date-time']),\n}\nDOTALL = '[\\\\U00000000-\\\\U0010FFFF]'\nDOT = '[^\\\\x0A\\\\x0D]'",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "DOTALL",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "DOTALL = '[\\\\U00000000-\\\\U0010FFFF]'\nDOT = '[^\\\\x0A\\\\x0D]'\nRESERVED_NAMES = set([\"root\", \"dot\", *PRIMITIVE_RULES.keys(), *STRING_FORMAT_RULES.keys()])\nINVALID_RULE_CHARS_RE = re.compile(r'[^a-zA-Z0-9-]+')\nGRAMMAR_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\\\]')\nGRAMMAR_RANGE_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\]\\-\\\\]')\nGRAMMAR_LITERAL_ESCAPES = {'\\r': '\\\\r', '\\n': '\\\\n', '\"': '\\\\\"', '-': '\\\\-', ']': '\\\\]', '\\\\': '\\\\\\\\'}\nNON_LITERAL_SET = set('|.()[]{}*+?')\nESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS = set('^$.[]()|{}*+?')\nclass SchemaConverter:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "DOT",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "DOT = '[^\\\\x0A\\\\x0D]'\nRESERVED_NAMES = set([\"root\", \"dot\", *PRIMITIVE_RULES.keys(), *STRING_FORMAT_RULES.keys()])\nINVALID_RULE_CHARS_RE = re.compile(r'[^a-zA-Z0-9-]+')\nGRAMMAR_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\\\]')\nGRAMMAR_RANGE_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\]\\-\\\\]')\nGRAMMAR_LITERAL_ESCAPES = {'\\r': '\\\\r', '\\n': '\\\\n', '\"': '\\\\\"', '-': '\\\\-', ']': '\\\\]', '\\\\': '\\\\\\\\'}\nNON_LITERAL_SET = set('|.()[]{}*+?')\nESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS = set('^$.[]()|{}*+?')\nclass SchemaConverter:\n    def __init__(self, *, prop_order, allow_fetch, dotall, raw_pattern):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "RESERVED_NAMES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "RESERVED_NAMES = set([\"root\", \"dot\", *PRIMITIVE_RULES.keys(), *STRING_FORMAT_RULES.keys()])\nINVALID_RULE_CHARS_RE = re.compile(r'[^a-zA-Z0-9-]+')\nGRAMMAR_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\\\]')\nGRAMMAR_RANGE_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\]\\-\\\\]')\nGRAMMAR_LITERAL_ESCAPES = {'\\r': '\\\\r', '\\n': '\\\\n', '\"': '\\\\\"', '-': '\\\\-', ']': '\\\\]', '\\\\': '\\\\\\\\'}\nNON_LITERAL_SET = set('|.()[]{}*+?')\nESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS = set('^$.[]()|{}*+?')\nclass SchemaConverter:\n    def __init__(self, *, prop_order, allow_fetch, dotall, raw_pattern):\n        self._prop_order = prop_order",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "INVALID_RULE_CHARS_RE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "INVALID_RULE_CHARS_RE = re.compile(r'[^a-zA-Z0-9-]+')\nGRAMMAR_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\\\]')\nGRAMMAR_RANGE_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\]\\-\\\\]')\nGRAMMAR_LITERAL_ESCAPES = {'\\r': '\\\\r', '\\n': '\\\\n', '\"': '\\\\\"', '-': '\\\\-', ']': '\\\\]', '\\\\': '\\\\\\\\'}\nNON_LITERAL_SET = set('|.()[]{}*+?')\nESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS = set('^$.[]()|{}*+?')\nclass SchemaConverter:\n    def __init__(self, *, prop_order, allow_fetch, dotall, raw_pattern):\n        self._prop_order = prop_order\n        self._allow_fetch = allow_fetch",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "GRAMMAR_LITERAL_ESCAPE_RE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "GRAMMAR_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\\\]')\nGRAMMAR_RANGE_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\]\\-\\\\]')\nGRAMMAR_LITERAL_ESCAPES = {'\\r': '\\\\r', '\\n': '\\\\n', '\"': '\\\\\"', '-': '\\\\-', ']': '\\\\]', '\\\\': '\\\\\\\\'}\nNON_LITERAL_SET = set('|.()[]{}*+?')\nESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS = set('^$.[]()|{}*+?')\nclass SchemaConverter:\n    def __init__(self, *, prop_order, allow_fetch, dotall, raw_pattern):\n        self._prop_order = prop_order\n        self._allow_fetch = allow_fetch\n        self._dotall = dotall",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "GRAMMAR_RANGE_LITERAL_ESCAPE_RE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "GRAMMAR_RANGE_LITERAL_ESCAPE_RE = re.compile(r'[\\r\\n\"\\]\\-\\\\]')\nGRAMMAR_LITERAL_ESCAPES = {'\\r': '\\\\r', '\\n': '\\\\n', '\"': '\\\\\"', '-': '\\\\-', ']': '\\\\]', '\\\\': '\\\\\\\\'}\nNON_LITERAL_SET = set('|.()[]{}*+?')\nESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS = set('^$.[]()|{}*+?')\nclass SchemaConverter:\n    def __init__(self, *, prop_order, allow_fetch, dotall, raw_pattern):\n        self._prop_order = prop_order\n        self._allow_fetch = allow_fetch\n        self._dotall = dotall\n        self._raw_pattern = raw_pattern",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "GRAMMAR_LITERAL_ESCAPES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "GRAMMAR_LITERAL_ESCAPES = {'\\r': '\\\\r', '\\n': '\\\\n', '\"': '\\\\\"', '-': '\\\\-', ']': '\\\\]', '\\\\': '\\\\\\\\'}\nNON_LITERAL_SET = set('|.()[]{}*+?')\nESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS = set('^$.[]()|{}*+?')\nclass SchemaConverter:\n    def __init__(self, *, prop_order, allow_fetch, dotall, raw_pattern):\n        self._prop_order = prop_order\n        self._allow_fetch = allow_fetch\n        self._dotall = dotall\n        self._raw_pattern = raw_pattern\n        self._rules = {",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "NON_LITERAL_SET",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "NON_LITERAL_SET = set('|.()[]{}*+?')\nESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS = set('^$.[]()|{}*+?')\nclass SchemaConverter:\n    def __init__(self, *, prop_order, allow_fetch, dotall, raw_pattern):\n        self._prop_order = prop_order\n        self._allow_fetch = allow_fetch\n        self._dotall = dotall\n        self._raw_pattern = raw_pattern\n        self._rules = {\n            'space': SPACE_RULE,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "ESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "peekOfCode": "ESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS = set('^$.[]()|{}*+?')\nclass SchemaConverter:\n    def __init__(self, *, prop_order, allow_fetch, dotall, raw_pattern):\n        self._prop_order = prop_order\n        self._allow_fetch = allow_fetch\n        self._dotall = dotall\n        self._raw_pattern = raw_pattern\n        self._rules = {\n            'space': SPACE_RULE,\n        }",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.json_schema_to_grammar",
        "documentation": {}
    },
    {
        "label": "PydanticDataType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "class PydanticDataType(Enum):\n    \"\"\"\n    Defines the data types supported by the grammar_generator.\n    Attributes:\n        STRING (str): Represents a string data type.\n        BOOLEAN (str): Represents a boolean data type.\n        INTEGER (str): Represents an integer data type.\n        FLOAT (str): Represents a float data type.\n        OBJECT (str): Represents an object data type.\n        ARRAY (str): Represents an array data type.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "map_pydantic_type_to_gbnf",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def map_pydantic_type_to_gbnf(pydantic_type: type[Any]) -> str:\n    origin_type = get_origin(pydantic_type)\n    origin_type = pydantic_type if origin_type is None else origin_type\n    if isclass(origin_type) and issubclass(origin_type, str):\n        return PydanticDataType.STRING.value\n    elif isclass(origin_type) and issubclass(origin_type, bool):\n        return PydanticDataType.BOOLEAN.value\n    elif isclass(origin_type) and issubclass(origin_type, int):\n        return PydanticDataType.INTEGER.value\n    elif isclass(origin_type) and issubclass(origin_type, float):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "format_model_and_field_name",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def format_model_and_field_name(model_name: str) -> str:\n    parts = re.findall(\"[A-Z][^A-Z]*\", model_name)\n    if not parts:  # Check if the list is empty\n        return model_name.lower().replace(\"_\", \"-\")\n    return \"-\".join(part.lower().replace(\"_\", \"-\") for part in parts)\ndef generate_list_rule(element_type):\n    \"\"\"\n    Generate a GBNF rule for a list of a given element type.\n    :param element_type: The type of the elements in the list (e.g., 'string').\n    :return: A string representing the GBNF rule for a list of the given type.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_list_rule",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_list_rule(element_type):\n    \"\"\"\n    Generate a GBNF rule for a list of a given element type.\n    :param element_type: The type of the elements in the list (e.g., 'string').\n    :return: A string representing the GBNF rule for a list of the given type.\n    \"\"\"\n    rule_name = f\"{map_pydantic_type_to_gbnf(element_type)}-list\"\n    element_rule = map_pydantic_type_to_gbnf(element_type)\n    list_rule = rf'{rule_name} ::= \"[\"  {element_rule} (\",\"  {element_rule})* \"]\"'\n    return list_rule",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "get_members_structure",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def get_members_structure(cls, rule_name):\n    if issubclass(cls, Enum):\n        # Handle Enum types\n        members = [f'\"\\\\\"{member.value}\\\\\"\"' for name, member in cls.__members__.items()]\n        return f\"{cls.__name__.lower()} ::= \" + \" | \".join(members)\n    if cls.__annotations__ and cls.__annotations__ != {}:\n        result = f'{rule_name} ::= \"{{\"'\n        # Modify this comprehension\n        members = [\n            f'  \"\\\\\"{name}\\\\\"\" \":\"  {map_pydantic_type_to_gbnf(param_type)}'",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "regex_to_gbnf",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def regex_to_gbnf(regex_pattern: str) -> str:\n    \"\"\"\n    Translate a basic regex pattern to a GBNF rule.\n    Note: This function handles only a subset of simple regex patterns.\n    \"\"\"\n    gbnf_rule = regex_pattern\n    # Translate common regex components to GBNF\n    gbnf_rule = gbnf_rule.replace(\"\\\\d\", \"[0-9]\")\n    gbnf_rule = gbnf_rule.replace(\"\\\\s\", \"[ \\t\\n]\")\n    # Handle quantifiers and other regex syntax that is similar in GBNF",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_gbnf_integer_rules",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_gbnf_integer_rules(max_digit=None, min_digit=None):\n    \"\"\"\n    Generate GBNF Integer Rules\n    Generates GBNF (Generalized Backus-Naur Form) rules for integers based on the given maximum and minimum digits.\n    Parameters:\n        max_digit (int): The maximum number of digits for the integer. Default is None.\n        min_digit (int): The minimum number of digits for the integer. Default is None.\n    Returns:\n        integer_rule (str): The identifier for the integer rule generated.\n        additional_rules (list): A list of additional rules generated based on the given maximum and minimum digits.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_gbnf_float_rules",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_gbnf_float_rules(max_digit=None, min_digit=None, max_precision=None, min_precision=None):\n    \"\"\"\n    Generate GBNF float rules based on the given constraints.\n    :param max_digit: Maximum number of digits in the integer part (default: None)\n    :param min_digit: Minimum number of digits in the integer part (default: None)\n    :param max_precision: Maximum number of digits in the fractional part (default: None)\n    :param min_precision: Minimum number of digits in the fractional part (default: None)\n    :return: A tuple containing the float rule and additional rules as a list\n    Example Usage:\n    max_digit = 3",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_gbnf_rule_for_type",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_gbnf_rule_for_type(\n    model_name, field_name, field_type, is_optional, processed_models, created_rules, field_info=None\n) -> tuple[str, list[str]]:\n    \"\"\"\n    Generate GBNF rule for a given field type.\n    :param model_name: Name of the model.\n    :param field_name: Name of the field.\n    :param field_type: Type of the field.\n    :param is_optional: Whether the field is optional.\n    :param processed_models: List of processed models.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_gbnf_grammar",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_gbnf_grammar(model: type[BaseModel], processed_models: set[type[BaseModel]], created_rules: dict[str, list[str]]) -> tuple[list[str], bool]:\n    \"\"\"\n    Generate GBnF Grammar\n    Generates a GBnF grammar for a given model.\n    :param model: A Pydantic model class to generate the grammar for. Must be a subclass of BaseModel.\n    :param processed_models: A set of already processed models to prevent infinite recursion.\n    :param created_rules: A dict containing already created rules to prevent duplicates.\n    :return: A list of GBnF grammar rules in string format. And two booleans indicating if an extra markdown or triple quoted string is in the grammar.\n    Example Usage:\n    ```",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_gbnf_grammar_from_pydantic_models",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_gbnf_grammar_from_pydantic_models(\n    models: list[type[BaseModel]], outer_object_name: str | None = None, outer_object_content: str | None = None,\n    list_of_outputs: bool = False\n) -> str:\n    \"\"\"\n    Generate GBNF Grammar from Pydantic Models.\n    This method takes a list of Pydantic models and uses them to generate a GBNF grammar string. The generated grammar string can be used for parsing and validating data using the generated\n    * grammar.\n    Args:\n        models (list[type[BaseModel]]): A list of Pydantic models to generate the grammar from.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "get_primitive_grammar",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def get_primitive_grammar(grammar):\n    \"\"\"\n    Returns the needed GBNF primitive grammar for a given GBNF grammar string.\n    Args:\n        grammar (str): The string containing the GBNF grammar.\n    Returns:\n        str: GBNF primitive grammar string.\n    \"\"\"\n    type_list: list[type[object]] = []\n    if \"string-list\" in grammar:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_markdown_documentation",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_markdown_documentation(\n    pydantic_models: list[type[BaseModel]], model_prefix=\"Model\", fields_prefix=\"Fields\",\n    documentation_with_field_description=True\n) -> str:\n    \"\"\"\n    Generate markdown documentation for a list of Pydantic models.\n    Args:\n        pydantic_models (list[type[BaseModel]]): list of Pydantic model classes.\n        model_prefix (str): Prefix for the model section.\n        fields_prefix (str): Prefix for the fields section.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_field_markdown",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_field_markdown(\n    field_name: str, field_type: type[Any], model: type[BaseModel], depth=1,\n    documentation_with_field_description=True\n) -> str:\n    \"\"\"\n    Generate markdown documentation for a Pydantic model field.\n    Args:\n        field_name (str): Name of the field.\n        field_type (type[Any]): Type of the field.\n        model (type[BaseModel]): Pydantic model class.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "format_json_example",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def format_json_example(example: dict[str, Any], depth: int) -> str:\n    \"\"\"\n    Format a JSON example into a readable string with indentation.\n    Args:\n        example (dict): JSON example to be formatted.\n        depth (int): Indentation depth.\n    Returns:\n        str: Formatted JSON example string.\n    \"\"\"\n    indent = \"    \" * depth",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_text_documentation",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_text_documentation(\n    pydantic_models: list[type[BaseModel]], model_prefix=\"Model\", fields_prefix=\"Fields\",\n    documentation_with_field_description=True\n) -> str:\n    \"\"\"\n    Generate text documentation for a list of Pydantic models.\n    Args:\n        pydantic_models (list[type[BaseModel]]): List of Pydantic model classes.\n        model_prefix (str): Prefix for the model section.\n        fields_prefix (str): Prefix for the fields section.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_field_text",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_field_text(\n    field_name: str, field_type: type[Any], model: type[BaseModel], depth=1,\n    documentation_with_field_description=True\n) -> str:\n    \"\"\"\n    Generate text documentation for a Pydantic model field.\n    Args:\n        field_name (str): Name of the field.\n        field_type (type[Any]): Type of the field.\n        model (type[BaseModel]): Pydantic model class.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "format_multiline_description",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def format_multiline_description(description: str, indent_level: int) -> str:\n    \"\"\"\n    Format a multiline description with proper indentation.\n    Args:\n        description (str): Multiline description.\n        indent_level (int): Indentation level.\n    Returns:\n        str: Formatted multiline description.\n    \"\"\"\n    indent = \"    \" * indent_level",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "save_gbnf_grammar_and_documentation",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def save_gbnf_grammar_and_documentation(\n    grammar, documentation, grammar_file_path=\"./grammar.gbnf\", documentation_file_path=\"./grammar_documentation.md\"\n):\n    \"\"\"\n    Save GBNF grammar and documentation to specified files.\n    Args:\n        grammar (str): GBNF grammar string.\n        documentation (str): Documentation string.\n        grammar_file_path (str): File path to save the GBNF grammar.\n        documentation_file_path (str): File path to save the documentation.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "remove_empty_lines",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def remove_empty_lines(string):\n    \"\"\"\n    Remove empty lines from a string.\n    Args:\n        string (str): Input string.\n    Returns:\n        str: String with empty lines removed.\n    \"\"\"\n    lines = string.splitlines()\n    non_empty_lines = [line for line in lines if line.strip() != \"\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_and_save_gbnf_grammar_and_documentation",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_and_save_gbnf_grammar_and_documentation(\n    pydantic_model_list,\n    grammar_file_path=\"./generated_grammar.gbnf\",\n    documentation_file_path=\"./generated_grammar_documentation.md\",\n    outer_object_name: str | None = None,\n    outer_object_content: str | None = None,\n    model_prefix: str = \"Output Model\",\n    fields_prefix: str = \"Output Fields\",\n    list_of_outputs: bool = False,\n    documentation_with_field_description=True,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_gbnf_grammar_and_documentation",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_gbnf_grammar_and_documentation(\n    pydantic_model_list,\n    outer_object_name: str | None = None,\n    outer_object_content: str | None = None,\n    model_prefix: str = \"Output Model\",\n    fields_prefix: str = \"Output Fields\",\n    list_of_outputs: bool = False,\n    documentation_with_field_description=True,\n):\n    \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "generate_gbnf_grammar_and_documentation_from_dictionaries",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def generate_gbnf_grammar_and_documentation_from_dictionaries(\n    dictionaries: list[dict[str, Any]],\n    outer_object_name: str | None = None,\n    outer_object_content: str | None = None,\n    model_prefix: str = \"Output Model\",\n    fields_prefix: str = \"Output Fields\",\n    list_of_outputs: bool = False,\n    documentation_with_field_description=True,\n):\n    \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "create_dynamic_model_from_function",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def create_dynamic_model_from_function(func: Callable[..., Any]):\n    \"\"\"\n    Creates a dynamic Pydantic model from a given function's type hints and adds the function as a 'run' method.\n    Args:\n        func (Callable): A function with type hints from which to create the model.\n    Returns:\n        A dynamic Pydantic model class with the provided function as a 'run' method.\n    \"\"\"\n    # Get the signature of the function\n    sig = inspect.signature(func)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "add_run_method_to_dynamic_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def add_run_method_to_dynamic_model(model: type[BaseModel], func: Callable[..., Any]):\n    \"\"\"\n    Add a 'run' method to a dynamic Pydantic model, using the provided function.\n    Args:\n        model (type[BaseModel]): Dynamic Pydantic model class.\n        func (Callable): Function to be added as a 'run' method to the model.\n    Returns:\n        type[BaseModel]: Pydantic model class with the added 'run' method.\n    \"\"\"\n    def run_method_wrapper(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "create_dynamic_models_from_dictionaries",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def create_dynamic_models_from_dictionaries(dictionaries: list[dict[str, Any]]):\n    \"\"\"\n    Create a list of dynamic Pydantic model classes from a list of dictionaries.\n    Args:\n        dictionaries (list[dict]): List of dictionaries representing model structures.\n    Returns:\n        list[type[BaseModel]]: List of generated dynamic Pydantic model classes.\n    \"\"\"\n    dynamic_models = []\n    for func in dictionaries:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "map_grammar_names_to_pydantic_model_class",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def map_grammar_names_to_pydantic_model_class(pydantic_model_list):\n    output = {}\n    for model in pydantic_model_list:\n        output[format_model_and_field_name(model.__name__)] = model\n    return output\ndef json_schema_to_python_types(schema):\n    type_map = {\n        \"any\": Any,\n        \"string\": str,\n        \"number\": float,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "json_schema_to_python_types",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def json_schema_to_python_types(schema):\n    type_map = {\n        \"any\": Any,\n        \"string\": str,\n        \"number\": float,\n        \"integer\": int,\n        \"boolean\": bool,\n        \"array\": list,\n    }\n    return type_map[schema]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "list_to_enum",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def list_to_enum(enum_name, values):\n    return Enum(enum_name, {value: value for value in values})\ndef convert_dictionary_to_pydantic_model(dictionary: dict[str, Any], model_name: str = \"CustomModel\") -> type[Any]:\n    \"\"\"\n    Convert a dictionary to a Pydantic model class.\n    Args:\n        dictionary (dict): Dictionary representing the model structure.\n        model_name (str): Name of the generated Pydantic model.\n    Returns:\n        type[BaseModel]: Generated Pydantic model class.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "convert_dictionary_to_pydantic_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "peekOfCode": "def convert_dictionary_to_pydantic_model(dictionary: dict[str, Any], model_name: str = \"CustomModel\") -> type[Any]:\n    \"\"\"\n    Convert a dictionary to a Pydantic model class.\n    Args:\n        dictionary (dict): Dictionary representing the model structure.\n        model_name (str): Name of the generated Pydantic model.\n    Returns:\n        type[BaseModel]: Generated Pydantic model class.\n    \"\"\"\n    fields: dict[str, Any] = {}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar",
        "documentation": {}
    },
    {
        "label": "SendMessageToUser",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "class SendMessageToUser(BaseModel):\n    \"\"\"Send a message to the User.\"\"\"\n    chain_of_thought: str = Field(..., description=\"Your chain of thought while sending the message.\")\n    message: str = Field(..., description=\"Message you want to send to the user.\")\n    def run(self):\n        print(f\"SendMessageToUser: {self.message}\")\ndef example_rce(host):\n    \"\"\"Minimal test case where the LLM call an arbitrary python function.\"\"\"\n    print(\"- example_rce\")\n    tools = [SendMessageToUser]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "MathOperation",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "class MathOperation(Enum):\n    ADD = \"add\"\n    SUBTRACT = \"subtract\"\n    MULTIPLY = \"multiply\"\n    DIVIDE = \"divide\"\n# Simple pydantic calculator tool for the agent that can add, subtract,\n# multiply, and divide. Docstring and description of fields will be used in\n# system prompt.\nclass Calculator(BaseModel):\n    \"\"\"Perform a math operation on two numbers.\"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "Calculator",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "class Calculator(BaseModel):\n    \"\"\"Perform a math operation on two numbers.\"\"\"\n    number_one: Union[int, float] = Field(..., description=\"First number.\")\n    operation: MathOperation = Field(..., description=\"Math operation to perform.\")\n    number_two: Union[int, float] = Field(..., description=\"Second number.\")\n    def run(self):\n        if self.operation == MathOperation.ADD:\n            return self.number_one + self.number_two\n        elif self.operation == MathOperation.SUBTRACT:\n            return self.number_one - self.number_two",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "Category",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "class Category(Enum):\n    \"\"\"The category of the book.\"\"\"\n    Fiction = \"Fiction\"\n    NonFiction = \"Non-Fiction\"\nclass Book(BaseModel):\n    \"\"\"Represents an entry about a book.\"\"\"\n    title: str = Field(..., description=\"Title of the book.\")\n    author: str = Field(..., description=\"Author of the book.\")\n    published_year: Optional[int] = Field(..., description=\"Publishing year of the book.\")\n    keywords: list[str] = Field(..., description=\"A list of keywords.\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "Book",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "class Book(BaseModel):\n    \"\"\"Represents an entry about a book.\"\"\"\n    title: str = Field(..., description=\"Title of the book.\")\n    author: str = Field(..., description=\"Author of the book.\")\n    published_year: Optional[int] = Field(..., description=\"Publishing year of the book.\")\n    keywords: list[str] = Field(..., description=\"A list of keywords.\")\n    category: Category = Field(..., description=\"Category of the book.\")\n    summary: str = Field(..., description=\"Summary of the book.\")\ndef example_struct(host):\n    \"\"\"A example structured output based on pydantic models.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "create_completion",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "def create_completion(host, prompt, gbnf_grammar):\n    \"\"\"Calls the /completion API on llama-server.\n    See\n    https://github.com/ggml-org/llama.cpp/tree/HEAD/tools/server#api-endpoints\n    \"\"\"\n    print(f\"  Request:\\n    Grammar:\\n{textwrap.indent(gbnf_grammar, '      ')}\\n    Prompt:\\n{textwrap.indent(prompt.rstrip(), '      ')}\")\n    headers = {\"Content-Type\": \"application/json\"}\n    data = {\"prompt\": prompt, \"grammar\": gbnf_grammar}\n    result = requests.post(f\"http://{host}/completion\", headers=headers, json=data).json()\n    assert data.get(\"error\") is None, data",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "example_rce",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "def example_rce(host):\n    \"\"\"Minimal test case where the LLM call an arbitrary python function.\"\"\"\n    print(\"- example_rce\")\n    tools = [SendMessageToUser]\n    gbnf_grammar, documentation = generate_gbnf_grammar_and_documentation(\n        pydantic_model_list=tools, outer_object_name=\"function\",\n        outer_object_content=\"function_parameters\", model_prefix=\"Function\", fields_prefix=\"Parameters\")\n    system_message = \"You are an advanced AI, tasked to assist the user by calling functions in JSON format. The following are the available functions and their parameters and types:\\n\\n\" + documentation\n    user_message = \"What is 42 * 42?\"\n    prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "example_calculator",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "def example_calculator(host):\n    \"\"\"Have the LLM ask to get a calculation done.\n    Here the grammar gets generated by passing the available function models to\n    generate_gbnf_grammar_and_documentation function. This also generates a\n    documentation usable by the LLM.\n    pydantic_model_list is the list of pydantic models outer_object_name is an\n    optional name for an outer object around the actual model object. Like a\n    \"function\" object with \"function_parameters\" which contains the actual model\n    object. If None, no outer object will be generated outer_object_content is\n    the name of outer object content.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "example_struct",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "def example_struct(host):\n    \"\"\"A example structured output based on pydantic models.\n    The LLM will create an entry for a Book database out of an unstructured\n    text. We need no additional parameters other than our list of pydantic\n    models.\n    \"\"\"\n    print(\"- example_struct\")\n    tools = [Book]\n    gbnf_grammar, documentation = generate_gbnf_grammar_and_documentation(pydantic_model_list=tools)\n    system_message = \"You are an advanced AI, tasked to create a dataset entry in JSON for a Book. The following is the expected output model:\\n\\n\" + documentation",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "get_current_datetime",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "def get_current_datetime(output_format: Optional[str] = None):\n    \"\"\"Get the current date and time in the given format.\n    Args:\n         output_format: formatting string for the date and time, defaults to '%Y-%m-%d %H:%M:%S'\n    \"\"\"\n    return datetime.datetime.now().strftime(output_format or \"%Y-%m-%d %H:%M:%S\")\n# Example function to get the weather.\ndef get_current_weather(location, unit):\n    \"\"\"Get the current weather in a given location\"\"\"\n    if \"London\" in location:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "get_current_weather",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "def get_current_weather(location, unit):\n    \"\"\"Get the current weather in a given location\"\"\"\n    if \"London\" in location:\n        return json.dumps({\"location\": \"London\", \"temperature\": \"42\", \"unit\": unit.value})\n    elif \"New York\" in location:\n        return json.dumps({\"location\": \"New York\", \"temperature\": \"24\", \"unit\": unit.value})\n    elif \"North Pole\" in location:\n        return json.dumps({\"location\": \"North Pole\", \"temperature\": \"-42\", \"unit\": unit.value})\n    return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\ndef example_concurrent(host):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "example_concurrent",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "def example_concurrent(host):\n    \"\"\"An example for parallel function calling with a Python function, a pydantic\n    function model and an OpenAI like function definition.\n    \"\"\"\n    print(\"- example_concurrent\")\n    # Function definition in OpenAI style.\n    current_weather_tool = {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=sys.modules[__name__].__doc__)\n    parser.add_argument(\"--host\", default=\"localhost:8080\", help=\"llama.cpp server\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"enables logging\")\n    args = parser.parse_args()\n    logging.basicConfig(level=logging.INFO if args.verbose else logging.ERROR)\n    ret = 0\n    # Comment out below to only run the example you want.\n    ret = ret or example_rce(args.host)\n    ret = ret or example_calculator(args.host)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.pydantic_models_to_grammar_examples",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.server_embd",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.server_embd",
        "peekOfCode": "n = 8\nresult = []\nasync def requests_post_async(*args, **kwargs):\n    return await asyncio.threads.to_thread(requests.post, *args, **kwargs)\nasync def main():\n    model_url = \"http://127.0.0.1:6900\"\n    responses: list[requests.Response] = await asyncio.gather(*[requests_post_async(\n        url= f\"{model_url}/embedding\",\n        json= {\"content\": \"a \"*1022}\n    ) for i in range(n)])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.server_embd",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.server_embd",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.server_embd",
        "peekOfCode": "result = []\nasync def requests_post_async(*args, **kwargs):\n    return await asyncio.threads.to_thread(requests.post, *args, **kwargs)\nasync def main():\n    model_url = \"http://127.0.0.1:6900\"\n    responses: list[requests.Response] = await asyncio.gather(*[requests_post_async(\n        url= f\"{model_url}/embedding\",\n        json= {\"content\": \"a \"*1022}\n    ) for i in range(n)])\n    for response in responses:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.examples.server_embd",
        "documentation": {}
    },
    {
        "label": "get_short_name",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "peekOfCode": "def get_short_name(long_quant_name):\n    return long_quant_name.replace(\"GGML_TYPE_\", \"\").lower()\nfor filename in glob(\"*.cu\"):\n    os.remove(filename)\nfor head_size_kq in HEAD_SIZES_KQ:\n    head_size_v = head_size_kq if head_size_kq != 576 else 512\n    with open(f\"fattn-tile-instance-dkq{head_size_kq}-dv{head_size_v}.cu\", \"w\") as f:\n        f.write(SOURCE_FATTN_TILE.format(head_size_kq=head_size_kq, head_size_v=head_size_v))\nfor type_k in TYPES_KV:\n    for type_v in TYPES_KV:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "documentation": {}
    },
    {
        "label": "HEAD_SIZES_KQ",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "peekOfCode": "HEAD_SIZES_KQ = [40, 64, 72, 80, 96, 112, 128, 256, 576]\nTYPES_KV = [\"GGML_TYPE_F16\", \"GGML_TYPE_Q4_0\", \"GGML_TYPE_Q4_1\", \"GGML_TYPE_Q5_0\", \"GGML_TYPE_Q5_1\", \"GGML_TYPE_Q8_0\"]\nSOURCE_FATTN_TILE = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../fattn-tile.cuh\"\nDECL_FATTN_TILE_CASE({head_size_kq}, {head_size_v});\n\"\"\"\nSOURCE_FATTN_VEC = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../fattn-vec.cuh\"\nDECL_FATTN_VEC_CASE( 64, {type_k}, {type_v});\nDECL_FATTN_VEC_CASE(128, {type_k}, {type_v});",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "documentation": {}
    },
    {
        "label": "TYPES_KV",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "peekOfCode": "TYPES_KV = [\"GGML_TYPE_F16\", \"GGML_TYPE_Q4_0\", \"GGML_TYPE_Q4_1\", \"GGML_TYPE_Q5_0\", \"GGML_TYPE_Q5_1\", \"GGML_TYPE_Q8_0\"]\nSOURCE_FATTN_TILE = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../fattn-tile.cuh\"\nDECL_FATTN_TILE_CASE({head_size_kq}, {head_size_v});\n\"\"\"\nSOURCE_FATTN_VEC = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../fattn-vec.cuh\"\nDECL_FATTN_VEC_CASE( 64, {type_k}, {type_v});\nDECL_FATTN_VEC_CASE(128, {type_k}, {type_v});\nDECL_FATTN_VEC_CASE(256, {type_k}, {type_v});",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "documentation": {}
    },
    {
        "label": "SOURCE_FATTN_TILE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "peekOfCode": "SOURCE_FATTN_TILE = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../fattn-tile.cuh\"\nDECL_FATTN_TILE_CASE({head_size_kq}, {head_size_v});\n\"\"\"\nSOURCE_FATTN_VEC = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../fattn-vec.cuh\"\nDECL_FATTN_VEC_CASE( 64, {type_k}, {type_v});\nDECL_FATTN_VEC_CASE(128, {type_k}, {type_v});\nDECL_FATTN_VEC_CASE(256, {type_k}, {type_v});\n\"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "documentation": {}
    },
    {
        "label": "SOURCE_FATTN_VEC",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "peekOfCode": "SOURCE_FATTN_VEC = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../fattn-vec.cuh\"\nDECL_FATTN_VEC_CASE( 64, {type_k}, {type_v});\nDECL_FATTN_VEC_CASE(128, {type_k}, {type_v});\nDECL_FATTN_VEC_CASE(256, {type_k}, {type_v});\n\"\"\"\nSOURCE_FATTN_MMA_START = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../fattn-mma-f16.cuh\"\n\"\"\"\nSOURCE_FATTN_MMA_CASE = \"DECL_FATTN_MMA_F16_CASE({head_size_kq}, {head_size_v}, {ncols1}, {ncols2});\\n\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "documentation": {}
    },
    {
        "label": "SOURCE_FATTN_MMA_START",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "peekOfCode": "SOURCE_FATTN_MMA_START = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../fattn-mma-f16.cuh\"\n\"\"\"\nSOURCE_FATTN_MMA_CASE = \"DECL_FATTN_MMA_F16_CASE({head_size_kq}, {head_size_v}, {ncols1}, {ncols2});\\n\"\nTYPES_MMQ = [\n    \"GGML_TYPE_Q4_0\", \"GGML_TYPE_Q4_1\", \"GGML_TYPE_Q5_0\", \"GGML_TYPE_Q5_1\", \"GGML_TYPE_Q8_0\",\n    \"GGML_TYPE_Q2_K\", \"GGML_TYPE_Q3_K\", \"GGML_TYPE_Q4_K\", \"GGML_TYPE_Q5_K\", \"GGML_TYPE_Q6_K\",\n    \"GGML_TYPE_IQ2_XXS\", \"GGML_TYPE_IQ2_XS\", \"GGML_TYPE_IQ2_S\", \"GGML_TYPE_IQ3_XXS\", \"GGML_TYPE_IQ3_S\",\n    \"GGML_TYPE_IQ1_S\", \"GGML_TYPE_IQ4_NL\", \"GGML_TYPE_IQ4_XS\", \"GGML_TYPE_MXFP4\"\n]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "documentation": {}
    },
    {
        "label": "SOURCE_FATTN_MMA_CASE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "peekOfCode": "SOURCE_FATTN_MMA_CASE = \"DECL_FATTN_MMA_F16_CASE({head_size_kq}, {head_size_v}, {ncols1}, {ncols2});\\n\"\nTYPES_MMQ = [\n    \"GGML_TYPE_Q4_0\", \"GGML_TYPE_Q4_1\", \"GGML_TYPE_Q5_0\", \"GGML_TYPE_Q5_1\", \"GGML_TYPE_Q8_0\",\n    \"GGML_TYPE_Q2_K\", \"GGML_TYPE_Q3_K\", \"GGML_TYPE_Q4_K\", \"GGML_TYPE_Q5_K\", \"GGML_TYPE_Q6_K\",\n    \"GGML_TYPE_IQ2_XXS\", \"GGML_TYPE_IQ2_XS\", \"GGML_TYPE_IQ2_S\", \"GGML_TYPE_IQ3_XXS\", \"GGML_TYPE_IQ3_S\",\n    \"GGML_TYPE_IQ1_S\", \"GGML_TYPE_IQ4_NL\", \"GGML_TYPE_IQ4_XS\", \"GGML_TYPE_MXFP4\"\n]\nSOURCE_MMQ = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../mmq.cuh\"\nDECL_MMQ_CASE({type});",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "documentation": {}
    },
    {
        "label": "TYPES_MMQ",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "peekOfCode": "TYPES_MMQ = [\n    \"GGML_TYPE_Q4_0\", \"GGML_TYPE_Q4_1\", \"GGML_TYPE_Q5_0\", \"GGML_TYPE_Q5_1\", \"GGML_TYPE_Q8_0\",\n    \"GGML_TYPE_Q2_K\", \"GGML_TYPE_Q3_K\", \"GGML_TYPE_Q4_K\", \"GGML_TYPE_Q5_K\", \"GGML_TYPE_Q6_K\",\n    \"GGML_TYPE_IQ2_XXS\", \"GGML_TYPE_IQ2_XS\", \"GGML_TYPE_IQ2_S\", \"GGML_TYPE_IQ3_XXS\", \"GGML_TYPE_IQ3_S\",\n    \"GGML_TYPE_IQ1_S\", \"GGML_TYPE_IQ4_NL\", \"GGML_TYPE_IQ4_XS\", \"GGML_TYPE_MXFP4\"\n]\nSOURCE_MMQ = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../mmq.cuh\"\nDECL_MMQ_CASE({type});\n\"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "documentation": {}
    },
    {
        "label": "SOURCE_MMQ",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "peekOfCode": "SOURCE_MMQ = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../mmq.cuh\"\nDECL_MMQ_CASE({type});\n\"\"\"\nSOURCE_MMF = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../mmf.cuh\"\nDECL_MMF_CASE({type});\n\"\"\"\ndef get_short_name(long_quant_name):\n    return long_quant_name.replace(\"GGML_TYPE_\", \"\").lower()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "documentation": {}
    },
    {
        "label": "SOURCE_MMF",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "peekOfCode": "SOURCE_MMF = \"\"\"// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n#include \"../mmf.cuh\"\nDECL_MMF_CASE({type});\n\"\"\"\ndef get_short_name(long_quant_name):\n    return long_quant_name.replace(\"GGML_TYPE_\", \"\").lower()\nfor filename in glob(\"*.cu\"):\n    os.remove(filename)\nfor head_size_kq in HEAD_SIZES_KQ:\n    head_size_v = head_size_kq if head_size_kq != 576 else 512",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-cuda.template-instances.generate_cu_files",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-opencl.kernels.embed_kernel",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-opencl.kernels.embed_kernel",
        "peekOfCode": "def main():\n    logging.basicConfig(level=logging.INFO)\n    if len(sys.argv) != 3:\n        logger.info(\"Usage: python embed_kernel.py <input_file> <output_file>\")\n        sys.exit(1)\n    ifile = open(sys.argv[1], \"r\")\n    ofile = open(sys.argv[2], \"w\")\n    for i in ifile:\n        ofile.write('R\"({})\"\\n'.format(i))\n    ifile.close()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-opencl.kernels.embed_kernel",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-opencl.kernels.embed_kernel",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-opencl.kernels.embed_kernel",
        "peekOfCode": "logger = logging.getLogger(\"opencl-embed-kernel\")\ndef main():\n    logging.basicConfig(level=logging.INFO)\n    if len(sys.argv) != 3:\n        logger.info(\"Usage: python embed_kernel.py <input_file> <output_file>\")\n        sys.exit(1)\n    ifile = open(sys.argv[1], \"r\")\n    ofile = open(sys.argv[2], \"w\")\n    for i in ifile:\n        ofile.write('R\"({})\"\\n'.format(i))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-opencl.kernels.embed_kernel",
        "documentation": {}
    },
    {
        "label": "extract_block",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "peekOfCode": "def extract_block(text, name):\n    pattern = rf'#define\\({name}\\)\\s*(.*?)#end\\({name}\\)'\n    match = re.search(pattern, text, re.DOTALL)\n    if not match:\n        raise ValueError(f\"Missing block: {name}\")\n    return match.group(1).strip()\ndef parse_decls(decls_text):\n    decls = {}\n    for name, code in re.findall(r'#decl\\((.*?)\\)\\s*(.*?)#enddecl\\(\\1\\)', decls_text, re.DOTALL):\n        decls[name.strip()] = code.strip()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "documentation": {}
    },
    {
        "label": "parse_decls",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "peekOfCode": "def parse_decls(decls_text):\n    decls = {}\n    for name, code in re.findall(r'#decl\\((.*?)\\)\\s*(.*?)#enddecl\\(\\1\\)', decls_text, re.DOTALL):\n        decls[name.strip()] = code.strip()\n    return decls\ndef replace_repl_placeholders(variant, template_map):\n    for repl, code in variant[\"REPLS\"].items():\n        for key, val in template_map.items():\n            # Match \"key\" and avoid matching subsequences using by using \\b\n            code = re.sub(rf'\\b{re.escape(str(key))}\\b', str(val), code)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "documentation": {}
    },
    {
        "label": "replace_repl_placeholders",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "peekOfCode": "def replace_repl_placeholders(variant, template_map):\n    for repl, code in variant[\"REPLS\"].items():\n        for key, val in template_map.items():\n            # Match \"key\" and avoid matching subsequences using by using \\b\n            code = re.sub(rf'\\b{re.escape(str(key))}\\b', str(val), code)\n        variant[\"REPLS\"][repl] = code\n    return variant\ndef replace_placeholders(shader_text, replacements):\n    for key, val in replacements.items():\n        # Match {{KEY}} literally, where KEY is escaped",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "documentation": {}
    },
    {
        "label": "replace_placeholders",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "peekOfCode": "def replace_placeholders(shader_text, replacements):\n    for key, val in replacements.items():\n        # Match {{KEY}} literally, where KEY is escaped\n        pattern = r'{{\\s*' + re.escape(key) + r'\\s*}}'\n        shader_text = re.sub(pattern, str(val), shader_text)\n    return shader_text\ndef expand_includes(shader, input_dir):\n    \"\"\"\n    Replace #include \"file\" lines in the text with the contents of that file.\n    Searches for files relative to input_dir.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "documentation": {}
    },
    {
        "label": "expand_includes",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "peekOfCode": "def expand_includes(shader, input_dir):\n    \"\"\"\n    Replace #include \"file\" lines in the text with the contents of that file.\n    Searches for files relative to input_dir.\n    \"\"\"\n    include_pattern = re.compile(r'^\\s*#include\\s+\"([^\"]+)\"\\s*$', re.MULTILINE)\n    def replacer(match):\n        fname = match.group(1)\n        file_path = os.path.join(input_dir, fname)\n        if not os.path.exists(file_path):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "documentation": {}
    },
    {
        "label": "write_shader",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "peekOfCode": "def write_shader(shader_name, shader_code, output_dir, outfile):\n    if output_dir:\n        wgsl_filename = os.path.join(output_dir, f\"{shader_name}.wgsl\")\n        with open(wgsl_filename, \"w\", encoding=\"utf-8\") as f_out:\n            f_out.write(shader_code)\n    outfile.write(f'const char* wgsl_{shader_name} = R\"({shader_code})\";\\n\\n')\ndef generate_variants(fname, input_dir, output_dir, outfile):\n    shader_path = os.path.join(input_dir, fname)\n    shader_base_name = fname.split(\".\")[0]\n    with open(shader_path, \"r\", encoding=\"utf-8\") as f:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "documentation": {}
    },
    {
        "label": "generate_variants",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "peekOfCode": "def generate_variants(fname, input_dir, output_dir, outfile):\n    shader_path = os.path.join(input_dir, fname)\n    shader_base_name = fname.split(\".\")[0]\n    with open(shader_path, \"r\", encoding=\"utf-8\") as f:\n        text = f.read()\n    try:\n        variants = ast.literal_eval(extract_block(text, \"VARIANTS\"))\n    except ValueError:\n        write_shader(shader_base_name, text, output_dir, outfile)\n    else:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input_dir\", required=True)\n    parser.add_argument(\"--output_file\", required=True)\n    parser.add_argument(\"--output_dir\")\n    args = parser.parse_args()\n    if args.output_dir:\n        os.makedirs(args.output_dir, exist_ok=True)\n    with open(args.output_file, \"w\", encoding=\"utf-8\") as out:\n        out.write(\"// Auto-generated shader embedding\\n\\n\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.ggml.src.ggml-webgpu.wgsl-shaders.embed_wgsl",
        "documentation": {}
    },
    {
        "label": "read_gguf_file",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.examples.reader",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.examples.reader",
        "peekOfCode": "def read_gguf_file(gguf_file_path):\n    \"\"\"\n    Reads and prints key-value pairs and tensor information from a GGUF file in an improved format.\n    Parameters:\n    - gguf_file_path: Path to the GGUF file.\n    \"\"\"\n    reader = GGUFReader(gguf_file_path)\n    # List all key-value pairs in a columnized format\n    print(\"Key-Value Pairs:\") # noqa: NP100\n    max_key_length = max(len(key) for key in reader.fields.keys())",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.examples.reader",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.examples.reader",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.examples.reader",
        "peekOfCode": "logger = logging.getLogger(\"reader\")\n# Necessary to load the local gguf package\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom gguf.gguf_reader import GGUFReader\ndef read_gguf_file(gguf_file_path):\n    \"\"\"\n    Reads and prints key-value pairs and tensor information from a GGUF file in an improved format.\n    Parameters:\n    - gguf_file_path: Path to the GGUF file.\n    \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.examples.reader",
        "documentation": {}
    },
    {
        "label": "writer_example",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.examples.writer",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.examples.writer",
        "peekOfCode": "def writer_example() -> None:\n    # Example usage with a file\n    gguf_writer = GGUFWriter(\"example.gguf\", \"llama\")\n    gguf_writer.add_block_count(12)\n    gguf_writer.add_uint32(\"answer\", 42)  # Write a 32-bit integer\n    gguf_writer.add_float32(\"answer_in_float\", 42.0)  # Write a 32-bit float\n    gguf_writer.add_custom_alignment(64)\n    tensor1 = np.ones((32,), dtype=np.float32) * 100.0\n    tensor2 = np.ones((64,), dtype=np.float32) * 101.0\n    tensor3 = np.ones((96,), dtype=np.float32) * 102.0",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.examples.writer",
        "documentation": {}
    },
    {
        "label": "byteswap_noop",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "peekOfCode": "def byteswap_noop(tensor, block_offs):\n    # this function is used when byteswapping is not needed\n    pass\ndef byteswap_q4_0(tensor, block_offs):\n    # Each block_q4_0 consists of an f16 delta (scaling factor) followed by 16 int8 quantizations.\n    # Byte-Swap f16 sized delta field\n    delta = tensor.data[block_offs:block_offs + 2].view(dtype=np.uint16)\n    delta.byteswap(inplace=True)\ndef byteswap_q8_0(tensor, block_offs):\n    # Each block_q8_0 consists of an f16 delta (scaling factor) followed by 32 int8 quantizations.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "documentation": {}
    },
    {
        "label": "byteswap_q4_0",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "peekOfCode": "def byteswap_q4_0(tensor, block_offs):\n    # Each block_q4_0 consists of an f16 delta (scaling factor) followed by 16 int8 quantizations.\n    # Byte-Swap f16 sized delta field\n    delta = tensor.data[block_offs:block_offs + 2].view(dtype=np.uint16)\n    delta.byteswap(inplace=True)\ndef byteswap_q8_0(tensor, block_offs):\n    # Each block_q8_0 consists of an f16 delta (scaling factor) followed by 32 int8 quantizations.\n    # Byte-Swap f16 sized delta field\n    delta = tensor.data[block_offs:block_offs + 2].view(dtype=np.uint16)\n    delta.byteswap(inplace=True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "documentation": {}
    },
    {
        "label": "byteswap_q8_0",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "peekOfCode": "def byteswap_q8_0(tensor, block_offs):\n    # Each block_q8_0 consists of an f16 delta (scaling factor) followed by 32 int8 quantizations.\n    # Byte-Swap f16 sized delta field\n    delta = tensor.data[block_offs:block_offs + 2].view(dtype=np.uint16)\n    delta.byteswap(inplace=True)\ndef byteswap_q4_k(tensor, block_offs):\n    # Each block_q4_k consists of 2 f16 values followed by 140 int8 values.\n    # Byte-Swap f16 sized fields\n    delta = tensor.data[block_offs:block_offs + 2].view(dtype=np.uint16)\n    delta.byteswap(inplace=True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "documentation": {}
    },
    {
        "label": "byteswap_q4_k",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "peekOfCode": "def byteswap_q4_k(tensor, block_offs):\n    # Each block_q4_k consists of 2 f16 values followed by 140 int8 values.\n    # Byte-Swap f16 sized fields\n    delta = tensor.data[block_offs:block_offs + 2].view(dtype=np.uint16)\n    delta.byteswap(inplace=True)\n    delta = tensor.data[block_offs + 2:block_offs + 4].view(dtype=np.uint16)\n    delta.byteswap(inplace=True)\ndef byteswap_q6_k(tensor, block_offs):\n    # Each block_q6_k consists of 208 int8 values followed by 1 f16 value.\n    # Byte-Swap f16 sized field",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "documentation": {}
    },
    {
        "label": "byteswap_q6_k",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "peekOfCode": "def byteswap_q6_k(tensor, block_offs):\n    # Each block_q6_k consists of 208 int8 values followed by 1 f16 value.\n    # Byte-Swap f16 sized field\n    delta = tensor.data[block_offs + 208:block_offs + 210].view(dtype=np.uint16)\n    delta.byteswap(inplace=True)\nbyteswap_tensors = {\n    gguf.GGMLQuantizationType.Q4_0:  byteswap_q4_0,\n    gguf.GGMLQuantizationType.Q8_0:  byteswap_q8_0,\n    gguf.GGMLQuantizationType.Q4_K:  byteswap_q4_k,\n    gguf.GGMLQuantizationType.Q6_K:  byteswap_q6_k,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "documentation": {}
    },
    {
        "label": "convert_byteorder",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "peekOfCode": "def convert_byteorder(reader: gguf.GGUFReader, args: argparse.Namespace) -> None:\n    file_endian = reader.endianess.name\n    if reader.byte_order == 'S':\n        host_endian = 'BIG' if file_endian == 'LITTLE' else 'LITTLE'\n    else:\n        host_endian = file_endian\n    order = host_endian if args.order == \"native\" else args.order.upper()\n    logger.info(f\"* Host is {host_endian} endian, GGUF file seems to be {file_endian} endian\")\n    if file_endian == order:\n        logger.info(f\"* File is already {order} endian. Nothing to do.\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "peekOfCode": "def main() -> None:\n    parser = argparse.ArgumentParser(description=\"Convert GGUF file byte order\")\n    parser.add_argument(\n        \"model\", type=str,\n        help=\"GGUF format model filename\",\n    )\n    parser.add_argument(\n        \"order\", type=str, choices=['big', 'little', 'native'],\n        help=\"Requested byte order\",\n    )",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "peekOfCode": "logger = logging.getLogger(\"gguf-convert-endian\")\ndef byteswap_noop(tensor, block_offs):\n    # this function is used when byteswapping is not needed\n    pass\ndef byteswap_q4_0(tensor, block_offs):\n    # Each block_q4_0 consists of an f16 delta (scaling factor) followed by 16 int8 quantizations.\n    # Byte-Swap f16 sized delta field\n    delta = tensor.data[block_offs:block_offs + 2].view(dtype=np.uint16)\n    delta.byteswap(inplace=True)\ndef byteswap_q8_0(tensor, block_offs):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "documentation": {}
    },
    {
        "label": "byteswap_tensors",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "peekOfCode": "byteswap_tensors = {\n    gguf.GGMLQuantizationType.Q4_0:  byteswap_q4_0,\n    gguf.GGMLQuantizationType.Q8_0:  byteswap_q8_0,\n    gguf.GGMLQuantizationType.Q4_K:  byteswap_q4_k,\n    gguf.GGMLQuantizationType.Q6_K:  byteswap_q6_k,\n    gguf.GGMLQuantizationType.MXFP4: byteswap_noop,\n}\ndef convert_byteorder(reader: gguf.GGUFReader, args: argparse.Namespace) -> None:\n    file_endian = reader.endianess.name\n    if reader.byte_order == 'S':",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_convert_endian",
        "documentation": {}
    },
    {
        "label": "get_file_host_endian",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "peekOfCode": "def get_file_host_endian(reader: GGUFReader) -> tuple[str, str]:\n    file_endian = reader.endianess.name\n    if reader.byte_order == 'S':\n        host_endian = 'BIG' if file_endian == 'LITTLE' else 'LITTLE'\n    else:\n        host_endian = file_endian\n    return (host_endian, file_endian)\n# For more information about what field.parts and field.data represent,\n# please see the comments in the modify_gguf.py example.\ndef dump_metadata(reader: GGUFReader, args: argparse.Namespace) -> None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "documentation": {}
    },
    {
        "label": "dump_metadata",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "peekOfCode": "def dump_metadata(reader: GGUFReader, args: argparse.Namespace) -> None:\n    host_endian, file_endian = get_file_host_endian(reader)\n    print(f'* File is {file_endian} endian, script is running on a {host_endian} endian host.')  # noqa: NP100\n    print(f'* Dumping {len(reader.fields)} key/value pair(s)')  # noqa: NP100\n    for n, field in enumerate(reader.fields.values(), 1):\n        if not field.types:\n            pretty_type = 'N/A'\n        elif field.types[0] == GGUFValueType.ARRAY:\n            nest_count = len(field.types) - 1\n            pretty_type = '[' * nest_count + str(field.types[-1].name) + ']' * nest_count",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "documentation": {}
    },
    {
        "label": "dump_metadata_json",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "peekOfCode": "def dump_metadata_json(reader: GGUFReader, args: argparse.Namespace) -> None:\n    import json\n    host_endian, file_endian = get_file_host_endian(reader)\n    metadata: dict[str, Any] = {}\n    tensors: dict[str, Any] = {}\n    result = {\n        \"filename\": args.model,\n        \"endian\": file_endian,\n        \"metadata\": metadata,\n        \"tensors\": tensors,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "documentation": {}
    },
    {
        "label": "markdown_table_with_alignment_support",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "peekOfCode": "def markdown_table_with_alignment_support(header_map: list[dict[str, str]], data: list[dict[str, Any]]):\n    # JSON to Markdown table formatting: https://stackoverflow.com/a/72983854/2850957\n    # Alignment Utility Function\n    def strAlign(padding: int, alignMode: str | None, strVal: str):\n        if alignMode == 'center':\n            return strVal.center(padding)\n        elif alignMode == 'right':\n            return strVal.rjust(padding - 1) + ' '\n        elif alignMode == 'left':\n            return ' ' + strVal.ljust(padding - 1)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "documentation": {}
    },
    {
        "label": "element_count_rounded_notation",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "peekOfCode": "def element_count_rounded_notation(count: int) -> str:\n    if count > 1e15 :\n        # Quadrillion\n        scaled_amount = count * 1e-15\n        scale_suffix = \"Q\"\n    elif count > 1e12 :\n        # Trillions\n        scaled_amount = count * 1e-12\n        scale_suffix = \"T\"\n    elif count > 1e9 :",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "documentation": {}
    },
    {
        "label": "translate_tensor_name",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "peekOfCode": "def translate_tensor_name(name):\n    words = name.split(\".\")\n    # Source: https://github.com/ggml-org/ggml/blob/master/docs/gguf.md#standardized-tensor-names\n    abbreviation_dictionary = {\n        'token_embd': 'Token embedding',\n        'pos_embd': 'Position embedding',\n        'output_norm': 'Output normalization',\n        'output': 'Output',\n        'attn_norm': 'Attention normalization',\n        'attn_norm_2': 'Attention normalization',",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "documentation": {}
    },
    {
        "label": "dump_markdown_metadata",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "peekOfCode": "def dump_markdown_metadata(reader: GGUFReader, args: argparse.Namespace) -> None:\n    host_endian, file_endian = get_file_host_endian(reader)\n    markdown_content = \"\"\n    markdown_content += f'# {args.model} - GGUF Internal File Dump\\n\\n'\n    markdown_content += f'- Endian: {file_endian} endian\\n'\n    markdown_content += '\\n'\n    markdown_content += '## Key Value Metadata Store\\n\\n'\n    markdown_content += f'There are {len(reader.fields)} key-value pairs in this file\\n'\n    markdown_content += '\\n'\n    total_model_bytes = 0",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "peekOfCode": "def main() -> None:\n    parser = argparse.ArgumentParser(description=\"Dump GGUF file metadata\")\n    parser.add_argument(\"model\",           type=str,            help=\"GGUF format model filename\")\n    parser.add_argument(\"--no-tensors\", action=\"store_true\", help=\"Don't dump tensor metadata\")\n    parser.add_argument(\"--json\",       action=\"store_true\", help=\"Produce JSON output\")\n    parser.add_argument(\"--json-array\", action=\"store_true\", help=\"Include full array values in JSON output (long)\")\n    parser.add_argument(\"--data-offset\",    action=\"store_true\", help=\"Start of data offset\")\n    parser.add_argument(\"--data-alignment\", action=\"store_true\", help=\"Data alignment applied globally to data field\")\n    parser.add_argument(\"--markdown\",   action=\"store_true\", help=\"Produce markdown output\")\n    parser.add_argument(\"--verbose\",    action=\"store_true\", help=\"increase output verbosity\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "peekOfCode": "logger = logging.getLogger(\"gguf-dump\")\ndef get_file_host_endian(reader: GGUFReader) -> tuple[str, str]:\n    file_endian = reader.endianess.name\n    if reader.byte_order == 'S':\n        host_endian = 'BIG' if file_endian == 'LITTLE' else 'LITTLE'\n    else:\n        host_endian = file_endian\n    return (host_endian, file_endian)\n# For more information about what field.parts and field.data represent,\n# please see the comments in the modify_gguf.py example.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_dump",
        "documentation": {}
    },
    {
        "label": "TokenizerEditorDialog",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "peekOfCode": "class TokenizerEditorDialog(QDialog):\n    def __init__(self, tokens, token_types, scores, parent=None):\n        super().__init__(parent)\n        self.setWindowTitle(\"Edit Tokenizer Data\")\n        self.resize(900, 600)\n        self.tokens = tokens.copy() if tokens else []\n        self.token_types = token_types.copy() if token_types else []\n        self.scores = scores.copy() if scores else []\n        # Ensure all arrays have the same length\n        max_len = max(len(self.tokens), len(self.token_types), len(self.scores))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "documentation": {}
    },
    {
        "label": "ArrayEditorDialog",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "peekOfCode": "class ArrayEditorDialog(QDialog):\n    def __init__(self, array_values, element_type, key=None, parent=None):\n        super().__init__(parent)\n        self.setWindowTitle(\"Edit Array Values\")\n        self.resize(700, 500)\n        self.array_values = array_values\n        self.element_type = element_type\n        self.key = key\n        # Get enum type for this array if applicable\n        self.enum_type = None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "documentation": {}
    },
    {
        "label": "AddMetadataDialog",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "peekOfCode": "class AddMetadataDialog(QDialog):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setWindowTitle(\"Add Metadata\")\n        self.resize(400, 200)\n        layout = QVBoxLayout(self)\n        form_layout = QFormLayout()\n        self.key_edit = QLineEdit()\n        form_layout.addRow(\"Key:\", self.key_edit)\n        self.type_combo = QComboBox()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "documentation": {}
    },
    {
        "label": "GGUFEditorWindow",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "peekOfCode": "class GGUFEditorWindow(QMainWindow):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"GGUF Editor\")\n        self.resize(1000, 800)\n        self.current_file = None\n        self.reader = None\n        self.modified = False\n        self.metadata_changes = {}  # Store changes to apply when saving\n        self.metadata_to_remove = set()  # Store keys to remove when saving",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "peekOfCode": "def main() -> None:\n    parser = argparse.ArgumentParser(description=\"GUI GGUF Editor\")\n    parser.add_argument(\"model_path\", nargs=\"?\", help=\"path to GGUF model file to load at startup\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"increase output verbosity\")\n    args = parser.parse_args()\n    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)\n    app = QApplication(sys.argv)\n    window = GGUFEditorWindow()\n    window.show()\n    # Load model if specified",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "peekOfCode": "logger = logging.getLogger(\"gguf-editor-gui\")\n# Map of key names to enum types for automatic enum interpretation\nKEY_TO_ENUM_TYPE = {\n    gguf.Keys.Tokenizer.TOKEN_TYPE: TokenType,\n    gguf.Keys.Rope.SCALING_TYPE: RopeScalingType,\n    gguf.Keys.LLM.POOLING_TYPE: PoolingType,\n    gguf.Keys.General.FILE_TYPE: GGMLQuantizationType,\n}\n# Define the tokenizer keys that should be edited together\nTOKENIZER_LINKED_KEYS = [",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "documentation": {}
    },
    {
        "label": "KEY_TO_ENUM_TYPE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "peekOfCode": "KEY_TO_ENUM_TYPE = {\n    gguf.Keys.Tokenizer.TOKEN_TYPE: TokenType,\n    gguf.Keys.Rope.SCALING_TYPE: RopeScalingType,\n    gguf.Keys.LLM.POOLING_TYPE: PoolingType,\n    gguf.Keys.General.FILE_TYPE: GGMLQuantizationType,\n}\n# Define the tokenizer keys that should be edited together\nTOKENIZER_LINKED_KEYS = [\n    gguf.Keys.Tokenizer.LIST,\n    gguf.Keys.Tokenizer.TOKEN_TYPE,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "documentation": {}
    },
    {
        "label": "TOKENIZER_LINKED_KEYS",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "peekOfCode": "TOKENIZER_LINKED_KEYS = [\n    gguf.Keys.Tokenizer.LIST,\n    gguf.Keys.Tokenizer.TOKEN_TYPE,\n    gguf.Keys.Tokenizer.SCORES\n]\nclass TokenizerEditorDialog(QDialog):\n    def __init__(self, tokens, token_types, scores, parent=None):\n        super().__init__(parent)\n        self.setWindowTitle(\"Edit Tokenizer Data\")\n        self.resize(900, 600)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_editor_gui",
        "documentation": {}
    },
    {
        "label": "gguf_hash",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "peekOfCode": "def gguf_hash(reader: GGUFReader, filename: str, disable_progress_bar: bool, no_layer: bool) -> None:\n    sha1 = hashlib.sha1()\n    sha256 = hashlib.sha256()\n    uuidv5_sha1 = hashlib.sha1()\n    uuidv5_sha1.update(UUID_NAMESPACE_LLAMA_CPP.bytes)\n    # Total Weight Calculation For Progress Bar\n    total_weights = 0\n    for n, tensor in enumerate(reader.tensors, 1):\n        # We don't need these\n        if tensor.name.endswith((\".attention.masked_bias\", \".attention.bias\", \".rotary_emb.inv_freq\")):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "peekOfCode": "def main() -> None:\n    parser = argparse.ArgumentParser(description=\"Dump GGUF file metadata\")\n    parser.add_argument(\"model\",         type=str,            help=\"GGUF format model filename\")\n    parser.add_argument(\"--no-layer\",    action=\"store_true\", help=\"exclude per layer hash\")\n    parser.add_argument(\"--verbose\",     action=\"store_true\", help=\"increase output verbosity\")\n    parser.add_argument(\"--progressbar\", action=\"store_true\", help=\"enable progressbar\")\n    args = parser.parse_args(None if len(sys.argv) > 1 else [\"--help\"])\n    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)\n    reader = GGUFReader(args.model, 'r')\n    gguf_hash(reader, args.model, not args.progressbar, args.no_layer)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "peekOfCode": "logger = logging.getLogger(\"gguf-hash\")\n# UUID_NAMESPACE_LLAMA_CPP = uuid.uuid5(uuid.NAMESPACE_URL, 'en.wikipedia.org/wiki/Llama.cpp')\nUUID_NAMESPACE_LLAMA_CPP = uuid.UUID('ef001206-dadc-5f6d-a15f-3359e577d4e5')\n# For more information about what field.parts and field.data represent,\n# please see the comments in the modify_gguf.py example.\ndef gguf_hash(reader: GGUFReader, filename: str, disable_progress_bar: bool, no_layer: bool) -> None:\n    sha1 = hashlib.sha1()\n    sha256 = hashlib.sha256()\n    uuidv5_sha1 = hashlib.sha1()\n    uuidv5_sha1.update(UUID_NAMESPACE_LLAMA_CPP.bytes)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "documentation": {}
    },
    {
        "label": "UUID_NAMESPACE_LLAMA_CPP",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "peekOfCode": "UUID_NAMESPACE_LLAMA_CPP = uuid.UUID('ef001206-dadc-5f6d-a15f-3359e577d4e5')\n# For more information about what field.parts and field.data represent,\n# please see the comments in the modify_gguf.py example.\ndef gguf_hash(reader: GGUFReader, filename: str, disable_progress_bar: bool, no_layer: bool) -> None:\n    sha1 = hashlib.sha1()\n    sha256 = hashlib.sha256()\n    uuidv5_sha1 = hashlib.sha1()\n    uuidv5_sha1.update(UUID_NAMESPACE_LLAMA_CPP.bytes)\n    # Total Weight Calculation For Progress Bar\n    total_weights = 0",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_hash",
        "documentation": {}
    },
    {
        "label": "MetadataDetails",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "peekOfCode": "class MetadataDetails(NamedTuple):\n    type: gguf.GGUFValueType\n    value: Any\n    description: str = ''\n    sub_type: gguf.GGUFValueType | None = None\ndef get_field_data(reader: gguf.GGUFReader, key: str) -> Any:\n    field = reader.get_field(key)\n    return field.contents() if field else None\ndef find_token(token_list: Sequence[int], token: str) -> Sequence[int]:\n    token_ids = [index for index, value in enumerate(token_list) if value == token]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "documentation": {}
    },
    {
        "label": "get_field_data",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "peekOfCode": "def get_field_data(reader: gguf.GGUFReader, key: str) -> Any:\n    field = reader.get_field(key)\n    return field.contents() if field else None\ndef find_token(token_list: Sequence[int], token: str) -> Sequence[int]:\n    token_ids = [index for index, value in enumerate(token_list) if value == token]\n    if len(token_ids) == 0:\n        raise LookupError(f'Unable to find \"{token}\" in token list!')\n    return token_ids\ndef copy_with_new_metadata(reader: gguf.GGUFReader, writer: gguf.GGUFWriter, new_metadata: dict[str, MetadataDetails], remove_metadata: Sequence[str]) -> None:\n    for field in reader.fields.values():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "documentation": {}
    },
    {
        "label": "find_token",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "peekOfCode": "def find_token(token_list: Sequence[int], token: str) -> Sequence[int]:\n    token_ids = [index for index, value in enumerate(token_list) if value == token]\n    if len(token_ids) == 0:\n        raise LookupError(f'Unable to find \"{token}\" in token list!')\n    return token_ids\ndef copy_with_new_metadata(reader: gguf.GGUFReader, writer: gguf.GGUFWriter, new_metadata: dict[str, MetadataDetails], remove_metadata: Sequence[str]) -> None:\n    for field in reader.fields.values():\n        # Suppress virtual fields and fields written by GGUFWriter\n        if field.name == gguf.Keys.General.ARCHITECTURE or field.name.startswith('GGUF.'):\n            logger.debug(f'Suppressing {field.name}')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "documentation": {}
    },
    {
        "label": "copy_with_new_metadata",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "peekOfCode": "def copy_with_new_metadata(reader: gguf.GGUFReader, writer: gguf.GGUFWriter, new_metadata: dict[str, MetadataDetails], remove_metadata: Sequence[str]) -> None:\n    for field in reader.fields.values():\n        # Suppress virtual fields and fields written by GGUFWriter\n        if field.name == gguf.Keys.General.ARCHITECTURE or field.name.startswith('GGUF.'):\n            logger.debug(f'Suppressing {field.name}')\n            continue\n        # Skip old chat templates if we have new ones\n        if field.name.startswith(gguf.Keys.Tokenizer.CHAT_TEMPLATE) and gguf.Keys.Tokenizer.CHAT_TEMPLATE in new_metadata:\n            logger.debug(f'Skipping {field.name}')\n            continue",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "peekOfCode": "def main() -> None:\n    tokenizer_metadata = (getattr(gguf.Keys.Tokenizer, n) for n in gguf.Keys.Tokenizer.__dict__.keys() if not n.startswith('_'))\n    token_names = dict((n.split('.')[-1][:-len('_token_id')], n) for n in tokenizer_metadata if n.endswith('_token_id'))\n    parser = argparse.ArgumentParser(description=\"Make a copy of a GGUF file with new metadata\")\n    parser.add_argument(\"input\",                                       type=Path, help=\"GGUF format model input filename\")\n    parser.add_argument(\"output\",                                      type=Path, help=\"GGUF format model output filename\")\n    parser.add_argument(\"--general-name\",                              type=str,  help=\"The models general.name\", metavar='\"name\"')\n    parser.add_argument(\"--general-description\",                       type=str,  help=\"The models general.description\", metavar='\"Description ...\"')\n    parser.add_argument(\"--chat-template\",                             type=str,  help=\"Chat template string (or JSON string containing templates)\", metavar='\"{% ... %} ...\"')\n    parser.add_argument(\"--chat-template-config\",                      type=Path, help=\"Config file containing chat template(s)\", metavar='tokenizer_config.json')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "peekOfCode": "logger = logging.getLogger(\"gguf-new-metadata\")\nclass MetadataDetails(NamedTuple):\n    type: gguf.GGUFValueType\n    value: Any\n    description: str = ''\n    sub_type: gguf.GGUFValueType | None = None\ndef get_field_data(reader: gguf.GGUFReader, key: str) -> Any:\n    field = reader.get_field(key)\n    return field.contents() if field else None\ndef find_token(token_list: Sequence[int], token: str) -> Sequence[int]:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_new_metadata",
        "documentation": {}
    },
    {
        "label": "minimal_example",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "peekOfCode": "def minimal_example(filename: str) -> None:\n    reader = GGUFReader(filename, 'r+')\n    field = reader.fields['tokenizer.ggml.bos_token_id']\n    if field is None:\n        return\n    part_index = field.data[0]\n    field.parts[part_index][0] = 2  # Set tokenizer.ggml.bos_token_id to 2\n    #\n    # So what's this field.data thing? It's helpful because field.parts contains\n    # _every_ part of the GGUF field. For example, tokenizer.ggml.bos_token_id consists",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "documentation": {}
    },
    {
        "label": "set_metadata",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "peekOfCode": "def set_metadata(reader: GGUFReader, args: argparse.Namespace) -> None:\n    field = reader.get_field(args.key)\n    if field is None:\n        logger.error(f'! Field {repr(args.key)} not found')\n        sys.exit(1)\n    # Note that field.types is a list of types. This is because the GGUF\n    # format supports arrays. For example, an array of UINT32 would\n    # look like [GGUFValueType.ARRAY, GGUFValueType.UINT32]\n    handler = reader.gguf_scalar_to_np.get(field.types[0]) if field.types else None\n    if handler is None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "peekOfCode": "def main() -> None:\n    parser = argparse.ArgumentParser(description=\"Set a simple value in GGUF file metadata\")\n    parser.add_argument(\"model\",     type=str,            help=\"GGUF format model filename\")\n    parser.add_argument(\"key\",       type=str,            help=\"Metadata key to set\")\n    parser.add_argument(\"value\",     type=str,            help=\"Metadata value to set\")\n    parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Don't actually change anything\")\n    parser.add_argument(\"--force\",   action=\"store_true\", help=\"Change the field without confirmation\")\n    parser.add_argument(\"--verbose\",      action=\"store_true\",    help=\"increase output verbosity\")\n    args = parser.parse_args(None if len(sys.argv) > 1 else [\"--help\"])\n    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "peekOfCode": "logger = logging.getLogger(\"gguf-set-metadata\")\ndef minimal_example(filename: str) -> None:\n    reader = GGUFReader(filename, 'r+')\n    field = reader.fields['tokenizer.ggml.bos_token_id']\n    if field is None:\n        return\n    part_index = field.data[0]\n    field.parts[part_index][0] = 2  # Set tokenizer.ggml.bos_token_id to 2\n    #\n    # So what's this field.data thing? It's helpful because field.parts contains",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.scripts.gguf_set_metadata",
        "documentation": {}
    },
    {
        "label": "Keys",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class Keys:\n    class General:\n        TYPE                       = \"general.type\"\n        ARCHITECTURE               = \"general.architecture\"\n        QUANTIZATION_VERSION       = \"general.quantization_version\"\n        ALIGNMENT                  = \"general.alignment\"\n        FILE_TYPE                  = \"general.file_type\"\n        # Recommended Sampler Parameters\n        SAMPLING_SEQUENCE           = \"general.sampling.sequence\"\n        SAMPLING_TOP_K              = \"general.sampling.top_k\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGUFType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class GGUFType:\n    MODEL   = \"model\"\n    ADAPTER = \"adapter\"\n    IMATRIX = \"imatrix\"\n    MMPROJ  = \"mmproj\" # dummy, unused for now\nclass MODEL_ARCH(IntEnum):\n    MMPROJ           = auto() # dummy arch for clip.cpp\n    LLAMA            = auto()\n    LLAMA4           = auto()\n    DECI             = auto()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "MODEL_ARCH",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class MODEL_ARCH(IntEnum):\n    MMPROJ           = auto() # dummy arch for clip.cpp\n    LLAMA            = auto()\n    LLAMA4           = auto()\n    DECI             = auto()\n    FALCON           = auto()\n    FALCON_H1        = auto()\n    BAICHUAN         = auto()\n    GROK             = auto()\n    GPT2             = auto()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "VISION_PROJECTOR_TYPE",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class VISION_PROJECTOR_TYPE(IntEnum):\n    MLP       = auto()\n    LDP       = auto()\n    LDPV2     = auto()\n    RESAMPLER = auto()\n    GLM_EDGE  = auto()\n    MERGER    = auto()\n    GEMMA3N   = auto()\n    GEMMA3    = auto()\n    QWEN3VL   = auto()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "MODEL_TENSOR",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class MODEL_TENSOR(IntEnum):\n    TOKEN_EMBD           = auto()\n    TOKEN_EMBD_NORM      = auto()\n    TOKEN_TYPES          = auto()\n    POS_EMBD             = auto()\n    OUTPUT               = auto()\n    DENSE_2_OUT          = auto() # embeddinggemma 2_Dense\n    DENSE_3_OUT          = auto() # embeddinggemma 3_Dense\n    OUTPUT_NORM          = auto()\n    ROPE_FREQS           = auto()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "TokenType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class TokenType(IntEnum):\n    NORMAL       = 1\n    UNKNOWN      = 2\n    CONTROL      = 3\n    USER_DEFINED = 4\n    UNUSED       = 5\n    BYTE         = 6\nclass RopeScalingType(Enum):\n    NONE     = 'none'\n    LINEAR   = 'linear'",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "RopeScalingType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class RopeScalingType(Enum):\n    NONE     = 'none'\n    LINEAR   = 'linear'\n    YARN     = 'yarn'\n    LONGROPE = 'longrope'\nclass PoolingType(IntEnum):\n    NONE = 0\n    MEAN = 1\n    CLS  = 2\n    LAST = 3",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "PoolingType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class PoolingType(IntEnum):\n    NONE = 0\n    MEAN = 1\n    CLS  = 2\n    LAST = 3\n    RANK = 4\nclass GGMLQuantizationType(IntEnum):\n    F32     = 0\n    F16     = 1\n    Q4_0    = 2",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGMLQuantizationType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class GGMLQuantizationType(IntEnum):\n    F32     = 0\n    F16     = 1\n    Q4_0    = 2\n    Q4_1    = 3\n    Q5_0    = 6\n    Q5_1    = 7\n    Q8_0    = 8\n    Q8_1    = 9\n    Q2_K    = 10",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "ExpertGatingFuncType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class ExpertGatingFuncType(IntEnum):\n    SOFTMAX  = 1\n    SIGMOID  = 2\n# TODO: add GGMLFileType from ggml_ftype in ggml.h\n# from llama_ftype in llama.h\n# ALL VALUES SHOULD BE THE SAME HERE AS THEY ARE OVER THERE.\nclass LlamaFileType(IntEnum):\n    ALL_F32              = 0\n    MOSTLY_F16           = 1   # except 1d tensors\n    MOSTLY_Q4_0          = 2   # except 1d tensors",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "LlamaFileType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class LlamaFileType(IntEnum):\n    ALL_F32              = 0\n    MOSTLY_F16           = 1   # except 1d tensors\n    MOSTLY_Q4_0          = 2   # except 1d tensors\n    MOSTLY_Q4_1          = 3   # except 1d tensors\n    # MOSTLY_Q4_1_SOME_F16 = 4   # tok_embeddings.weight and output.weight are F16\n    # MOSTLY_Q4_2        = 5   # support has been removed\n    # MOSTLY_Q4_3        = 6   # support has been removed\n    MOSTLY_Q8_0          = 7   # except 1d tensors\n    MOSTLY_Q5_0          = 8   # except 1d tensors",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGUFEndian",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class GGUFEndian(IntEnum):\n    LITTLE = 0\n    BIG = 1\nclass GGUFValueType(IntEnum):\n    UINT8   = 0\n    INT8    = 1\n    UINT16  = 2\n    INT16   = 3\n    UINT32  = 4\n    INT32   = 5",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGUFValueType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class GGUFValueType(IntEnum):\n    UINT8   = 0\n    INT8    = 1\n    UINT16  = 2\n    INT16   = 3\n    UINT32  = 4\n    INT32   = 5\n    FLOAT32 = 6\n    BOOL    = 7\n    STRING  = 8",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "VisionProjectorType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "class VisionProjectorType:\n    GEMMA3 = \"gemma3\"\n    GEMMA3NV = \"gemma3nv\"\n    GEMMA3NA = \"gemma3na\"\n    IDEFICS3 = \"idefics3\"\n    PIXTRAL = \"pixtral\"\n    LLAMA4 = \"llama4\"\n    QWEN2VL = \"qwen2vl_merger\"\n    QWEN25VL = \"qwen2.5vl_merger\"\n    QWEN3VL = \"qwen3vl_merger\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "GGUF_DEFAULT_ALIGNMENT",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "GGUF_DEFAULT_ALIGNMENT = 32\nGGML_QUANT_VERSION     = 2  # GGML_QNT_VERSION from ggml.h\n#\n# metadata keys\n#\nclass Keys:\n    class General:\n        TYPE                       = \"general.type\"\n        ARCHITECTURE               = \"general.architecture\"\n        QUANTIZATION_VERSION       = \"general.quantization_version\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "QK_K",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "QK_K = 256\nGGML_QUANT_SIZES: dict[GGMLQuantizationType, tuple[int, int]] = {\n    GGMLQuantizationType.F32:     (1, 4),\n    GGMLQuantizationType.F16:     (1, 2),\n    GGMLQuantizationType.Q4_0:    (32, 2 + 16),\n    GGMLQuantizationType.Q4_1:    (32, 2 + 2 + 16),\n    GGMLQuantizationType.Q5_0:    (32, 2 + 4 + 16),\n    GGMLQuantizationType.Q5_1:    (32, 2 + 2 + 4 + 16),\n    GGMLQuantizationType.Q8_0:    (32, 2 + 32),\n    GGMLQuantizationType.Q8_1:    (32, 4 + 4 + 32),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_GENERAL_QUANTIZATION_VERSION",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_GENERAL_QUANTIZATION_VERSION = Keys.General.QUANTIZATION_VERSION\nKEY_GENERAL_ALIGNMENT            = Keys.General.ALIGNMENT\nKEY_GENERAL_NAME                 = Keys.General.NAME\nKEY_GENERAL_AUTHOR               = Keys.General.AUTHOR\nKEY_GENERAL_URL                  = Keys.General.URL\nKEY_GENERAL_DESCRIPTION          = Keys.General.DESCRIPTION\nKEY_GENERAL_LICENSE              = Keys.General.LICENSE\nKEY_GENERAL_SOURCE_URL           = Keys.General.SOURCE_URL\nKEY_GENERAL_FILE_TYPE            = Keys.General.FILE_TYPE\n# LLM",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_USE_PARALLEL_RESIDUAL",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_USE_PARALLEL_RESIDUAL = Keys.LLM.USE_PARALLEL_RESIDUAL\nKEY_TENSOR_DATA_LAYOUT    = Keys.LLM.TENSOR_DATA_LAYOUT\n# attention\nKEY_ATTENTION_HEAD_COUNT        = Keys.Attention.HEAD_COUNT\nKEY_ATTENTION_HEAD_COUNT_KV     = Keys.Attention.HEAD_COUNT_KV\nKEY_ATTENTION_MAX_ALIBI_BIAS    = Keys.Attention.MAX_ALIBI_BIAS\nKEY_ATTENTION_CLAMP_KQV         = Keys.Attention.CLAMP_KQV\nKEY_ATTENTION_LAYERNORM_EPS     = Keys.Attention.LAYERNORM_EPS\nKEY_ATTENTION_LAYERNORM_RMS_EPS = Keys.Attention.LAYERNORM_RMS_EPS\n# RoPE",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_ATTENTION_LAYERNORM_RMS_EPS",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_ATTENTION_LAYERNORM_RMS_EPS = Keys.Attention.LAYERNORM_RMS_EPS\n# RoPE\nKEY_ROPE_DIMENSION_COUNT      = Keys.Rope.DIMENSION_COUNT\nKEY_ROPE_FREQ_BASE            = Keys.Rope.FREQ_BASE\nKEY_ROPE_SCALING_TYPE         = Keys.Rope.SCALING_TYPE\nKEY_ROPE_SCALING_FACTOR       = Keys.Rope.SCALING_FACTOR\nKEY_ROPE_SCALING_ORIG_CTX_LEN = Keys.Rope.SCALING_ORIG_CTX_LEN\nKEY_ROPE_SCALING_FINETUNED    = Keys.Rope.SCALING_FINETUNED\n# SSM\nKEY_SSM_CONV_KERNEL    = Keys.SSM.CONV_KERNEL",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_ROPE_SCALING_ORIG_CTX_LEN",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_ROPE_SCALING_ORIG_CTX_LEN = Keys.Rope.SCALING_ORIG_CTX_LEN\nKEY_ROPE_SCALING_FINETUNED    = Keys.Rope.SCALING_FINETUNED\n# SSM\nKEY_SSM_CONV_KERNEL    = Keys.SSM.CONV_KERNEL\nKEY_SSM_INNER_SIZE     = Keys.SSM.INNER_SIZE\nKEY_SSM_STATE_SIZE     = Keys.SSM.STATE_SIZE\nKEY_SSM_TIME_STEP_RANK = Keys.SSM.TIME_STEP_RANK\nKEY_SSM_GROUP_COUNT    = Keys.SSM.GROUP_COUNT\nKEY_SSM_DT_B_C_RMS     = Keys.SSM.DT_B_C_RMS\n# tokenization",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_SSM_TIME_STEP_RANK",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_SSM_TIME_STEP_RANK = Keys.SSM.TIME_STEP_RANK\nKEY_SSM_GROUP_COUNT    = Keys.SSM.GROUP_COUNT\nKEY_SSM_DT_B_C_RMS     = Keys.SSM.DT_B_C_RMS\n# tokenization\nKEY_TOKENIZER_MODEL      = Keys.Tokenizer.MODEL\nKEY_TOKENIZER_PRE        = Keys.Tokenizer.PRE\nKEY_TOKENIZER_LIST       = Keys.Tokenizer.LIST\nKEY_TOKENIZER_TOKEN_TYPE = Keys.Tokenizer.TOKEN_TYPE\nKEY_TOKENIZER_SCORES     = Keys.Tokenizer.SCORES\nKEY_TOKENIZER_MERGES     = Keys.Tokenizer.MERGES",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_TOKENIZER_TOKEN_TYPE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_TOKENIZER_TOKEN_TYPE = Keys.Tokenizer.TOKEN_TYPE\nKEY_TOKENIZER_SCORES     = Keys.Tokenizer.SCORES\nKEY_TOKENIZER_MERGES     = Keys.Tokenizer.MERGES\nKEY_TOKENIZER_BOS_ID     = Keys.Tokenizer.BOS_ID\nKEY_TOKENIZER_EOS_ID     = Keys.Tokenizer.EOS_ID\nKEY_TOKENIZER_EOT_ID     = Keys.Tokenizer.EOT_ID\nKEY_TOKENIZER_EOM_ID     = Keys.Tokenizer.EOM_ID\nKEY_TOKENIZER_UNK_ID     = Keys.Tokenizer.UNK_ID\nKEY_TOKENIZER_SEP_ID     = Keys.Tokenizer.SEP_ID\nKEY_TOKENIZER_PAD_ID     = Keys.Tokenizer.PAD_ID",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_TOKENIZER_FIM_PRE_ID",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_TOKENIZER_FIM_PRE_ID = Keys.Tokenizer.FIM_PRE_ID\nKEY_TOKENIZER_FIM_SUF_ID = Keys.Tokenizer.FIM_SUF_ID\nKEY_TOKENIZER_FIM_MID_ID = Keys.Tokenizer.FIM_MID_ID\nKEY_TOKENIZER_FIM_PAD_ID = Keys.Tokenizer.FIM_PAD_ID\nKEY_TOKENIZER_FIM_REP_ID = Keys.Tokenizer.FIM_REP_ID\nKEY_TOKENIZER_FIM_SEP_ID = Keys.Tokenizer.FIM_SEP_ID\n# deprecated\nKEY_TOKENIZER_PREFIX_ID  = Keys.Tokenizer.PREFIX_ID\nKEY_TOKENIZER_SUFFIX_ID  = Keys.Tokenizer.SUFFIX_ID\nKEY_TOKENIZER_MIDDLE_ID  = Keys.Tokenizer.MIDDLE_ID",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_TOKENIZER_FIM_SUF_ID",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_TOKENIZER_FIM_SUF_ID = Keys.Tokenizer.FIM_SUF_ID\nKEY_TOKENIZER_FIM_MID_ID = Keys.Tokenizer.FIM_MID_ID\nKEY_TOKENIZER_FIM_PAD_ID = Keys.Tokenizer.FIM_PAD_ID\nKEY_TOKENIZER_FIM_REP_ID = Keys.Tokenizer.FIM_REP_ID\nKEY_TOKENIZER_FIM_SEP_ID = Keys.Tokenizer.FIM_SEP_ID\n# deprecated\nKEY_TOKENIZER_PREFIX_ID  = Keys.Tokenizer.PREFIX_ID\nKEY_TOKENIZER_SUFFIX_ID  = Keys.Tokenizer.SUFFIX_ID\nKEY_TOKENIZER_MIDDLE_ID  = Keys.Tokenizer.MIDDLE_ID",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_TOKENIZER_FIM_MID_ID",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_TOKENIZER_FIM_MID_ID = Keys.Tokenizer.FIM_MID_ID\nKEY_TOKENIZER_FIM_PAD_ID = Keys.Tokenizer.FIM_PAD_ID\nKEY_TOKENIZER_FIM_REP_ID = Keys.Tokenizer.FIM_REP_ID\nKEY_TOKENIZER_FIM_SEP_ID = Keys.Tokenizer.FIM_SEP_ID\n# deprecated\nKEY_TOKENIZER_PREFIX_ID  = Keys.Tokenizer.PREFIX_ID\nKEY_TOKENIZER_SUFFIX_ID  = Keys.Tokenizer.SUFFIX_ID\nKEY_TOKENIZER_MIDDLE_ID  = Keys.Tokenizer.MIDDLE_ID",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_TOKENIZER_FIM_PAD_ID",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_TOKENIZER_FIM_PAD_ID = Keys.Tokenizer.FIM_PAD_ID\nKEY_TOKENIZER_FIM_REP_ID = Keys.Tokenizer.FIM_REP_ID\nKEY_TOKENIZER_FIM_SEP_ID = Keys.Tokenizer.FIM_SEP_ID\n# deprecated\nKEY_TOKENIZER_PREFIX_ID  = Keys.Tokenizer.PREFIX_ID\nKEY_TOKENIZER_SUFFIX_ID  = Keys.Tokenizer.SUFFIX_ID\nKEY_TOKENIZER_MIDDLE_ID  = Keys.Tokenizer.MIDDLE_ID",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_TOKENIZER_FIM_REP_ID",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_TOKENIZER_FIM_REP_ID = Keys.Tokenizer.FIM_REP_ID\nKEY_TOKENIZER_FIM_SEP_ID = Keys.Tokenizer.FIM_SEP_ID\n# deprecated\nKEY_TOKENIZER_PREFIX_ID  = Keys.Tokenizer.PREFIX_ID\nKEY_TOKENIZER_SUFFIX_ID  = Keys.Tokenizer.SUFFIX_ID\nKEY_TOKENIZER_MIDDLE_ID  = Keys.Tokenizer.MIDDLE_ID",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "KEY_TOKENIZER_FIM_SEP_ID",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "peekOfCode": "KEY_TOKENIZER_FIM_SEP_ID = Keys.Tokenizer.FIM_SEP_ID\n# deprecated\nKEY_TOKENIZER_PREFIX_ID  = Keys.Tokenizer.PREFIX_ID\nKEY_TOKENIZER_SUFFIX_ID  = Keys.Tokenizer.SUFFIX_ID\nKEY_TOKENIZER_MIDDLE_ID  = Keys.Tokenizer.MIDDLE_ID",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.constants",
        "documentation": {}
    },
    {
        "label": "ReaderField",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "peekOfCode": "class ReaderField(NamedTuple):\n    # Offset to start of this field.\n    offset: int\n    # Name of the field (not necessarily from file data).\n    name: str\n    # Data parts. Some types have multiple components, such as strings\n    # that consist of a length followed by the string data.\n    parts: list[npt.NDArray[Any]] = []\n    # Indexes into parts that we can call the actual data. For example\n    # an array of strings will be populated with indexes to the actual",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "documentation": {}
    },
    {
        "label": "ReaderTensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "peekOfCode": "class ReaderTensor(NamedTuple):\n    name: str\n    tensor_type: GGMLQuantizationType\n    shape: npt.NDArray[np.uint32]\n    n_elements: int\n    n_bytes: int\n    data_offset: int\n    data: npt.NDArray[Any]\n    field: ReaderField\nclass GGUFReader:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "documentation": {}
    },
    {
        "label": "GGUFReader",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "peekOfCode": "class GGUFReader:\n    # I - same as host, S - swapped\n    byte_order: Literal['I', 'S'] = 'I'\n    alignment: int = GGUF_DEFAULT_ALIGNMENT\n    data_offset: int\n    # Note: Internal helper, API may change.\n    gguf_scalar_to_np: dict[GGUFValueType, type[np.generic]] = {\n        GGUFValueType.UINT8:   np.uint8,\n        GGUFValueType.INT8:    np.int8,\n        GGUFValueType.UINT16:  np.uint16,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "peekOfCode": "logger = logging.getLogger(__name__)\nREADER_SUPPORTED_VERSIONS = [2, GGUF_VERSION]\nclass ReaderField(NamedTuple):\n    # Offset to start of this field.\n    offset: int\n    # Name of the field (not necessarily from file data).\n    name: str\n    # Data parts. Some types have multiple components, such as strings\n    # that consist of a length followed by the string data.\n    parts: list[npt.NDArray[Any]] = []",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "documentation": {}
    },
    {
        "label": "READER_SUPPORTED_VERSIONS",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "peekOfCode": "READER_SUPPORTED_VERSIONS = [2, GGUF_VERSION]\nclass ReaderField(NamedTuple):\n    # Offset to start of this field.\n    offset: int\n    # Name of the field (not necessarily from file data).\n    name: str\n    # Data parts. Some types have multiple components, such as strings\n    # that consist of a length followed by the string data.\n    parts: list[npt.NDArray[Any]] = []\n    # Indexes into parts that we can call the actual data. For example",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_reader",
        "documentation": {}
    },
    {
        "label": "TensorInfo",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "peekOfCode": "class TensorInfo:\n    shape: Sequence[int]\n    dtype: GGMLQuantizationType\n    nbytes: int\n    tensor: np.ndarray[Any, Any] | None = None\n@dataclass\nclass GGUFValue:\n    value: Any\n    type: GGUFValueType\n    sub_type: GGUFValueType | None = None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "documentation": {}
    },
    {
        "label": "GGUFValue",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "peekOfCode": "class GGUFValue:\n    value: Any\n    type: GGUFValueType\n    sub_type: GGUFValueType | None = None\nclass WriterState(Enum):\n    NO_FILE = auto()\n    EMPTY   = auto()\n    HEADER  = auto()\n    KV_DATA = auto()\n    TI_DATA = auto()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "documentation": {}
    },
    {
        "label": "WriterState",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "peekOfCode": "class WriterState(Enum):\n    NO_FILE = auto()\n    EMPTY   = auto()\n    HEADER  = auto()\n    KV_DATA = auto()\n    TI_DATA = auto()\n    WEIGHTS = auto()\nclass GGUFWriter:\n    fout: list[BufferedWriter] | None\n    path: Path | None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "documentation": {}
    },
    {
        "label": "GGUFWriter",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "peekOfCode": "class GGUFWriter:\n    fout: list[BufferedWriter] | None\n    path: Path | None\n    temp_file: tempfile.SpooledTemporaryFile[bytes] | None\n    tensors: list[dict[str, TensorInfo]]\n    kv_data: list[dict[str, GGUFValue]]\n    state: WriterState\n    _simple_value_packing = {\n        GGUFValueType.UINT8:   \"B\",\n        GGUFValueType.INT8:    \"b\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "peekOfCode": "logger = logging.getLogger(__name__)\nSHARD_NAME_FORMAT = \"{:s}-{:05d}-of-{:05d}.gguf\"\n@dataclass\nclass TensorInfo:\n    shape: Sequence[int]\n    dtype: GGMLQuantizationType\n    nbytes: int\n    tensor: np.ndarray[Any, Any] | None = None\n@dataclass\nclass GGUFValue:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "documentation": {}
    },
    {
        "label": "SHARD_NAME_FORMAT",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "peekOfCode": "SHARD_NAME_FORMAT = \"{:s}-{:05d}-of-{:05d}.gguf\"\n@dataclass\nclass TensorInfo:\n    shape: Sequence[int]\n    dtype: GGMLQuantizationType\n    nbytes: int\n    tensor: np.ndarray[Any, Any] | None = None\n@dataclass\nclass GGUFValue:\n    value: Any",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.gguf_writer",
        "documentation": {}
    },
    {
        "label": "LazyMeta",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "peekOfCode": "class LazyMeta(ABCMeta):\n    def __new__(cls, name: str, bases: tuple[type, ...], namespace: dict[str, Any], **kwargs):\n        def __getattr__(self, name: str) -> Any:\n            meta_attr = getattr(self._meta, name)\n            if callable(meta_attr):\n                return type(self)._wrap_fn(\n                    (lambda s, *args, **kwargs: getattr(s, name)(*args, **kwargs)),\n                    use_self=self,\n                )\n            elif isinstance(meta_attr, self._tensor_type):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "documentation": {}
    },
    {
        "label": "LazyBase",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "peekOfCode": "class LazyBase(ABC, metaclass=LazyMeta):\n    _tensor_type: type\n    _meta: Any\n    _data: Any | None\n    _args: tuple\n    _kwargs: dict[str, Any]\n    _func: Callable[[Any], Any] | None\n    def __init__(self, *, meta: Any, data: Any | None = None, args: tuple = (), kwargs: dict[str, Any] | None = None, func: Callable[[Any], Any] | None = None):\n        super().__init__()\n        self._meta = meta",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "documentation": {}
    },
    {
        "label": "LazyNumpyTensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "peekOfCode": "class LazyNumpyTensor(LazyBase):\n    _tensor_type = np.ndarray\n    shape: tuple[int, ...]  # Makes the type checker happy in quants.py\n    @classmethod\n    def meta_with_dtype_and_shape(cls, dtype: DTypeLike, shape: tuple[int, ...]) -> np.ndarray[Any, Any]:\n        # The initial idea was to use np.nan as the fill value,\n        # but non-float types like np.int16 can't use that.\n        # So zero it is.\n        cheat = np.zeros(1, dtype)\n        return np.lib.stride_tricks.as_strided(cheat, shape, (0 for _ in shape))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass LazyMeta(ABCMeta):\n    def __new__(cls, name: str, bases: tuple[type, ...], namespace: dict[str, Any], **kwargs):\n        def __getattr__(self, name: str) -> Any:\n            meta_attr = getattr(self._meta, name)\n            if callable(meta_attr):\n                return type(self)._wrap_fn(\n                    (lambda s, *args, **kwargs: getattr(s, name)(*args, **kwargs)),\n                    use_self=self,\n                )",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.lazy",
        "documentation": {}
    },
    {
        "label": "Metadata",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.metadata",
        "peekOfCode": "class Metadata:\n    # Recommended Sampler Parameters to be written to GGUF KV Store\n    sampling_sequence: Optional[str] = None\n    sampling_top_k: Optional[int] = None\n    sampling_top_p: Optional[float] = None\n    sampling_min_p: Optional[float] = None\n    sampling_xtc_probability: Optional[float] = None\n    sampling_xtc_threshold: Optional[float] = None\n    sampling_temp: Optional[float] = None\n    sampling_penalty_last_n: Optional[int] = None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.metadata",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.metadata",
        "peekOfCode": "logger = logging.getLogger(\"metadata\")\n@dataclass\nclass Metadata:\n    # Recommended Sampler Parameters to be written to GGUF KV Store\n    sampling_sequence: Optional[str] = None\n    sampling_top_k: Optional[int] = None\n    sampling_top_p: Optional[float] = None\n    sampling_min_p: Optional[float] = None\n    sampling_xtc_probability: Optional[float] = None\n    sampling_xtc_threshold: Optional[float] = None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.metadata",
        "documentation": {}
    },
    {
        "label": "QuantError",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class QuantError(Exception): ...\n_type_traits: dict[GGMLQuantizationType, type[__Quant]] = {}\ndef quantize(data: np.ndarray, qtype: GGMLQuantizationType) -> np.ndarray:\n    if qtype == GGMLQuantizationType.F32:\n        return data.astype(np.float32, copy=False)\n    elif qtype == GGMLQuantizationType.F16:\n        return data.astype(np.float16, copy=False)\n    elif (q := _type_traits.get(qtype)) is not None:\n        return q.quantize(data)\n    else:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "__Quant",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class __Quant(ABC):\n    qtype: GGMLQuantizationType\n    block_size: int\n    type_size: int\n    grid: np.ndarray[Any, np.dtype[np.float32]] | None = None\n    grid_shape: tuple[int, int] = (0, 0)\n    grid_map: tuple[int | float, ...] = ()\n    grid_hex: bytes | None = None\n    def __init__(self):\n        return TypeError(\"Quant conversion classes can't have instances\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "BF16",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class BF16(__Quant, qtype=GGMLQuantizationType.BF16):\n    @classmethod\n    # same as ggml_compute_fp32_to_bf16 in ggml-impl.h\n    def quantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n = blocks.view(np.uint32)\n        # force nan to quiet\n        n = np.where((n & 0x7fffffff) > 0x7f800000, (n & np.uint32(0xffff0000)) | np.uint32(64 << 16), n)\n        # round to nearest even\n        n = (np.uint64(n) + (0x7fff + ((n >> 16) & 1))) >> 16\n        return n.astype(np.uint16).view(np.uint8)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "Q4_0",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class Q4_0(__Quant, qtype=GGMLQuantizationType.Q4_0):\n    @classmethod\n    def quantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        imax = abs(blocks).argmax(axis=-1, keepdims=True)\n        max = np.take_along_axis(blocks, imax, axis=-1)\n        d = max / -8\n        with np.errstate(divide=\"ignore\"):\n            id = np.where(d == 0, 0, 1 / d)\n        qs = np.trunc((blocks * id) + np.float32(8.5), dtype=np.float32).astype(np.uint8).clip(0, 15)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "Q4_1",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class Q4_1(__Quant, qtype=GGMLQuantizationType.Q4_1):\n    @classmethod\n    def quantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        max = blocks.max(axis=-1, keepdims=True)\n        min = blocks.min(axis=-1, keepdims=True)\n        d = (max - min) / 15\n        with np.errstate(divide=\"ignore\"):\n            id = np.where(d == 0, 0, 1 / d)\n        qs = np.trunc((blocks - min) * id + np.float32(0.5), dtype=np.float32).astype(np.uint8).clip(0, 15)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "Q5_0",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class Q5_0(__Quant, qtype=GGMLQuantizationType.Q5_0):\n    @classmethod\n    def quantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        imax = abs(blocks).argmax(axis=-1, keepdims=True)\n        max = np.take_along_axis(blocks, imax, axis=-1)\n        d = max / -16\n        with np.errstate(divide=\"ignore\"):\n            id = np.where(d == 0, 0, 1 / d)\n        q = np.trunc((blocks * id) + np.float32(16.5), dtype=np.float32).astype(np.uint8).clip(0, 31)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "Q5_1",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class Q5_1(__Quant, qtype=GGMLQuantizationType.Q5_1):\n    @classmethod\n    def quantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        max = blocks.max(axis=-1, keepdims=True)\n        min = blocks.min(axis=-1, keepdims=True)\n        d = (max - min) / 31\n        with np.errstate(divide=\"ignore\"):\n            id = np.where(d == 0, 0, 1 / d)\n        q = np.trunc((blocks - min) * id + np.float32(0.5), dtype=np.float32).astype(np.uint8).clip(0, 31)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "Q8_0",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class Q8_0(__Quant, qtype=GGMLQuantizationType.Q8_0):\n    @classmethod\n    # Implementation of Q8_0 with bit-exact same results as reference implementation in ggml-quants.c\n    def quantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        d = abs(blocks).max(axis=1, keepdims=True) / 127\n        with np.errstate(divide=\"ignore\"):\n            id = np.where(d == 0, 0, 1 / d)\n        qs = np_roundf(blocks * id)\n        # (n_blocks, 2)\n        d = d.astype(np.float16).view(np.uint8)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "Q2_K",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class Q2_K(__Quant, qtype=GGMLQuantizationType.Q2_K):\n    @classmethod\n    def dequantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        scales, rest = np.hsplit(blocks, [QK_K // 16])\n        qs, rest = np.hsplit(rest, [QK_K // 4])\n        d, dmin = np.hsplit(rest, [2])\n        d = d.view(np.float16).astype(np.float32)\n        dmin = dmin.view(np.float16).astype(np.float32)\n        # (n_blocks, 16, 1)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "Q3_K",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class Q3_K(__Quant, qtype=GGMLQuantizationType.Q3_K):\n    @classmethod\n    def dequantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        hmask, rest = np.hsplit(blocks, [QK_K // 8])\n        qs, rest = np.hsplit(rest, [QK_K // 4])\n        scales, d = np.hsplit(rest, [12])\n        d = d.view(np.float16).astype(np.float32)\n        # The scales are packed at 6-bit each in this pattern:\n        #  0: IIIIAAAA",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "Q4_K",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class Q4_K(__Quant, qtype=GGMLQuantizationType.Q4_K):\n    K_SCALE_SIZE = 12\n    @staticmethod\n    def get_scale_min(scales: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n        n_blocks = scales.shape[0]\n        scales = scales.view(np.uint8)\n        ### Unpacking the following: ###\n        #  0 EEAAAAAA\n        #  1 FFBBBBBB\n        #  2 GGCCCCCC",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "Q5_K",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class Q5_K(__Quant, qtype=GGMLQuantizationType.Q5_K):\n    @classmethod\n    def dequantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        d, rest = np.hsplit(blocks, [2])\n        dmin, rest = np.hsplit(rest, [2])\n        scales, rest = np.hsplit(rest, [Q4_K.K_SCALE_SIZE])\n        qh, qs = np.hsplit(rest, [QK_K // 8])\n        d = d.view(np.float16).astype(np.float32)\n        dmin = dmin.view(np.float16).astype(np.float32)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "Q6_K",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class Q6_K(__Quant, qtype=GGMLQuantizationType.Q6_K):\n    @classmethod\n    def dequantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        ql, rest = np.hsplit(blocks, [QK_K // 2])\n        qh, rest = np.hsplit(rest, [QK_K // 4])\n        scales, d = np.hsplit(rest, [QK_K // 16])\n        scales = scales.view(np.int8).astype(np.float32)\n        d = d.view(np.float16).astype(np.float32)\n        d = (d * scales).reshape((n_blocks, QK_K // 16, 1))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "TQ1_0",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class TQ1_0(__Quant, qtype=GGMLQuantizationType.TQ1_0):\n    @classmethod\n    def quantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        d = abs(blocks).max(axis=-1, keepdims=True)\n        with np.errstate(divide=\"ignore\"):\n            id = np.where(d == 0, 0, 1 / d)\n        qs = np_roundf(blocks * id)\n        qs = (qs.astype(np.int8) + np.int8(1)).astype(np.uint8)\n        qs0, qs1, qh = qs[..., :(32 * 5)], qs[..., (32 * 5):(48 * 5)], qs[..., (48 * 5):]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "TQ2_0",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class TQ2_0(__Quant, qtype=GGMLQuantizationType.TQ2_0):\n    @classmethod\n    def quantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        d = abs(blocks).max(axis=-1, keepdims=True)\n        with np.errstate(divide=\"ignore\"):\n            id = np.where(d == 0, 0, 1 / d)\n        qs = np_roundf(blocks * id)\n        qs = (qs.astype(np.int8) + np.int8(1)).astype(np.uint8)\n        qs = qs.reshape((n_blocks, -1, 4, 32)) << np.array([0, 2, 4, 6], dtype=np.uint8).reshape((1, 1, 4, 1))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "MXFP4",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class MXFP4(__Quant, qtype=GGMLQuantizationType.MXFP4):\n    # e2m1 values (doubled)\n    # ref: https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf\n    kvalues = (0, 1, 2, 3, 4, 6, 8, 12, 0, -1, -2, -3, -4, -6, -8, -12)\n    @staticmethod\n    # see ggml_e8m0_to_fp32_half in ggml-impl.h\n    def e8m0_to_fp32_half(x: np.ndarray) -> np.ndarray:\n        bits = np.where(x < 2, np.uint32(0x00200000) << np.uint32(x), np.uint32(x - 1) << np.uint32(23))\n        return bits.view(np.float32)\n    @classmethod",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "IQ2_XXS",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class IQ2_XXS(__Quant, qtype=GGMLQuantizationType.IQ2_XXS):\n    ksigns: bytes = (\n        b\"\\x00\\x81\\x82\\x03\\x84\\x05\\x06\\x87\\x88\\x09\\x0a\\x8b\\x0c\\x8d\\x8e\\x0f\"\n        b\"\\x90\\x11\\x12\\x93\\x14\\x95\\x96\\x17\\x18\\x99\\x9a\\x1b\\x9c\\x1d\\x1e\\x9f\"\n        b\"\\xa0\\x21\\x22\\xa3\\x24\\xa5\\xa6\\x27\\x28\\xa9\\xaa\\x2b\\xac\\x2d\\x2e\\xaf\"\n        b\"\\x30\\xb1\\xb2\\x33\\xb4\\x35\\x36\\xb7\\xb8\\x39\\x3a\\xbb\\x3c\\xbd\\xbe\\x3f\"\n        b\"\\xc0\\x41\\x42\\xc3\\x44\\xc5\\xc6\\x47\\x48\\xc9\\xca\\x4b\\xcc\\x4d\\x4e\\xcf\"\n        b\"\\x50\\xd1\\xd2\\x53\\xd4\\x55\\x56\\xd7\\xd8\\x59\\x5a\\xdb\\x5c\\xdd\\xde\\x5f\"\n        b\"\\x60\\xe1\\xe2\\x63\\xe4\\x65\\x66\\xe7\\xe8\\x69\\x6a\\xeb\\x6c\\xed\\xee\\x6f\"\n        b\"\\xf0\\x71\\x72\\xf3\\x74\\xf5\\xf6\\x77\\x78\\xf9\\xfa\\x7b\\xfc\\x7d\\x7e\\xff\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "IQ2_XS",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class IQ2_XS(__Quant, qtype=GGMLQuantizationType.IQ2_XS):\n    # iq2xs_grid, but with each byte of the original packed in 2 bits,\n    # by mapping 0x08 to 0, 0x19 to 1, and 0x2b to 2.\n    grid_shape = (512, 8)\n    grid_map = (0x08, 0x19, 0x2b)\n    grid_hex = (\n        b\"00000200050008000a0011001400160019002000220025002800410044004600\"\n        b\"49005000520055005800610064008000820085008800910094009900a0000101\"\n        b\"04010601090110011201150118011a0121012401400142014501480151015401\"\n        b\"6001680181018401900100020202050208021102140220024102440250025502\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "IQ2_S",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class IQ2_S(__Quant, qtype=GGMLQuantizationType.IQ2_S):\n    # iq2s_grid, but with each byte of the original packed in 2 bits,\n    # by mapping 0x08 to 0, 0x19 to 1, and 0x2b to 2.\n    grid_shape = (1024, 8)\n    grid_map = (0x08, 0x19, 0x2b)\n    grid_hex = (\n        b\"00000200050008000a0011001400160019002000220025002800410044004600\"\n        b\"490050005200550058006100640066006900800082008500880091009400a000\"\n        b\"a500aa0001010401060109011001120115011801210124014001420145014801\"\n        b\"510154015601590160016501680181018401900192019501a101a40100020202\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "IQ3_XXS",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class IQ3_XXS(__Quant, qtype=GGMLQuantizationType.IQ3_XXS):\n    grid_shape = (256, 4)\n    grid_map = (0x04, 0x0c, 0x14, 0x1c, 0x24, 0x2c, 0x34, 0x3e)\n    grid_hex = (\n        b\"0000020004001100130017002000220031004200730075000101030110011201\"\n        b\"2101250130013201410154017001000202020402110220022202310233023702\"\n        b\"5102570275020103070310031203250370031304370444045704730475040105\"\n        b\"0705320552053506640610071407160743076107011003101010121021102310\"\n        b\"3010321034104710501000110211111120112211011203121012121221123012\"\n        b\"7212001302132013311346136613011405145014201524154615711505162217\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "IQ3_S",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class IQ3_S(__Quant, qtype=GGMLQuantizationType.IQ3_S):\n    grid_shape = (512, 4)\n    grid_map = (0x01, 0x03, 0x05, 0x07, 0x09, 0x0b, 0x0d, 0x0f)\n    grid_hex = (\n        b\"0000010002000500070010001100120014001600200021002500330040004200\"\n        b\"4500470051005300600062007100740077000001010102010401100111011501\"\n        b\"2001230127013101350144016101650172010002010205020702100213021602\"\n        b\"2102250230023402420245024702510253027002730203031103150320032203\"\n        b\"3103330336034403500352036703710375030004130417042104240432044004\"\n        b\"4304510470040205040520052205260533054105450547056605730506061106\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "IQ1_S",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class IQ1_S(__Quant, qtype=GGMLQuantizationType.IQ1_S):\n    # iq1s_grid, with each byte packed into 2 bits\n    # -1, 0, 1 <=> 0, 1, 2\n    grid_shape = (2048, 8)\n    grid_map = (-1, 0, 1)\n    grid_hex = (\n        b\"00000200050008000a00110015002000220028002a0045005100540056006500\"\n        b\"8000820088008a009500a000a200a800aa000401050111011401160119011a01\"\n        b\"2501410146014901520155015a0161016401660168018501910194019601a501\"\n        b\"0002020208020a0215022002220228022a024502510259026402690280028202\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "IQ1_M",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class IQ1_M(__Quant, qtype=GGMLQuantizationType.IQ1_M):\n    grid_shape = IQ1_S.grid_shape\n    grid_map = IQ1_S.grid_map\n    grid_hex = IQ1_S.grid_hex\n    delta = IQ1_S.delta\n    # Okay *this* type is weird. It's the only one which stores the f16 scales in multiple parts.\n    @classmethod\n    def dequantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        qs, rest = np.hsplit(blocks, [QK_K // 8])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "IQ4_NL",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class IQ4_NL(__Quant, qtype=GGMLQuantizationType.IQ4_NL):\n    kvalues = (-127, -104, -83, -65, -49, -35, -22, -10, 1, 13, 25, 38, 53, 69, 89, 113)\n    @classmethod\n    def dequantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        d, qs = np.hsplit(blocks, [2])\n        d = d.view(np.float16).astype(np.float32)\n        qs = qs.reshape((n_blocks, -1, 1, cls.block_size // 2)) >> np.array([0, 4], dtype=np.uint8).reshape((1, 1, 2, 1))\n        qs = (qs & np.uint8(0x0F)).reshape((n_blocks, -1, 1))\n        kvalues = np.array(cls.kvalues, dtype=np.int8).reshape(1, 1, 16)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "IQ4_XS",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "class IQ4_XS(__Quant, qtype=GGMLQuantizationType.IQ4_XS):\n    @classmethod\n    def dequantize_blocks(cls, blocks: np.ndarray) -> np.ndarray:\n        n_blocks = blocks.shape[0]\n        d, rest = np.hsplit(blocks, [2])\n        scales_h, rest = np.hsplit(rest, [2])\n        scales_l, qs = np.hsplit(rest, [QK_K // 64])\n        d = d.view(np.float16).astype(np.float32)\n        scales_h = scales_h.view(np.uint16)\n        scales_l = scales_l.reshape((n_blocks, -1, 1)) >> np.array([0, 4], dtype=np.uint8).reshape((1, 1, 2))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "quant_shape_to_byte_shape",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "def quant_shape_to_byte_shape(shape: Sequence[int], quant_type: GGMLQuantizationType) -> tuple[int, ...]:\n    block_size, type_size = GGML_QUANT_SIZES[quant_type]\n    if shape[-1] % block_size != 0:\n        raise ValueError(f\"Quantized tensor row size ({shape[-1]}) is not a multiple of {quant_type.name} block size ({block_size})\")\n    return (*shape[:-1], shape[-1] // block_size * type_size)\ndef quant_shape_from_byte_shape(shape: Sequence[int], quant_type: GGMLQuantizationType) -> tuple[int, ...]:\n    block_size, type_size = GGML_QUANT_SIZES[quant_type]\n    if shape[-1] % type_size != 0:\n        raise ValueError(f\"Quantized tensor bytes per row ({shape[-1]}) is not a multiple of {quant_type.name} type size ({type_size})\")\n    return (*shape[:-1], shape[-1] // type_size * block_size)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "quant_shape_from_byte_shape",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "def quant_shape_from_byte_shape(shape: Sequence[int], quant_type: GGMLQuantizationType) -> tuple[int, ...]:\n    block_size, type_size = GGML_QUANT_SIZES[quant_type]\n    if shape[-1] % type_size != 0:\n        raise ValueError(f\"Quantized tensor bytes per row ({shape[-1]}) is not a multiple of {quant_type.name} type size ({type_size})\")\n    return (*shape[:-1], shape[-1] // type_size * block_size)\n# This is faster than np.vectorize and np.apply_along_axis because it works on more than one row at a time\ndef _apply_over_grouped_rows(func: Callable[[np.ndarray], np.ndarray], arr: np.ndarray, otype: DTypeLike, oshape: tuple[int, ...]) -> np.ndarray:\n    rows = arr.reshape((-1, arr.shape[-1]))\n    osize = 1\n    for dim in oshape:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "np_roundf",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "def np_roundf(n: np.ndarray) -> np.ndarray:\n    a = abs(n)\n    floored = np.floor(a)\n    b = floored + np.floor(2 * (a - floored))\n    return np.sign(n) * b\nclass QuantError(Exception): ...\n_type_traits: dict[GGMLQuantizationType, type[__Quant]] = {}\ndef quantize(data: np.ndarray, qtype: GGMLQuantizationType) -> np.ndarray:\n    if qtype == GGMLQuantizationType.F32:\n        return data.astype(np.float32, copy=False)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "quantize",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "def quantize(data: np.ndarray, qtype: GGMLQuantizationType) -> np.ndarray:\n    if qtype == GGMLQuantizationType.F32:\n        return data.astype(np.float32, copy=False)\n    elif qtype == GGMLQuantizationType.F16:\n        return data.astype(np.float16, copy=False)\n    elif (q := _type_traits.get(qtype)) is not None:\n        return q.quantize(data)\n    else:\n        raise NotImplementedError(f\"Quantization for {qtype.name} is not yet implemented\")\ndef dequantize(data: np.ndarray, qtype: GGMLQuantizationType) -> np.ndarray:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "dequantize",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "peekOfCode": "def dequantize(data: np.ndarray, qtype: GGMLQuantizationType) -> np.ndarray:\n    if qtype == GGMLQuantizationType.F32:\n        return data.view(np.float32)\n    elif qtype == GGMLQuantizationType.F16:\n        return data.view(np.float16).astype(np.float32)\n    elif (q := _type_traits.get(qtype)) is not None:\n        return q.dequantize(data)\n    else:\n        raise NotImplementedError(f\"Dequantization for {qtype.name} is not yet implemented\")\nclass __Quant(ABC):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.quants",
        "documentation": {}
    },
    {
        "label": "TensorNameMap",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.tensor_mapping",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.tensor_mapping",
        "peekOfCode": "class TensorNameMap:\n    mappings_cfg: dict[MODEL_TENSOR, tuple[str, ...]] = {\n        # Token embeddings\n        MODEL_TENSOR.TOKEN_EMBD: (\n            \"gpt_neox.embed_in\",                         # gptneox\n            \"transformer.wte\",                           # gpt2 gpt-j mpt refact qwen dbrx jais exaone\n            \"transformer.word_embeddings\",               # falcon\n            \"word_embeddings\",                           # bloom\n            \"model.embed_tokens\",                        # llama-hf nemotron olmoe olmo2 rwkv6qwen2 glm4-0414 plamo2 granite-hybrid\n            \"embed_tokens\",                              # embeddinggemma",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.tensor_mapping",
        "documentation": {}
    },
    {
        "label": "get_tensor_name_map",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.tensor_mapping",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.tensor_mapping",
        "peekOfCode": "def get_tensor_name_map(arch: MODEL_ARCH, n_blocks: int) -> TensorNameMap:\n    return TensorNameMap(arch, n_blocks)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.tensor_mapping",
        "documentation": {}
    },
    {
        "label": "RemoteTensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "peekOfCode": "class RemoteTensor:\n    dtype: str\n    shape: tuple[int, ...]\n    offset_start: int\n    size: int\n    url: str\n    def data(self) -> bytearray:\n        # TODO: handle request errors (maybe with limited retries?)\n        # NOTE: using a bytearray, otherwise PyTorch complains the buffer is not writeable\n        data = bytearray(SafetensorRemote.get_data_by_range(url=self.url, start=self.offset_start, size=self.size))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "documentation": {}
    },
    {
        "label": "SafetensorRemote",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "peekOfCode": "class SafetensorRemote:\n    \"\"\"\n    Uility class to handle remote safetensor files.\n    This class is designed to work with Hugging Face model repositories.\n    Example (one model has single safetensor file, the other has multiple):\n        for model_id in [\"ngxson/TEST-Tiny-Llama4\", \"Qwen/Qwen2.5-7B-Instruct\"]:\n            tensors = SafetensorRemote.get_list_tensors_hf_model(model_id)\n            print(tensors)\n    Example reading tensor data:\n        tensors = SafetensorRemote.get_list_tensors_hf_model(model_id)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "documentation": {}
    },
    {
        "label": "LocalTensorRange",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "peekOfCode": "class LocalTensorRange:\n    filename: Path\n    offset: int\n    size: int\n@dataclass\nclass LocalTensor:\n    dtype: str\n    shape: tuple[int, ...]\n    data_range: LocalTensorRange\n    def mmap_bytes(self) -> np.ndarray:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "documentation": {}
    },
    {
        "label": "LocalTensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "peekOfCode": "class LocalTensor:\n    dtype: str\n    shape: tuple[int, ...]\n    data_range: LocalTensorRange\n    def mmap_bytes(self) -> np.ndarray:\n        return np.memmap(self.data_range.filename, mode='c', offset=self.data_range.offset, shape=self.data_range.size)\nclass SafetensorsLocal:\n    \"\"\"\n        Read a safetensors file from the local filesystem.\n        Custom parsing gives a bit more control over the memory usage.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "documentation": {}
    },
    {
        "label": "SafetensorsLocal",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "peekOfCode": "class SafetensorsLocal:\n    \"\"\"\n        Read a safetensors file from the local filesystem.\n        Custom parsing gives a bit more control over the memory usage.\n        The official safetensors library doesn't expose file ranges.\n    \"\"\"\n    tensors: dict[str, LocalTensor]\n    def __init__(self, filename: Path):\n        with open(filename, \"rb\") as f:\n            metadata_length = int.from_bytes(f.read(8), byteorder='little')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "documentation": {}
    },
    {
        "label": "fill_templated_filename",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "peekOfCode": "def fill_templated_filename(filename: str, output_type: str | None) -> str:\n    # Given a file name fill in any type templates e.g. 'some-model-name.{ftype}.gguf'\n    ftype_lowercase: str = output_type.lower() if output_type is not None else \"\"\n    ftype_uppercase: str = output_type.upper() if output_type is not None else \"\"\n    return filename.format(ftype_lowercase,\n                           outtype=ftype_lowercase, ftype=ftype_lowercase,\n                           OUTTYPE=ftype_uppercase, FTYPE=ftype_uppercase)\ndef model_weight_count_rounded_notation(model_params_count: int, min_digits: int = 2) -> str:\n    if model_params_count > 1e12 :\n        # Trillions Of Parameters",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "documentation": {}
    },
    {
        "label": "model_weight_count_rounded_notation",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "peekOfCode": "def model_weight_count_rounded_notation(model_params_count: int, min_digits: int = 2) -> str:\n    if model_params_count > 1e12 :\n        # Trillions Of Parameters\n        scaled_model_params = model_params_count * 1e-12\n        scale_suffix = \"T\"\n    elif model_params_count > 1e9 :\n        # Billions Of Parameters\n        scaled_model_params = model_params_count * 1e-9\n        scale_suffix = \"B\"\n    elif model_params_count > 1e6 :",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "documentation": {}
    },
    {
        "label": "size_label",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "peekOfCode": "def size_label(total_params: int, shared_params: int, expert_params: int, expert_count: int) -> str:\n    if expert_count > 0:\n        pretty_size = model_weight_count_rounded_notation(abs(shared_params) + abs(expert_params), min_digits=2)\n        size_class = f\"{expert_count}x{pretty_size}\"\n    else:\n        size_class = model_weight_count_rounded_notation(abs(total_params), min_digits=2)\n    return size_class\ndef naming_convention(model_name: str | None, base_name: str | None, finetune_string: str | None, version_string: str | None, size_label: str | None, output_type: str | None, model_type: Literal['vocab', 'LoRA'] | None = None) -> str:\n    # Reference: https://github.com/ggml-org/ggml/blob/master/docs/gguf.md#gguf-naming-convention\n    if base_name is not None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "documentation": {}
    },
    {
        "label": "naming_convention",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "peekOfCode": "def naming_convention(model_name: str | None, base_name: str | None, finetune_string: str | None, version_string: str | None, size_label: str | None, output_type: str | None, model_type: Literal['vocab', 'LoRA'] | None = None) -> str:\n    # Reference: https://github.com/ggml-org/ggml/blob/master/docs/gguf.md#gguf-naming-convention\n    if base_name is not None:\n        name = base_name.strip().replace(' ', '-').replace('/', '-')\n    elif model_name is not None:\n        name = model_name.strip().replace(' ', '-').replace('/', '-')\n    else:\n        name = \"ggml-model\"\n    parameters = f\"-{size_label}\" if size_label is not None else \"\"\n    finetune = f\"-{finetune_string.strip().replace(' ', '-')}\" if finetune_string is not None else \"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.utility",
        "documentation": {}
    },
    {
        "label": "SpecialVocab",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "class SpecialVocab:\n    merges: list[str]\n    add_special_token: dict[str, bool]\n    special_token_ids: dict[str, int]\n    chat_template: str | Sequence[Mapping[str, str]] | None\n    def __init__(\n        self, path: str | os.PathLike[str], load_merges: bool = False,\n        special_token_types: Iterable[str] | None = None,\n        n_vocab: int | None = None,\n    ):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "BaseVocab",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "class BaseVocab(Protocol):\n    tokenizer_model: ClassVar[str]\n    name: ClassVar[str]\n@runtime_checkable\nclass Vocab(BaseVocab, Protocol):\n    vocab_size: int\n    added_tokens_dict: dict[str, int]\n    added_tokens_list: list[str]\n    fname_tokenizer: Path\n    def __init__(self, base_path: Path): ...",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "Vocab",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "class Vocab(BaseVocab, Protocol):\n    vocab_size: int\n    added_tokens_dict: dict[str, int]\n    added_tokens_list: list[str]\n    fname_tokenizer: Path\n    def __init__(self, base_path: Path): ...\n    def all_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]: ...\nclass NoVocab(BaseVocab):\n    tokenizer_model = \"no_vocab\"\n    name = \"no_vocab\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "NoVocab",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "class NoVocab(BaseVocab):\n    tokenizer_model = \"no_vocab\"\n    name = \"no_vocab\"\n    def __repr__(self) -> str:\n        return \"<NoVocab for a model without integrated vocabulary>\"\nclass BpeVocab(Vocab):\n    tokenizer_model = \"gpt2\"\n    name = \"bpe\"\n    def __init__(self, base_path: Path):\n        added_tokens: dict[str, int] = {}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "BpeVocab",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "class BpeVocab(Vocab):\n    tokenizer_model = \"gpt2\"\n    name = \"bpe\"\n    def __init__(self, base_path: Path):\n        added_tokens: dict[str, int] = {}\n        if (fname_tokenizer := base_path / 'vocab.json').exists():\n            # \"slow\" tokenizer\n            with open(fname_tokenizer, encoding=\"utf-8\") as f:\n                self.vocab = json.load(f)\n            try:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "SentencePieceVocab",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "class SentencePieceVocab(Vocab):\n    tokenizer_model = \"llama\"\n    name = \"spm\"\n    def __init__(self, base_path: Path):\n        if SentencePieceProcessor is None:\n            raise RuntimeError(\"sentencepiece is not installed\")\n        added_tokens: dict[str, int] = {}\n        if (fname_tokenizer := base_path / 'tokenizer.model').exists():\n            # normal location\n            try:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "LlamaHfVocab",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "class LlamaHfVocab(Vocab):\n    tokenizer_model = \"llama\"\n    name = \"hfft\"\n    def __init__(self, base_path: Path):\n        fname_tokenizer = base_path / 'tokenizer.json'\n        # if this fails, FileNotFoundError propagates to caller\n        with open(fname_tokenizer, encoding='utf-8') as f:\n            tokenizer_json = json.load(f)\n        # pre-check so we know if we need transformers\n        tokenizer_model: dict[str, Any] = tokenizer_json['model']",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "MistralTokenizerType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "class MistralTokenizerType(str, Enum):\n    spm = \"spm\"\n    tekken = \"tekken\"\n# Copied from Transformers (Apache 2.0)\n# https://github.com/huggingface/transformers/blob/main/src/transformers/convert_slow_tokenizer.py#L1544\ndef bytes_to_unicode() -> dict[int, str]:\n    \"\"\"\n    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n    characters the bpe code barfs on.\n    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "MistralVocab",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "class MistralVocab(Vocab):\n    tokenizer_model = \"mistral\"\n    name = \"mistral\"\n    added_tokens_dict: dict[str, int] = {}\n    added_tokens_list: list[str] = []\n    def __init__(self, base_path: Path):\n        if not _mistral_common_installed:\n            raise ImportError(\n                \"To use MistralVocab, please install the `mistral-common` package. \"\n                \"You can install it with `pip install mistral-common`.\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "bytes_to_unicode",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "def bytes_to_unicode() -> dict[int, str]:\n    \"\"\"\n    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n    characters the bpe code barfs on.\n    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n    tables between utf-8 bytes and unicode strings.\n    \"\"\"\n    bs = (",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SpecialVocab:\n    merges: list[str]\n    add_special_token: dict[str, bool]\n    special_token_ids: dict[str, int]\n    chat_template: str | Sequence[Mapping[str, str]] | None\n    def __init__(\n        self, path: str | os.PathLike[str], load_merges: bool = False,\n        special_token_types: Iterable[str] | None = None,\n        n_vocab: int | None = None,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.gguf.vocab",
        "documentation": {}
    },
    {
        "label": "TestMetadataMethod",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_metadata",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_metadata",
        "peekOfCode": "class TestMetadataMethod(unittest.TestCase):\n    def test_id_to_title(self):\n        self.assertEqual(gguf.Metadata.id_to_title(\"Mixtral-8x7B-Instruct-v0.1\"), \"Mixtral 8x7B Instruct v0.1\")\n        self.assertEqual(gguf.Metadata.id_to_title(\"Meta-Llama-3-8B\"), \"Meta Llama 3 8B\")\n        self.assertEqual(gguf.Metadata.id_to_title(\"hermes-2-pro-llama-3-8b-DPO\"), \"Hermes 2 Pro Llama 3 8b DPO\")\n    def test_get_model_id_components(self):\n        # This is the basic standard form with organization marker\n        self.assertEqual(gguf.Metadata.get_model_id_components(\"Mistral/Mixtral-8x7B-Instruct-v0.1\"),\n                         ('Mixtral-8x7B-Instruct-v0.1', \"Mistral\", 'Mixtral', 'Instruct', 'v0.1', '8x7B'))\n        # Similar to basic standard form but without organization marker",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_metadata",
        "documentation": {}
    },
    {
        "label": "ggml_init_params",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "peekOfCode": "class ggml_init_params(ctypes.Structure):\n    _fields_ = [\n        (\"mem_size\", ctypes.c_size_t),\n        (\"mem_buffer\", ctypes.c_void_p),\n        (\"no_alloc\", ctypes.c_bool),\n    ]\nclass GGMLQuants:\n    libggml: ctypes.CDLL\n    def __init__(self, libggml: Path):\n        self.libggml = ctypes.CDLL(str(libggml))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "documentation": {}
    },
    {
        "label": "GGMLQuants",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "peekOfCode": "class GGMLQuants:\n    libggml: ctypes.CDLL\n    def __init__(self, libggml: Path):\n        self.libggml = ctypes.CDLL(str(libggml))\n        self.libggml.ggml_quantize_chunk.restype = ctypes.c_size_t\n        # enum ggml_type   type,\n        #    const float * src,\n        #           void * dst,\n        #        int64_t   start,\n        #        int64_t   nrows,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "documentation": {}
    },
    {
        "label": "compare_tensors",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "peekOfCode": "def compare_tensors(t1: np.ndarray, t2: np.ndarray, qtype: GGMLQuantizationType) -> bool:\n    same = np.array_equal(t1, t2)\n    if same:\n        return True\n    else:\n        block_size, type_size = gguf.GGML_QUANT_SIZES[qtype]\n        if t1.dtype == np.float32:\n            t1 = t1.reshape((-1, block_size))\n            t2 = t2.reshape((-1, block_size))\n        else:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "documentation": {}
    },
    {
        "label": "do_test",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "peekOfCode": "def do_test(libggml_path: Path, quick: bool = False, user_type: GGMLQuantizationType | None = None):\n    ggml_quants = GGMLQuants(libggml_path)\n    np.set_printoptions(precision=None, threshold=(4 * 256) + 1, formatter={\"int\": lambda n: \"0x%02X\" % n})\n    r = np.random.randn(8, 1024, 1024).astype(np.float32, copy=False)\n    # test zero blocks\n    r[0, 0, :] = 0\n    ## Maybe test infinities? (can make NANs, not really useful in practice)\n    # r[0, 1, 0] = np.inf\n    # r[0, 2, 0] = -np.inf\n    # r[0, 3, 0] = np.inf",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "peekOfCode": "logger = logging.getLogger(\"test-quants\")\nc_float_p = ctypes.POINTER(ctypes.c_float)\nclass ggml_init_params(ctypes.Structure):\n    _fields_ = [\n        (\"mem_size\", ctypes.c_size_t),\n        (\"mem_buffer\", ctypes.c_void_p),\n        (\"no_alloc\", ctypes.c_bool),\n    ]\nclass GGMLQuants:\n    libggml: ctypes.CDLL",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "documentation": {}
    },
    {
        "label": "c_float_p",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "peekOfCode": "c_float_p = ctypes.POINTER(ctypes.c_float)\nclass ggml_init_params(ctypes.Structure):\n    _fields_ = [\n        (\"mem_size\", ctypes.c_size_t),\n        (\"mem_buffer\", ctypes.c_void_p),\n        (\"no_alloc\", ctypes.c_bool),\n    ]\nclass GGMLQuants:\n    libggml: ctypes.CDLL\n    def __init__(self, libggml: Path):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.gguf-py.tests.test_quants",
        "documentation": {}
    },
    {
        "label": "LineNumberArea",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "peekOfCode": "class LineNumberArea(QWidget):\n    def __init__(self, editor):\n        super().__init__(editor)\n        self.code_editor = editor\n    def sizeHint(self):\n        return QSize(self.code_editor.line_number_area_width(), 0)\n    def paintEvent(self, event):\n        self.code_editor.line_number_area_paint_event(event)\nclass CodeEditor(QPlainTextEdit):\n    def __init__(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "documentation": {}
    },
    {
        "label": "CodeEditor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "peekOfCode": "class CodeEditor(QPlainTextEdit):\n    def __init__(self):\n        super().__init__()\n        self.line_number_area = LineNumberArea(self)\n        self.blockCountChanged.connect(self.update_line_number_area_width)\n        self.updateRequest.connect(self.update_line_number_area)\n        self.cursorPositionChanged.connect(self.highlight_current_line)\n        self.update_line_number_area_width(0)\n        self.highlight_current_line()\n    def line_number_area_width(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "documentation": {}
    },
    {
        "label": "JinjaTester",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "peekOfCode": "class JinjaTester(QMainWindow):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"Jinja Template Tester\")\n        self.resize(1200, 800)\n        central = QWidget()\n        main_layout = QVBoxLayout(central)\n        # -------- Top input area --------\n        input_layout = QHBoxLayout()\n        # Template editor with label",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "documentation": {}
    },
    {
        "label": "format_template_content",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "peekOfCode": "def format_template_content(template_content):\n    \"\"\"Format the Jinja template content using Jinja2's lexer.\"\"\"\n    if not template_content.strip():\n        return template_content\n    env = ImmutableSandboxedEnvironment()\n    tc_rstrip = template_content.rstrip()\n    tokens = list(env.lex(tc_rstrip))\n    result = \"\"\n    indent_level = 0\n    i = 0",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.jinja.jinja-tester",
        "documentation": {}
    },
    {
        "label": "run_cmd",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "peekOfCode": "def run_cmd(cmd):\n    p = subprocess.run(cmd, text = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT)\n    sys.stdout.write(p.stdout)\n    assert(p.returncode == 0)\n@pytest.mark.dependency()\ndef test_install():\n    run_cmd(['adb', 'push', 'llama.cpp', f'{tmp_path}'])\n    run_cmd(['adb', 'shell', f'chmod 755 {bin_path}/*'])\n## Basic cli tests\ndef run_llama_cli(dev, opts):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "documentation": {}
    },
    {
        "label": "test_install",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "peekOfCode": "def test_install():\n    run_cmd(['adb', 'push', 'llama.cpp', f'{tmp_path}'])\n    run_cmd(['adb', 'shell', f'chmod 755 {bin_path}/*'])\n## Basic cli tests\ndef run_llama_cli(dev, opts):\n    prompt='what is the most popular cookie in the world?\\nPlease provide a very brief bullet point summary.\\nBegin your answer with **BEGIN**.'\n    opts = '--batch-size 128 -n 128 -no-cnv --seed 42 ' + opts\n    run_cmd(['adb', 'shell', f'{cli_pref}/llama-cli -m {model} --device {dev} -ngl 99 -t 4 {opts} -p \"{prompt}\"'])\n@pytest.mark.dependency(depends=['test_install'])\ndef test_llama_cli_cpu():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "documentation": {}
    },
    {
        "label": "run_llama_cli",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "peekOfCode": "def run_llama_cli(dev, opts):\n    prompt='what is the most popular cookie in the world?\\nPlease provide a very brief bullet point summary.\\nBegin your answer with **BEGIN**.'\n    opts = '--batch-size 128 -n 128 -no-cnv --seed 42 ' + opts\n    run_cmd(['adb', 'shell', f'{cli_pref}/llama-cli -m {model} --device {dev} -ngl 99 -t 4 {opts} -p \"{prompt}\"'])\n@pytest.mark.dependency(depends=['test_install'])\ndef test_llama_cli_cpu():\n    run_llama_cli('none', '-ctk q8_0 -ctv q8_0 -fa on')\n@pytest.mark.dependency(depends=['test_install'])\ndef test_llama_cli_gpu():\n    run_llama_cli('GPUOpenCL', '-fa on')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "documentation": {}
    },
    {
        "label": "test_llama_cli_cpu",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "peekOfCode": "def test_llama_cli_cpu():\n    run_llama_cli('none', '-ctk q8_0 -ctv q8_0 -fa on')\n@pytest.mark.dependency(depends=['test_install'])\ndef test_llama_cli_gpu():\n    run_llama_cli('GPUOpenCL', '-fa on')\n@pytest.mark.dependency(depends=['test_install'])\ndef test_llama_cli_npu():\n    run_llama_cli('HTP0', '-ctk q8_0 -ctv q8_0 -fa on')\n## Basic bench tests\ndef run_llama_bench(dev):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "documentation": {}
    },
    {
        "label": "test_llama_cli_gpu",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "peekOfCode": "def test_llama_cli_gpu():\n    run_llama_cli('GPUOpenCL', '-fa on')\n@pytest.mark.dependency(depends=['test_install'])\ndef test_llama_cli_npu():\n    run_llama_cli('HTP0', '-ctk q8_0 -ctv q8_0 -fa on')\n## Basic bench tests\ndef run_llama_bench(dev):\n    run_cmd(['adb', 'shell', f'{cli_pref}/llama-bench -m {model} --device {dev} -ngl 99 --batch-size 128 -t 4 -p 128 -n 32'])\n@pytest.mark.dependency(depends=['test_install'])\ndef test_llama_bench_cpu():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "documentation": {}
    },
    {
        "label": "test_llama_cli_npu",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "peekOfCode": "def test_llama_cli_npu():\n    run_llama_cli('HTP0', '-ctk q8_0 -ctv q8_0 -fa on')\n## Basic bench tests\ndef run_llama_bench(dev):\n    run_cmd(['adb', 'shell', f'{cli_pref}/llama-bench -m {model} --device {dev} -ngl 99 --batch-size 128 -t 4 -p 128 -n 32'])\n@pytest.mark.dependency(depends=['test_install'])\ndef test_llama_bench_cpu():\n    run_llama_bench('none')\ndef test_llama_bench_gpu():\n    run_llama_bench('GPUOpenCL')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "documentation": {}
    },
    {
        "label": "run_llama_bench",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "peekOfCode": "def run_llama_bench(dev):\n    run_cmd(['adb', 'shell', f'{cli_pref}/llama-bench -m {model} --device {dev} -ngl 99 --batch-size 128 -t 4 -p 128 -n 32'])\n@pytest.mark.dependency(depends=['test_install'])\ndef test_llama_bench_cpu():\n    run_llama_bench('none')\ndef test_llama_bench_gpu():\n    run_llama_bench('GPUOpenCL')\ndef test_llama_bench_npu():\n    run_llama_bench('HTP0')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "documentation": {}
    },
    {
        "label": "test_llama_bench_cpu",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "peekOfCode": "def test_llama_bench_cpu():\n    run_llama_bench('none')\ndef test_llama_bench_gpu():\n    run_llama_bench('GPUOpenCL')\ndef test_llama_bench_npu():\n    run_llama_bench('HTP0')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "documentation": {}
    },
    {
        "label": "test_llama_bench_gpu",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "peekOfCode": "def test_llama_bench_gpu():\n    run_llama_bench('GPUOpenCL')\ndef test_llama_bench_npu():\n    run_llama_bench('HTP0')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "documentation": {}
    },
    {
        "label": "test_llama_bench_npu",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "peekOfCode": "def test_llama_bench_npu():\n    run_llama_bench('HTP0')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.snapdragon.qdc.tests.test_bench",
        "documentation": {}
    },
    {
        "label": "LlamaBenchData",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "class LlamaBenchData:\n    repo: Optional[git.Repo]\n    build_len_min: int\n    build_len_max: int\n    build_len: int = 8\n    builds: list[str] = []\n    tool: str = \"llama-bench\"  # Tool type: \"llama-bench\" or \"test-backend-ops\"\n    def __init__(self, tool: str = \"llama-bench\"):\n        self.tool = tool\n        try:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "LlamaBenchDataSQLite3",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "class LlamaBenchDataSQLite3(LlamaBenchData):\n    connection: Optional[sqlite3.Connection] = None\n    cursor: sqlite3.Cursor\n    table_name: str\n    def __init__(self, tool: str = \"llama-bench\"):\n        super().__init__(tool)\n        if self.connection is None:\n            self.connection = sqlite3.connect(\":memory:\")\n            self.cursor = self.connection.cursor()\n            # Set table name and schema based on tool",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "LlamaBenchDataSQLite3File",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "class LlamaBenchDataSQLite3File(LlamaBenchDataSQLite3):\n    def __init__(self, data_file: str, tool: Any):\n        self.connection = sqlite3.connect(data_file)\n        self.cursor = self.connection.cursor()\n        # Check which table exists in the database\n        tables = self.cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n        table_names = [table[0] for table in tables]\n        # Tool selection logic\n        if tool is None:\n            if \"llama_bench\" in table_names:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "LlamaBenchDataJSONL",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "class LlamaBenchDataJSONL(LlamaBenchDataSQLite3):\n    def __init__(self, data_file: str, tool: str = \"llama-bench\"):\n        super().__init__(tool)\n        # Get the appropriate field list based on tool\n        db_fields = LLAMA_BENCH_DB_FIELDS if tool == \"llama-bench\" else TEST_BACKEND_OPS_DB_FIELDS\n        with open(data_file, \"r\", encoding=\"utf-8\") as fp:\n            for i, line in enumerate(fp):\n                parsed = json.loads(line)\n                for k in parsed.keys() - set(db_fields):\n                    del parsed[k]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "LlamaBenchDataJSON",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "class LlamaBenchDataJSON(LlamaBenchDataSQLite3):\n    def __init__(self, data_files: list[str], tool: str = \"llama-bench\"):\n        super().__init__(tool)\n        # Get the appropriate field list based on tool\n        db_fields = LLAMA_BENCH_DB_FIELDS if tool == \"llama-bench\" else TEST_BACKEND_OPS_DB_FIELDS\n        for data_file in data_files:\n            with open(data_file, \"r\", encoding=\"utf-8\") as fp:\n                parsed = json.load(fp)\n                for i, entry in enumerate(parsed):\n                    for k in entry.keys() - set(db_fields):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "LlamaBenchDataCSV",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "class LlamaBenchDataCSV(LlamaBenchDataSQLite3):\n    def __init__(self, data_files: list[str], tool: str = \"llama-bench\"):\n        super().__init__(tool)\n        # Get the appropriate field list based on tool\n        db_fields = LLAMA_BENCH_DB_FIELDS if tool == \"llama-bench\" else TEST_BACKEND_OPS_DB_FIELDS\n        for data_file in data_files:\n            with open(data_file, \"r\", encoding=\"utf-8\") as fp:\n                for i, parsed in enumerate(csv.DictReader(fp)):\n                    keys = set(parsed.keys())\n                    for k in keys - set(db_fields):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "format_flops",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "def format_flops(flops_value: float) -> str:\n    \"\"\"Format FLOPS values with appropriate units for better readability.\"\"\"\n    if flops_value == 0:\n        return \"0.00\"\n    # Define unit thresholds and names\n    units = [\n        (1e12, \"T\"),   # TeraFLOPS\n        (1e9, \"G\"),    # GigaFLOPS\n        (1e6, \"M\"),    # MegaFLOPS\n        (1e3, \"k\"),    # kiloFLOPS",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "format_flops_for_table",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "def format_flops_for_table(flops_value: float, target_unit: str) -> str:\n    \"\"\"Format FLOPS values for table display without unit suffix (since unit is in header).\"\"\"\n    if flops_value == 0:\n        return \"0.00\"\n    # Define unit thresholds based on target unit\n    unit_divisors = {\n        \"TFLOPS\": 1e12,\n        \"GFLOPS\": 1e9,\n        \"MFLOPS\": 1e6,\n        \"kFLOPS\": 1e3,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "get_flops_unit_name",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "def get_flops_unit_name(flops_values: list) -> str:\n    \"\"\"Determine the best FLOPS unit name based on the magnitude of values.\"\"\"\n    if not flops_values or all(v == 0 for v in flops_values):\n        return \"FLOPS\"\n    # Find the maximum absolute value to determine appropriate unit\n    max_flops = max(abs(v) for v in flops_values if v != 0)\n    if max_flops >= 1e12:\n        return \"TFLOPS\"\n    elif max_flops >= 1e9:\n        return \"GFLOPS\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "logger = logging.getLogger(\"compare-llama-bench\")\n# All llama-bench SQL fields\nLLAMA_BENCH_DB_FIELDS = [\n    \"build_commit\", \"build_number\", \"cpu_info\",       \"gpu_info\",   \"backends\",     \"model_filename\",\n    \"model_type\",   \"model_size\",   \"model_n_params\", \"n_batch\",    \"n_ubatch\",     \"n_threads\",\n    \"cpu_mask\",     \"cpu_strict\",   \"poll\",           \"type_k\",     \"type_v\",       \"n_gpu_layers\",\n    \"split_mode\",   \"main_gpu\",     \"no_kv_offload\",  \"flash_attn\", \"tensor_split\", \"tensor_buft_overrides\",\n    \"use_mmap\",     \"embeddings\",   \"no_op_offload\",  \"n_prompt\",   \"n_gen\",        \"n_depth\",\n    \"test_time\",    \"avg_ns\",       \"stddev_ns\",      \"avg_ts\",     \"stddev_ts\",\n]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "LLAMA_BENCH_DB_FIELDS",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "LLAMA_BENCH_DB_FIELDS = [\n    \"build_commit\", \"build_number\", \"cpu_info\",       \"gpu_info\",   \"backends\",     \"model_filename\",\n    \"model_type\",   \"model_size\",   \"model_n_params\", \"n_batch\",    \"n_ubatch\",     \"n_threads\",\n    \"cpu_mask\",     \"cpu_strict\",   \"poll\",           \"type_k\",     \"type_v\",       \"n_gpu_layers\",\n    \"split_mode\",   \"main_gpu\",     \"no_kv_offload\",  \"flash_attn\", \"tensor_split\", \"tensor_buft_overrides\",\n    \"use_mmap\",     \"embeddings\",   \"no_op_offload\",  \"n_prompt\",   \"n_gen\",        \"n_depth\",\n    \"test_time\",    \"avg_ns\",       \"stddev_ns\",      \"avg_ts\",     \"stddev_ts\",\n]\nLLAMA_BENCH_DB_TYPES = [\n    \"TEXT\",    \"INTEGER\", \"TEXT\",    \"TEXT\",    \"TEXT\",    \"TEXT\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "LLAMA_BENCH_DB_TYPES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "LLAMA_BENCH_DB_TYPES = [\n    \"TEXT\",    \"INTEGER\", \"TEXT\",    \"TEXT\",    \"TEXT\",    \"TEXT\",\n    \"TEXT\",    \"INTEGER\", \"INTEGER\", \"INTEGER\", \"INTEGER\", \"INTEGER\",\n    \"TEXT\",    \"INTEGER\", \"INTEGER\", \"TEXT\",    \"TEXT\",    \"INTEGER\",\n    \"TEXT\",    \"INTEGER\", \"INTEGER\", \"INTEGER\", \"TEXT\",    \"TEXT\",\n    \"INTEGER\", \"INTEGER\", \"INTEGER\", \"INTEGER\", \"INTEGER\", \"INTEGER\",\n    \"TEXT\",    \"INTEGER\", \"INTEGER\", \"REAL\",    \"REAL\",\n]\n# All test-backend-ops SQL fields\nTEST_BACKEND_OPS_DB_FIELDS = [",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "TEST_BACKEND_OPS_DB_FIELDS",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "TEST_BACKEND_OPS_DB_FIELDS = [\n    \"test_time\", \"build_commit\", \"backend_name\",  \"op_name\", \"op_params\", \"test_mode\",\n    \"supported\", \"passed\",       \"error_message\", \"time_us\", \"flops\",     \"bandwidth_gb_s\",\n    \"memory_kb\", \"n_runs\"\n]\nTEST_BACKEND_OPS_DB_TYPES = [\n    \"TEXT\",    \"TEXT\",    \"TEXT\", \"TEXT\", \"TEXT\", \"TEXT\",\n    \"INTEGER\", \"INTEGER\", \"TEXT\", \"REAL\", \"REAL\", \"REAL\",\n    \"INTEGER\", \"INTEGER\"\n]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "TEST_BACKEND_OPS_DB_TYPES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "TEST_BACKEND_OPS_DB_TYPES = [\n    \"TEXT\",    \"TEXT\",    \"TEXT\", \"TEXT\", \"TEXT\", \"TEXT\",\n    \"INTEGER\", \"INTEGER\", \"TEXT\", \"REAL\", \"REAL\", \"REAL\",\n    \"INTEGER\", \"INTEGER\"\n]\nassert len(LLAMA_BENCH_DB_FIELDS) == len(LLAMA_BENCH_DB_TYPES)\nassert len(TEST_BACKEND_OPS_DB_FIELDS) == len(TEST_BACKEND_OPS_DB_TYPES)\n# Properties by which to differentiate results per commit for llama-bench:\nLLAMA_BENCH_KEY_PROPERTIES = [\n    \"cpu_info\", \"gpu_info\", \"backends\", \"n_gpu_layers\", \"tensor_buft_overrides\", \"model_filename\", \"model_type\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "LLAMA_BENCH_KEY_PROPERTIES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "LLAMA_BENCH_KEY_PROPERTIES = [\n    \"cpu_info\", \"gpu_info\", \"backends\", \"n_gpu_layers\", \"tensor_buft_overrides\", \"model_filename\", \"model_type\",\n    \"n_batch\", \"n_ubatch\", \"embeddings\", \"cpu_mask\", \"cpu_strict\", \"poll\", \"n_threads\", \"type_k\", \"type_v\",\n    \"use_mmap\", \"no_kv_offload\", \"split_mode\", \"main_gpu\", \"tensor_split\", \"flash_attn\", \"n_prompt\", \"n_gen\", \"n_depth\"\n]\n# Properties by which to differentiate results per commit for test-backend-ops:\nTEST_BACKEND_OPS_KEY_PROPERTIES = [\n    \"backend_name\", \"op_name\", \"op_params\", \"test_mode\"\n]\n# Properties that are boolean and are converted to Yes/No for the table:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "TEST_BACKEND_OPS_KEY_PROPERTIES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "TEST_BACKEND_OPS_KEY_PROPERTIES = [\n    \"backend_name\", \"op_name\", \"op_params\", \"test_mode\"\n]\n# Properties that are boolean and are converted to Yes/No for the table:\nLLAMA_BENCH_BOOL_PROPERTIES = [\"embeddings\", \"cpu_strict\", \"use_mmap\", \"no_kv_offload\", \"flash_attn\"]\nTEST_BACKEND_OPS_BOOL_PROPERTIES = [\"supported\", \"passed\"]\n# Header names for the table (llama-bench):\nLLAMA_BENCH_PRETTY_NAMES = {\n    \"cpu_info\": \"CPU\", \"gpu_info\": \"GPU\", \"backends\": \"Backends\", \"n_gpu_layers\": \"GPU layers\",\n    \"tensor_buft_overrides\": \"Tensor overrides\", \"model_filename\": \"File\", \"model_type\": \"Model\", \"model_size\": \"Model size [GiB]\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "LLAMA_BENCH_BOOL_PROPERTIES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "LLAMA_BENCH_BOOL_PROPERTIES = [\"embeddings\", \"cpu_strict\", \"use_mmap\", \"no_kv_offload\", \"flash_attn\"]\nTEST_BACKEND_OPS_BOOL_PROPERTIES = [\"supported\", \"passed\"]\n# Header names for the table (llama-bench):\nLLAMA_BENCH_PRETTY_NAMES = {\n    \"cpu_info\": \"CPU\", \"gpu_info\": \"GPU\", \"backends\": \"Backends\", \"n_gpu_layers\": \"GPU layers\",\n    \"tensor_buft_overrides\": \"Tensor overrides\", \"model_filename\": \"File\", \"model_type\": \"Model\", \"model_size\": \"Model size [GiB]\",\n    \"model_n_params\": \"Num. of par.\", \"n_batch\": \"Batch size\", \"n_ubatch\": \"Microbatch size\", \"embeddings\": \"Embeddings\",\n    \"cpu_mask\": \"CPU mask\", \"cpu_strict\": \"CPU strict\", \"poll\": \"Poll\", \"n_threads\": \"Threads\", \"type_k\": \"K type\", \"type_v\": \"V type\",\n    \"use_mmap\": \"Use mmap\", \"no_kv_offload\": \"NKVO\", \"split_mode\": \"Split mode\", \"main_gpu\": \"Main GPU\", \"tensor_split\": \"Tensor split\",\n    \"flash_attn\": \"FlashAttention\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "TEST_BACKEND_OPS_BOOL_PROPERTIES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "TEST_BACKEND_OPS_BOOL_PROPERTIES = [\"supported\", \"passed\"]\n# Header names for the table (llama-bench):\nLLAMA_BENCH_PRETTY_NAMES = {\n    \"cpu_info\": \"CPU\", \"gpu_info\": \"GPU\", \"backends\": \"Backends\", \"n_gpu_layers\": \"GPU layers\",\n    \"tensor_buft_overrides\": \"Tensor overrides\", \"model_filename\": \"File\", \"model_type\": \"Model\", \"model_size\": \"Model size [GiB]\",\n    \"model_n_params\": \"Num. of par.\", \"n_batch\": \"Batch size\", \"n_ubatch\": \"Microbatch size\", \"embeddings\": \"Embeddings\",\n    \"cpu_mask\": \"CPU mask\", \"cpu_strict\": \"CPU strict\", \"poll\": \"Poll\", \"n_threads\": \"Threads\", \"type_k\": \"K type\", \"type_v\": \"V type\",\n    \"use_mmap\": \"Use mmap\", \"no_kv_offload\": \"NKVO\", \"split_mode\": \"Split mode\", \"main_gpu\": \"Main GPU\", \"tensor_split\": \"Tensor split\",\n    \"flash_attn\": \"FlashAttention\",\n}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "LLAMA_BENCH_PRETTY_NAMES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "LLAMA_BENCH_PRETTY_NAMES = {\n    \"cpu_info\": \"CPU\", \"gpu_info\": \"GPU\", \"backends\": \"Backends\", \"n_gpu_layers\": \"GPU layers\",\n    \"tensor_buft_overrides\": \"Tensor overrides\", \"model_filename\": \"File\", \"model_type\": \"Model\", \"model_size\": \"Model size [GiB]\",\n    \"model_n_params\": \"Num. of par.\", \"n_batch\": \"Batch size\", \"n_ubatch\": \"Microbatch size\", \"embeddings\": \"Embeddings\",\n    \"cpu_mask\": \"CPU mask\", \"cpu_strict\": \"CPU strict\", \"poll\": \"Poll\", \"n_threads\": \"Threads\", \"type_k\": \"K type\", \"type_v\": \"V type\",\n    \"use_mmap\": \"Use mmap\", \"no_kv_offload\": \"NKVO\", \"split_mode\": \"Split mode\", \"main_gpu\": \"Main GPU\", \"tensor_split\": \"Tensor split\",\n    \"flash_attn\": \"FlashAttention\",\n}\n# Header names for the table (test-backend-ops):\nTEST_BACKEND_OPS_PRETTY_NAMES = {",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "TEST_BACKEND_OPS_PRETTY_NAMES",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "TEST_BACKEND_OPS_PRETTY_NAMES = {\n    \"backend_name\": \"Backend\", \"op_name\": \"GGML op\", \"op_params\": \"Op parameters\", \"test_mode\": \"Mode\",\n    \"supported\": \"Supported\", \"passed\": \"Passed\", \"error_message\": \"Error\",\n    \"flops\": \"FLOPS\", \"bandwidth_gb_s\": \"Bandwidth (GB/s)\", \"memory_kb\": \"Memory (KB)\", \"n_runs\": \"Runs\"\n}\nDEFAULT_SHOW_LLAMA_BENCH = [\"model_type\"]  # Always show these properties by default.\nDEFAULT_HIDE_LLAMA_BENCH = [\"model_filename\"]  # Always hide these properties by default.\nDEFAULT_SHOW_TEST_BACKEND_OPS = [\"backend_name\", \"op_name\"]  # Always show these properties by default.\nDEFAULT_HIDE_TEST_BACKEND_OPS = [\"error_message\"]  # Always hide these properties by default.\nGPU_NAME_STRIP = [\"NVIDIA GeForce \", \"Tesla \", \"AMD Radeon \", \"AMD Instinct \"]  # Strip prefixes for smaller tables.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SHOW_LLAMA_BENCH",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "DEFAULT_SHOW_LLAMA_BENCH = [\"model_type\"]  # Always show these properties by default.\nDEFAULT_HIDE_LLAMA_BENCH = [\"model_filename\"]  # Always hide these properties by default.\nDEFAULT_SHOW_TEST_BACKEND_OPS = [\"backend_name\", \"op_name\"]  # Always show these properties by default.\nDEFAULT_HIDE_TEST_BACKEND_OPS = [\"error_message\"]  # Always hide these properties by default.\nGPU_NAME_STRIP = [\"NVIDIA GeForce \", \"Tesla \", \"AMD Radeon \", \"AMD Instinct \"]  # Strip prefixes for smaller tables.\nMODEL_SUFFIX_REPLACE = {\" - Small\": \"_S\", \" - Medium\": \"_M\", \" - Large\": \"_L\"}\nDESCRIPTION = \"\"\"Creates tables from llama-bench or test-backend-ops data written to multiple JSON/CSV files, a single JSONL file or SQLite database. Example usage (Linux):\nFor llama-bench:\n$ git checkout master\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HIDE_LLAMA_BENCH",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "DEFAULT_HIDE_LLAMA_BENCH = [\"model_filename\"]  # Always hide these properties by default.\nDEFAULT_SHOW_TEST_BACKEND_OPS = [\"backend_name\", \"op_name\"]  # Always show these properties by default.\nDEFAULT_HIDE_TEST_BACKEND_OPS = [\"error_message\"]  # Always hide these properties by default.\nGPU_NAME_STRIP = [\"NVIDIA GeForce \", \"Tesla \", \"AMD Radeon \", \"AMD Instinct \"]  # Strip prefixes for smaller tables.\nMODEL_SUFFIX_REPLACE = {\" - Small\": \"_S\", \" - Medium\": \"_M\", \" - Large\": \"_L\"}\nDESCRIPTION = \"\"\"Creates tables from llama-bench or test-backend-ops data written to multiple JSON/CSV files, a single JSONL file or SQLite database. Example usage (Linux):\nFor llama-bench:\n$ git checkout master\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)\n$ ./llama-bench -o sql | sqlite3 llama-bench.sqlite",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SHOW_TEST_BACKEND_OPS",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "DEFAULT_SHOW_TEST_BACKEND_OPS = [\"backend_name\", \"op_name\"]  # Always show these properties by default.\nDEFAULT_HIDE_TEST_BACKEND_OPS = [\"error_message\"]  # Always hide these properties by default.\nGPU_NAME_STRIP = [\"NVIDIA GeForce \", \"Tesla \", \"AMD Radeon \", \"AMD Instinct \"]  # Strip prefixes for smaller tables.\nMODEL_SUFFIX_REPLACE = {\" - Small\": \"_S\", \" - Medium\": \"_M\", \" - Large\": \"_L\"}\nDESCRIPTION = \"\"\"Creates tables from llama-bench or test-backend-ops data written to multiple JSON/CSV files, a single JSONL file or SQLite database. Example usage (Linux):\nFor llama-bench:\n$ git checkout master\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)\n$ ./llama-bench -o sql | sqlite3 llama-bench.sqlite\n$ git checkout some_branch",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HIDE_TEST_BACKEND_OPS",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "DEFAULT_HIDE_TEST_BACKEND_OPS = [\"error_message\"]  # Always hide these properties by default.\nGPU_NAME_STRIP = [\"NVIDIA GeForce \", \"Tesla \", \"AMD Radeon \", \"AMD Instinct \"]  # Strip prefixes for smaller tables.\nMODEL_SUFFIX_REPLACE = {\" - Small\": \"_S\", \" - Medium\": \"_M\", \" - Large\": \"_L\"}\nDESCRIPTION = \"\"\"Creates tables from llama-bench or test-backend-ops data written to multiple JSON/CSV files, a single JSONL file or SQLite database. Example usage (Linux):\nFor llama-bench:\n$ git checkout master\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)\n$ ./llama-bench -o sql | sqlite3 llama-bench.sqlite\n$ git checkout some_branch\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "GPU_NAME_STRIP",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "GPU_NAME_STRIP = [\"NVIDIA GeForce \", \"Tesla \", \"AMD Radeon \", \"AMD Instinct \"]  # Strip prefixes for smaller tables.\nMODEL_SUFFIX_REPLACE = {\" - Small\": \"_S\", \" - Medium\": \"_M\", \" - Large\": \"_L\"}\nDESCRIPTION = \"\"\"Creates tables from llama-bench or test-backend-ops data written to multiple JSON/CSV files, a single JSONL file or SQLite database. Example usage (Linux):\nFor llama-bench:\n$ git checkout master\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)\n$ ./llama-bench -o sql | sqlite3 llama-bench.sqlite\n$ git checkout some_branch\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)\n$ ./llama-bench -o sql | sqlite3 llama-bench.sqlite",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "MODEL_SUFFIX_REPLACE",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "MODEL_SUFFIX_REPLACE = {\" - Small\": \"_S\", \" - Medium\": \"_M\", \" - Large\": \"_L\"}\nDESCRIPTION = \"\"\"Creates tables from llama-bench or test-backend-ops data written to multiple JSON/CSV files, a single JSONL file or SQLite database. Example usage (Linux):\nFor llama-bench:\n$ git checkout master\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)\n$ ./llama-bench -o sql | sqlite3 llama-bench.sqlite\n$ git checkout some_branch\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)\n$ ./llama-bench -o sql | sqlite3 llama-bench.sqlite\n$ ./scripts/compare-llama-bench.py",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "DESCRIPTION",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "DESCRIPTION = \"\"\"Creates tables from llama-bench or test-backend-ops data written to multiple JSON/CSV files, a single JSONL file or SQLite database. Example usage (Linux):\nFor llama-bench:\n$ git checkout master\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)\n$ ./llama-bench -o sql | sqlite3 llama-bench.sqlite\n$ git checkout some_branch\n$ cmake -B ${BUILD_DIR} ${CMAKE_OPTS} && cmake --build ${BUILD_DIR} -t llama-bench -j $(nproc)\n$ ./llama-bench -o sql | sqlite3 llama-bench.sqlite\n$ ./scripts/compare-llama-bench.py\nFor test-backend-ops:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=DESCRIPTION, formatter_class=argparse.RawDescriptionHelpFormatter)\nhelp_b = (\n    \"The baseline commit to compare performance to. \"\n    \"Accepts either a branch name, tag name, or commit hash. \"\n    \"Defaults to latest master commit with data.\"\n)\nparser.add_argument(\"-b\", \"--baseline\", help=help_b)\nhelp_c = (\n    \"The commit whose performance is to be compared to the baseline. \"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "help_b",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "help_b = (\n    \"The baseline commit to compare performance to. \"\n    \"Accepts either a branch name, tag name, or commit hash. \"\n    \"Defaults to latest master commit with data.\"\n)\nparser.add_argument(\"-b\", \"--baseline\", help=help_b)\nhelp_c = (\n    \"The commit whose performance is to be compared to the baseline. \"\n    \"Accepts either a branch name, tag name, or commit hash. \"\n    \"Defaults to the non-master commit for which llama-bench was run most recently.\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "help_c",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "help_c = (\n    \"The commit whose performance is to be compared to the baseline. \"\n    \"Accepts either a branch name, tag name, or commit hash. \"\n    \"Defaults to the non-master commit for which llama-bench was run most recently.\"\n)\nparser.add_argument(\"-c\", \"--compare\", help=help_c)\nhelp_t = (\n    \"The tool whose data is being compared. \"\n    \"Either 'llama-bench' or 'test-backend-ops'. \"\n    \"This determines the database schema and comparison logic used. \"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "help_t",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "help_t = (\n    \"The tool whose data is being compared. \"\n    \"Either 'llama-bench' or 'test-backend-ops'. \"\n    \"This determines the database schema and comparison logic used. \"\n    \"If left unspecified, try to determine from the input file.\"\n)\nparser.add_argument(\"-t\", \"--tool\", help=help_t, default=None, choices=[None, \"llama-bench\", \"test-backend-ops\"])\nhelp_i = (\n    \"JSON/JSONL/SQLite/CSV files for comparing commits. \"\n    \"Specify multiple times to use multiple input files (JSON/CSV only). \"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "help_i",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "help_i = (\n    \"JSON/JSONL/SQLite/CSV files for comparing commits. \"\n    \"Specify multiple times to use multiple input files (JSON/CSV only). \"\n    \"Defaults to 'llama-bench.sqlite' in the current working directory. \"\n    \"If no such file is found and there is exactly one .sqlite file in the current directory, \"\n    \"that file is instead used as input.\"\n)\nparser.add_argument(\"-i\", \"--input\", action=\"append\", help=help_i)\nhelp_o = (\n    \"Output format for the table. \"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "help_o",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "help_o = (\n    \"Output format for the table. \"\n    \"Defaults to 'pipe' (GitHub compatible). \"\n    \"Also supports e.g. 'latex' or 'mediawiki'. \"\n    \"See tabulate documentation for full list.\"\n)\nparser.add_argument(\"-o\", \"--output\", help=help_o, default=\"pipe\")\nhelp_s = (\n    \"Columns to add to the table. \"\n    \"Accepts a comma-separated list of values. \"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "help_s",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "help_s = (\n    \"Columns to add to the table. \"\n    \"Accepts a comma-separated list of values. \"\n    f\"Legal values for test-backend-ops: {', '.join(TEST_BACKEND_OPS_KEY_PROPERTIES)}. \"\n    f\"Legal values for llama-bench: {', '.join(LLAMA_BENCH_KEY_PROPERTIES[:-3])}. \"\n    \"Defaults to model name (model_type) and CPU and/or GPU name (cpu_info, gpu_info) \"\n    \"plus any column where not all data points are the same. \"\n    \"If the columns are manually specified, then the results for each unique combination of the \"\n    \"specified values are averaged WITHOUT weighing by the --repetitions parameter of llama-bench.\"\n)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "input_file",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "input_file = known_args.input\ntool = known_args.tool\nif not input_file:\n    if tool == \"llama-bench\" and os.path.exists(\"./llama-bench.sqlite\"):\n        input_file = [\"llama-bench.sqlite\"]\n    elif tool == \"test-backend-ops\" and os.path.exists(\"./test-backend-ops.sqlite\"):\n        input_file = [\"test-backend-ops.sqlite\"]\nif not input_file:\n    sqlite_files = glob(\"*.sqlite\")\n    if len(sqlite_files) == 1:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "tool",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "tool = known_args.tool\nif not input_file:\n    if tool == \"llama-bench\" and os.path.exists(\"./llama-bench.sqlite\"):\n        input_file = [\"llama-bench.sqlite\"]\n    elif tool == \"test-backend-ops\" and os.path.exists(\"./test-backend-ops.sqlite\"):\n        input_file = [\"test-backend-ops.sqlite\"]\nif not input_file:\n    sqlite_files = glob(\"*.sqlite\")\n    if len(sqlite_files) == 1:\n        input_file = sqlite_files",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "bench_data",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "bench_data = None\nif len(input_file) == 1:\n    if LlamaBenchDataSQLite3File.valid_format(input_file[0]):\n        bench_data = LlamaBenchDataSQLite3File(input_file[0], tool)\n    elif LlamaBenchDataJSON.valid_format(input_file):\n        bench_data = LlamaBenchDataJSON(input_file, tool)\n    elif LlamaBenchDataJSONL.valid_format(input_file[0]):\n        bench_data = LlamaBenchDataJSONL(input_file[0], tool)\n    elif LlamaBenchDataCSV.valid_format(input_file):\n        bench_data = LlamaBenchDataCSV(input_file, tool)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "tool",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "tool = bench_data.tool  # May have chosen a default if tool was None.\nhexsha8_baseline = name_baseline = None\n# If the user specified a baseline, try to find a commit for it:\nif known_args.baseline is not None:\n    if known_args.baseline in bench_data.builds:\n        hexsha8_baseline = known_args.baseline\n    if hexsha8_baseline is None:\n        hexsha8_baseline = bench_data.get_commit_hexsha8(known_args.baseline)\n        name_baseline = known_args.baseline\n    if hexsha8_baseline is None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "hexsha8_baseline",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "hexsha8_baseline = name_baseline = None\n# If the user specified a baseline, try to find a commit for it:\nif known_args.baseline is not None:\n    if known_args.baseline in bench_data.builds:\n        hexsha8_baseline = known_args.baseline\n    if hexsha8_baseline is None:\n        hexsha8_baseline = bench_data.get_commit_hexsha8(known_args.baseline)\n        name_baseline = known_args.baseline\n    if hexsha8_baseline is None:\n        logger.error(f\"cannot find data for baseline={known_args.baseline}.\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "name_baseline",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "name_baseline = bench_data.get_commit_name(hexsha8_baseline)\nhexsha8_compare = name_compare = None\n# If the user has specified a compare value, try to find a corresponding commit:\nif known_args.compare is not None:\n    if known_args.compare in bench_data.builds:\n        hexsha8_compare = known_args.compare\n    if hexsha8_compare is None:\n        hexsha8_compare = bench_data.get_commit_hexsha8(known_args.compare)\n        name_compare = known_args.compare\n    if hexsha8_compare is None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "hexsha8_compare",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "hexsha8_compare = name_compare = None\n# If the user has specified a compare value, try to find a corresponding commit:\nif known_args.compare is not None:\n    if known_args.compare in bench_data.builds:\n        hexsha8_compare = known_args.compare\n    if hexsha8_compare is None:\n        hexsha8_compare = bench_data.get_commit_hexsha8(known_args.compare)\n        name_compare = known_args.compare\n    if hexsha8_compare is None:\n        logger.error(f\"cannot find data for compare={known_args.compare}.\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "name_compare",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "name_compare = bench_data.get_commit_name(hexsha8_compare)\n# Get tool-specific configuration\nif tool == \"llama-bench\":\n    key_properties = LLAMA_BENCH_KEY_PROPERTIES\n    bool_properties = LLAMA_BENCH_BOOL_PROPERTIES\n    pretty_names = LLAMA_BENCH_PRETTY_NAMES\n    default_show = DEFAULT_SHOW_LLAMA_BENCH\n    default_hide = DEFAULT_HIDE_LLAMA_BENCH\nelif tool == \"test-backend-ops\":\n    key_properties = TEST_BACKEND_OPS_KEY_PROPERTIES",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "table",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "table = []\nprimary_metric = \"FLOPS\"  # Default to FLOPS for test-backend-ops\nif tool == \"llama-bench\":\n    # For llama-bench, create test names and compare avg_ts values\n    for row in rows_show:\n        n_prompt = int(row[-5])\n        n_gen    = int(row[-4])\n        n_depth  = int(row[-3])\n        if n_prompt != 0 and n_gen == 0:\n            test_name = f\"pp{n_prompt}\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "primary_metric",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "peekOfCode": "primary_metric = \"FLOPS\"  # Default to FLOPS for test-backend-ops\nif tool == \"llama-bench\":\n    # For llama-bench, create test names and compare avg_ts values\n    for row in rows_show:\n        n_prompt = int(row[-5])\n        n_gen    = int(row[-4])\n        n_depth  = int(row[-3])\n        if n_prompt != 0 and n_gen == 0:\n            test_name = f\"pp{n_prompt}\"\n        elif n_prompt == 0 and n_gen != 0:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-llama-bench",
        "documentation": {}
    },
    {
        "label": "generate_input_prompt",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "peekOfCode": "def generate_input_prompt(length: int) -> list[str]:\n    CORPUS = \"\"\"\n    You are an advanced AI assistant capable of using tools to gather information, perform calculations, or execute tasks. Always think step by step before responding. If a user's query requires external data, computation, or actions beyond your internal knowledge, use the appropriate tools via function calls.\n    ### Tool Call Format:\n    When you need to use a tool, output the call in this exact XML format. Include the opening and closing tags. Do not escape arguments; they will be parsed as plain text.\n    You can make multiple calls in one go by placing them one after another.\n    \"\"\"\n    words = [w.strip() for w in CORPUS.strip().split(\" \")]\n    words = [w for w in words if len(w) > 0]  # filter out empty strings\n    while len(words) < length:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "documentation": {}
    },
    {
        "label": "dump_logits",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "peekOfCode": "def dump_logits(\n    endpoint: str,\n    output_path: Path,\n    input_words: list[str],\n    pattern: list[tuple[bool, int]],\n    api_key=None,\n):\n    logger.info(f\"Dumping logits to {output_path} from endpoint {endpoint}...\")\n    words = input_words\n    curr_text = \"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "documentation": {}
    },
    {
        "label": "get_token_logprobs",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "peekOfCode": "def get_token_logprobs(data: dict):\n    logprobs = data[\"choices\"][0][\"logprobs\"]\n    if \"content\" in logprobs:\n        # llama.cpp case\n        top = logprobs[\"content\"][0][\"top_logprobs\"][0]\n        return top[\"token\"], top[\"logprob\"]\n    else:\n        # vllm case\n        tokens = logprobs[\"tokens\"]\n        token_logprobs = logprobs[\"token_logprobs\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "peekOfCode": "def clean_text(text: str) -> str:\n    return (\n        \"'\"\n        + text.replace(\"\\n\", \"\\\\n\")\n        .replace(\"\\t\", \"\\\\t\")\n        .replace(\"\\r\", \"\\\\r\")\n        .replace(\"|\", \"\\\\|\")\n        + \"'\"\n    )\ndef compare_logits(input1: Path, input2: Path, output_path: Path):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "documentation": {}
    },
    {
        "label": "compare_logits",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "peekOfCode": "def compare_logits(input1: Path, input2: Path, output_path: Path):\n    with input1.open(\"r\") as f1, input2.open(\"r\") as f2, output_path.open(\"w\") as fout:\n        lines1 = f1.readlines()\n        lines2 = f2.readlines()\n        tab_header = [\n            \"idx\",\n            input1.name,\n            \"logprob_1\",\n            input2.name,\n            \"logprob_2\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "documentation": {}
    },
    {
        "label": "parse_pattern",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "peekOfCode": "def parse_pattern(pattern: str) -> list[tuple[bool, int]]:\n    parts = pattern.split(\",\")\n    result = []\n    for i, part in enumerate(parts):\n        n = int(part)\n        if i % 2 == 0:\n            result.append((True, n))  # get n words\n        else:\n            result.append((False, n))  # skip n words\n    return result",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "peekOfCode": "def parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=DESCRIPTION, formatter_class=argparse.RawTextHelpFormatter\n    )\n    subparsers = parser.add_subparsers(\n        dest=\"verb\", required=True, help=\"action to perform\"\n    )\n    # dump subcommand\n    parser_dump = subparsers.add_parser(\"dump\", help=\"dump logits from an endpoint\")\n    parser_dump.add_argument(",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "peekOfCode": "def main():\n    args = parse_args()\n    if args.verb == \"dump\":\n        pattern = parse_pattern(args.pattern)\n        input_length = sum(n for _, n in pattern)\n        input_words = generate_input_prompt(input_length)\n        if args.file is not None:\n            with args.file.open(\"r\") as f:\n                input_words = f.read().strip().split(\" \")\n                if input_length < sum(n for _, n in pattern):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "peekOfCode": "logger = logging.getLogger(\"compare-logprobs\")\nlogging.basicConfig(level=logging.INFO)\nDESCRIPTION = \"\"\"\nCompare logits between llama.cpp and another inference engine using OpenAI-compatible server endpoints.\nUnlike compare-logits.py, it allows dumping logits from a hosted API endpoint. Useful when it's not possible to run both models locally.\nExample usage:\n    Step 1: Dump logits from two different servers\n        python scripts/compare-logprobs.py dump logits_llama.log http://localhost:8080/v1/completions\n        python scripts/compare-logprobs.py dump logits_other.log http://other-engine:8000/v1/completions\n        (optionally, you can add --api-key <key> if the endpoint requires authentication)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "documentation": {}
    },
    {
        "label": "DESCRIPTION",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "peekOfCode": "DESCRIPTION = \"\"\"\nCompare logits between llama.cpp and another inference engine using OpenAI-compatible server endpoints.\nUnlike compare-logits.py, it allows dumping logits from a hosted API endpoint. Useful when it's not possible to run both models locally.\nExample usage:\n    Step 1: Dump logits from two different servers\n        python scripts/compare-logprobs.py dump logits_llama.log http://localhost:8080/v1/completions\n        python scripts/compare-logprobs.py dump logits_other.log http://other-engine:8000/v1/completions\n        (optionally, you can add --api-key <key> if the endpoint requires authentication)\n    Step 2: Compare the dumped logits\n        python scripts/compare-logprobs.py compare logits_llama.log logits_other.log report.md",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.compare-logprobs",
        "documentation": {}
    },
    {
        "label": "DocsGenerator",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.create_ops_docs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.create_ops_docs",
        "peekOfCode": "class DocsGenerator:\n    def __init__(self, ggml_root: str, output_filename: str = \"ops.md\"):\n        self.ggml_root = Path(ggml_root)\n        self.ops_dir = self.ggml_root / \"docs\" / \"ops\"\n        self.output_filename = output_filename\n        self.backend_support: dict[str, dict[str, list[bool]]] = defaultdict(\n            lambda: defaultdict(list)\n        )\n        self.all_operations: set[str] = set()\n        self.all_backends: set[str] = set()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.create_ops_docs",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.create_ops_docs",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.create_ops_docs",
        "peekOfCode": "def main():\n    logging.basicConfig(level=logging.INFO)\n    if len(sys.argv) > 1:\n        output_filename = sys.argv[1]\n    else:\n        output_filename = \"ops.md\"\n    generator = DocsGenerator(\".\", output_filename)\n    generator.run()\nif __name__ == \"__main__\":\n    main()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.create_ops_docs",
        "documentation": {}
    },
    {
        "label": "HuggingFaceModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.fetch_server_test_models",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.fetch_server_test_models",
        "peekOfCode": "class HuggingFaceModel(BaseModel):\n    hf_repo: str\n    hf_file: Optional[str] = None\n    class Config:\n        frozen = True\ndef collect_hf_model_test_parameters(test_file) -> Generator[HuggingFaceModel, None, None]:\n    try:\n        with open(test_file) as f:\n            tree = ast.parse(f.read())\n    except Exception as e:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.fetch_server_test_models",
        "documentation": {}
    },
    {
        "label": "collect_hf_model_test_parameters",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.fetch_server_test_models",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.fetch_server_test_models",
        "peekOfCode": "def collect_hf_model_test_parameters(test_file) -> Generator[HuggingFaceModel, None, None]:\n    try:\n        with open(test_file) as f:\n            tree = ast.parse(f.read())\n    except Exception as e:\n        logging.error(f'collect_hf_model_test_parameters failed on {test_file}: {e}')\n        return\n    for node in ast.walk(tree):\n        if isinstance(node, ast.FunctionDef):\n            for dec in node.decorator_list:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.fetch_server_test_models",
        "documentation": {}
    },
    {
        "label": "unicode_data_iter",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "def unicode_data_iter():\n    res = requests.get(UNICODE_DATA_URL)\n    res.raise_for_status()\n    data = res.content.decode()\n    prev = []\n    for line in data.splitlines():\n        # ej: 0000;<control>;Cc;0;BN;;;;;N;NULL;;;;\n        line = line.split(\";\")\n        cpt = int(line[0], base=16)\n        assert cpt < MAX_CODEPOINTS",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "out",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "def out(line=\"\"):\n    print(line, end='\\n')  # noqa\nout(\"\"\"\\\n// generated with scripts/gen-unicode-data.py\n#include \"unicode-data.h\"\n#include <cstdint>\n#include <vector>\n#include <unordered_map>\n#include <unordered_set>\n\"\"\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "MAX_CODEPOINTS",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "MAX_CODEPOINTS = 0x110000\nUNICODE_DATA_URL = \"https://www.unicode.org/Public/UCD/latest/ucd/UnicodeData.txt\"\n# see https://www.unicode.org/L2/L1999/UnicodeData.html\ndef unicode_data_iter():\n    res = requests.get(UNICODE_DATA_URL)\n    res.raise_for_status()\n    data = res.content.decode()\n    prev = []\n    for line in data.splitlines():\n        # ej: 0000;<control>;Cc;0;BN;;;;;N;NULL;;;;",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "UNICODE_DATA_URL",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "UNICODE_DATA_URL = \"https://www.unicode.org/Public/UCD/latest/ucd/UnicodeData.txt\"\n# see https://www.unicode.org/L2/L1999/UnicodeData.html\ndef unicode_data_iter():\n    res = requests.get(UNICODE_DATA_URL)\n    res.raise_for_status()\n    data = res.content.decode()\n    prev = []\n    for line in data.splitlines():\n        # ej: 0000;<control>;Cc;0;BN;;;;;N;NULL;;;;\n        line = line.split(\";\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "CODEPOINT_FLAG_PUNCTUATION",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "CODEPOINT_FLAG_PUNCTUATION = 0x0020  # \\p{P}\nCODEPOINT_FLAG_SYMBOL      = 0x0040  # \\p{S}\nCODEPOINT_FLAG_CONTROL     = 0x0080  # \\p{C}\nUNICODE_CATEGORY_TO_FLAG = {\n    \"Cn\": CODEPOINT_FLAG_UNDEFINED,    # Undefined\n    \"Cc\": CODEPOINT_FLAG_CONTROL,      # Control\n    \"Cf\": CODEPOINT_FLAG_CONTROL,      # Format\n    \"Co\": CODEPOINT_FLAG_CONTROL,      # Private Use\n    \"Cs\": CODEPOINT_FLAG_CONTROL,      # Surrrogate\n    \"Ll\": CODEPOINT_FLAG_LETTER,       # Lowercase Letter",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "UNICODE_CATEGORY_TO_FLAG",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "UNICODE_CATEGORY_TO_FLAG = {\n    \"Cn\": CODEPOINT_FLAG_UNDEFINED,    # Undefined\n    \"Cc\": CODEPOINT_FLAG_CONTROL,      # Control\n    \"Cf\": CODEPOINT_FLAG_CONTROL,      # Format\n    \"Co\": CODEPOINT_FLAG_CONTROL,      # Private Use\n    \"Cs\": CODEPOINT_FLAG_CONTROL,      # Surrrogate\n    \"Ll\": CODEPOINT_FLAG_LETTER,       # Lowercase Letter\n    \"Lm\": CODEPOINT_FLAG_LETTER,       # Modifier Letter\n    \"Lo\": CODEPOINT_FLAG_LETTER,       # Other Letter\n    \"Lt\": CODEPOINT_FLAG_LETTER,       # Titlecase Letter",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "codepoint_flags",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "codepoint_flags = array.array('H', [CODEPOINT_FLAG_UNDEFINED]) * MAX_CODEPOINTS\ntable_whitespace = []\ntable_lowercase = []\ntable_uppercase = []\ntable_nfd = []\nfor (cpt, cpt_lower, cpt_upper, categ, bidir) in unicode_data_iter():\n    # convert codepoint to unicode character\n    char = chr(cpt)\n    # codepoint category flags\n    codepoint_flags[cpt] = UNICODE_CATEGORY_TO_FLAG[categ]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "table_whitespace",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "table_whitespace = []\ntable_lowercase = []\ntable_uppercase = []\ntable_nfd = []\nfor (cpt, cpt_lower, cpt_upper, categ, bidir) in unicode_data_iter():\n    # convert codepoint to unicode character\n    char = chr(cpt)\n    # codepoint category flags\n    codepoint_flags[cpt] = UNICODE_CATEGORY_TO_FLAG[categ]\n    # lowercase conversion",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "table_lowercase",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "table_lowercase = []\ntable_uppercase = []\ntable_nfd = []\nfor (cpt, cpt_lower, cpt_upper, categ, bidir) in unicode_data_iter():\n    # convert codepoint to unicode character\n    char = chr(cpt)\n    # codepoint category flags\n    codepoint_flags[cpt] = UNICODE_CATEGORY_TO_FLAG[categ]\n    # lowercase conversion\n    if cpt_lower:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "table_uppercase",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "table_uppercase = []\ntable_nfd = []\nfor (cpt, cpt_lower, cpt_upper, categ, bidir) in unicode_data_iter():\n    # convert codepoint to unicode character\n    char = chr(cpt)\n    # codepoint category flags\n    codepoint_flags[cpt] = UNICODE_CATEGORY_TO_FLAG[categ]\n    # lowercase conversion\n    if cpt_lower:\n        table_lowercase.append((cpt, cpt_lower))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "table_nfd",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "peekOfCode": "table_nfd = []\nfor (cpt, cpt_lower, cpt_upper, categ, bidir) in unicode_data_iter():\n    # convert codepoint to unicode character\n    char = chr(cpt)\n    # codepoint category flags\n    codepoint_flags[cpt] = UNICODE_CATEGORY_TO_FLAG[categ]\n    # lowercase conversion\n    if cpt_lower:\n        table_lowercase.append((cpt, cpt_lower))\n    # uppercase conversion",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.gen-unicode-data",
        "documentation": {}
    },
    {
        "label": "get_chat_template",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.get_chat_template",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.get_chat_template",
        "peekOfCode": "def get_chat_template(model_id, variant=None):\n    try:\n        # Use huggingface_hub library if available.\n        # Allows access to gated models if the user has access and ran `huggingface-cli login`.\n        from huggingface_hub import hf_hub_download\n        with open(hf_hub_download(repo_id=model_id, filename=\"tokenizer_config.json\"), encoding=\"utf-8\") as f:\n            config_str = f.read()\n    except ImportError:\n        import requests\n        assert re.match(r\"^[\\w.-]+/[\\w.-]+$\", model_id), f\"Invalid model ID: {model_id}\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.get_chat_template",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.get_chat_template",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.get_chat_template",
        "peekOfCode": "def main(args):\n    if len(args) < 1:\n        raise ValueError(\"Please provide a model ID and an optional variant name\")\n    model_id = args[0]\n    variant = None if len(args) < 2 else args[1]\n    template = get_chat_template(model_id, variant)\n    sys.stdout.write(template)\nif __name__ == '__main__':\n    main(sys.argv[1:])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.get_chat_template",
        "documentation": {}
    },
    {
        "label": "get_prompts_text",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "peekOfCode": "def get_prompts_text(dataset_name: str, n_prompts: int) -> Optional[list[str]]:\n    ret = []\n    if dataset_name.lower() == \"mmlu\":\n        logger.info(\"Loading MMLU dataset...\")\n        ret = datasets.load_dataset(\"cais/mmlu\", \"all\")[\"test\"][\"question\"]  # type: ignore\n    else:\n        return None\n    if n_prompts >= 0:\n        ret = ret[:n_prompts]\n    return ret",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "documentation": {}
    },
    {
        "label": "get_prompt_lengths_rng",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "peekOfCode": "def get_prompt_lengths_rng(n_prompts: int, prompt_length_min: int, prompt_length_max: int, seed_offset: int) -> list[int]:\n    assert n_prompts >= 0\n    ret: list[int] = []\n    for i in range(n_prompts):\n        if seed_offset >= 0:\n            random.seed(3 * (seed_offset + 1000 * i) + 0)\n        ret.append(random.randint(prompt_length_min, prompt_length_max))\n    return ret\ndef get_prompts_rng(prompt_lengths: list[int]) -> list[list[int]]:\n    return [[random.randint(100, 10000) for _ in range(pl)] for pl in prompt_lengths]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "documentation": {}
    },
    {
        "label": "get_prompts_rng",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "peekOfCode": "def get_prompts_rng(prompt_lengths: list[int]) -> list[list[int]]:\n    return [[random.randint(100, 10000) for _ in range(pl)] for pl in prompt_lengths]\ndef get_server(path_server: str, path_log: Optional[str]) -> dict:\n    if path_server.startswith(\"http://\") or path_server.startswith(\"https://\"):\n        return {\"process\": None, \"address\": path_server, \"fout\": None}\n    if os.environ.get(\"LLAMA_ARG_HOST\") is None:\n        logger.info(\"LLAMA_ARG_HOST not explicitly set, using 127.0.0.1\")\n        os.environ[\"LLAMA_ARG_HOST\"] = \"127.0.0.1\"\n    if os.environ.get(\"LLAMA_ARG_PORT\") is None:\n        logger.info(\"LLAMA_ARG_PORT not explicitly set, using 8080\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "documentation": {}
    },
    {
        "label": "get_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "peekOfCode": "def get_server(path_server: str, path_log: Optional[str]) -> dict:\n    if path_server.startswith(\"http://\") or path_server.startswith(\"https://\"):\n        return {\"process\": None, \"address\": path_server, \"fout\": None}\n    if os.environ.get(\"LLAMA_ARG_HOST\") is None:\n        logger.info(\"LLAMA_ARG_HOST not explicitly set, using 127.0.0.1\")\n        os.environ[\"LLAMA_ARG_HOST\"] = \"127.0.0.1\"\n    if os.environ.get(\"LLAMA_ARG_PORT\") is None:\n        logger.info(\"LLAMA_ARG_PORT not explicitly set, using 8080\")\n        os.environ[\"LLAMA_ARG_PORT\"] = \"8080\"\n    hostname: Optional[str] = os.environ.get(\"LLAMA_ARG_HOST\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "documentation": {}
    },
    {
        "label": "get_prompt_length",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "peekOfCode": "def get_prompt_length(data: dict) -> int:\n    session = data[\"session\"]\n    server_address: str = data[\"server_address\"]\n    response = session.post(\n        f\"{server_address}/apply-template\",\n        json={\"messages\": [{\"role\": \"user\", \"content\": data[\"prompt\"], \"stream\": True}]}\n    )\n    response.raise_for_status()\n    prompt: str = json.loads(response.text)[\"prompt\"]\n    response = session.post(",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "documentation": {}
    },
    {
        "label": "send_prompt",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "peekOfCode": "def send_prompt(data: dict) -> tuple[float, list[float]]:\n    session = data[\"session\"]\n    server_address: str = data[\"server_address\"]\n    t_submit = time()\n    if data[\"external_server\"]:\n        json_data: dict = {\n            \"prompt\": data[\"prompt\"], \"ignore_eos\": True,\n            \"seed\": data[\"seed\"], \"max_tokens\": data[\"n_predict\"], \"stream\": True}\n        response = session.post(f\"{server_address}/v1/completions\", json=json_data, stream=True)\n    elif data[\"synthetic_prompt\"]:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "documentation": {}
    },
    {
        "label": "benchmark",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "peekOfCode": "def benchmark(\n        path_server: str, path_log: Optional[str], path_db: Optional[str], name: Optional[str], prompt_source: str, n_prompts: int,\n        n_predict: int, n_predict_min: int, seed_offset: int):\n    external_server: bool = path_server.startswith(\"http://\") or path_server.startswith(\"https://\")\n    if os.environ.get(\"LLAMA_ARG_N_PARALLEL\") is None:\n        logger.info(\"LLAMA_ARG_N_PARALLEL not explicitly set, using 32\")\n        os.environ[\"LLAMA_ARG_N_PARALLEL\"] = \"32\"\n    parallel: int = int(os.environ.get(\"LLAMA_ARG_N_PARALLEL\")) # type: ignore\n    prompts: Union[None, list[str], list[list[int]]] = get_prompts_text(prompt_source, n_prompts)\n    synthetic_prompts: bool = prompts is None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "peekOfCode": "logger = logging.getLogger(\"server-bench\")\ndef get_prompts_text(dataset_name: str, n_prompts: int) -> Optional[list[str]]:\n    ret = []\n    if dataset_name.lower() == \"mmlu\":\n        logger.info(\"Loading MMLU dataset...\")\n        ret = datasets.load_dataset(\"cais/mmlu\", \"all\")[\"test\"][\"question\"]  # type: ignore\n    else:\n        return None\n    if n_prompts >= 0:\n        ret = ret[:n_prompts]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.server-bench",
        "documentation": {}
    },
    {
        "label": "vendor",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.sync_vendor",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.sync_vendor",
        "peekOfCode": "vendor = {\n    \"https://github.com/nlohmann/json/releases/latest/download/json.hpp\":     \"vendor/nlohmann/json.hpp\",\n    \"https://github.com/nlohmann/json/releases/latest/download/json_fwd.hpp\": \"vendor/nlohmann/json_fwd.hpp\",\n    # sync manually\n    # \"https://raw.githubusercontent.com/ochafik/minja/refs/heads/main/include/minja/minja.hpp\":         \"vendor/minja/minja.hpp\",\n    # \"https://raw.githubusercontent.com/ochafik/minja/refs/heads/main/include/minja/chat-template.hpp\": \"vendor/minja/chat-template.hpp\",\n    \"https://raw.githubusercontent.com/nothings/stb/refs/heads/master/stb_image.h\": \"vendor/stb/stb_image.h\",\n    # not using latest tag to avoid this issue: https://github.com/ggml-org/llama.cpp/pull/17179#discussion_r2515877926\n    # \"https://github.com/mackron/miniaudio/raw/refs/tags/0.11.23/miniaudio.h\": \"vendor/miniaudio/miniaudio.h\",\n    \"https://github.com/mackron/miniaudio/raw/669ed3e844524fcd883231b13095baee9f6de304/miniaudio.h\": \"vendor/miniaudio/miniaudio.h\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.sync_vendor",
        "documentation": {}
    },
    {
        "label": "scoped_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "peekOfCode": "def scoped_server(sp: ServerProcess):\n    def stop():\n        nonlocal sp\n        if sp is not None:\n            sp.stop()\n            sp = None # type: ignore\n    atexit.register(stop)\n    yield sp\n    stop()\nlogging.basicConfig(",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "documentation": {}
    },
    {
        "label": "plot",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "peekOfCode": "def plot(files: List[Path], output: Optional[Path] = None, test_regex: Optional[str] = None, server_regex: Optional[str] = None):\n    lines: List[Dict] = []\n    for file in files:\n        if not file.exists():\n            logger.error(f\"File not found: {file}\")\n            continue\n        try:\n            with file.open() as f:\n                raw_data = f.read()\n            logger.info(f\"Reading {file} ({len(raw_data)} bytes)\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "peekOfCode": "def run(\n    output: Annotated[Path, typer.Option(help=\"Output JSON file\")],\n    model: Annotated[Optional[str], typer.Option(help=\"Name of the model to test (server agnostic)\")] = None,\n    hf: Annotated[Optional[str], typer.Option(help=\"GGUF huggingface model repo id (+ optional quant) to test w/ llama-server\")] = None,\n    chat_template: Annotated[Optional[str], typer.Option(help=\"Chat template override for llama-server\")] = None,\n    chat_template_file: Annotated[Optional[str], typer.Option(help=\"Chat template file override for llama-server\")] = None,\n    ollama: Annotated[Optional[str], typer.Option(help=\"Ollama model tag to test\")] = None,\n    llama_baseline: Annotated[Optional[str], typer.Option(help=\"llama-server baseline binary path to use as baseline\")] = None,\n    n: Annotated[int, typer.Option(help=\"Number of times to run each test\")] = 10,\n    temp: Annotated[Optional[List[float]], typer.Option(help=\"Set of temperatures to test\")] = None,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "peekOfCode": "logger = logging.getLogger(__name__)\napp = typer.Typer()\n@app.command()\ndef plot(files: List[Path], output: Optional[Path] = None, test_regex: Optional[str] = None, server_regex: Optional[str] = None):\n    lines: List[Dict] = []\n    for file in files:\n        if not file.exists():\n            logger.error(f\"File not found: {file}\")\n            continue\n        try:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "peekOfCode": "app = typer.Typer()\n@app.command()\ndef plot(files: List[Path], output: Optional[Path] = None, test_regex: Optional[str] = None, server_regex: Optional[str] = None):\n    lines: List[Dict] = []\n    for file in files:\n        if not file.exists():\n            logger.error(f\"File not found: {file}\")\n            continue\n        try:\n            with file.open() as f:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.tool_bench",
        "documentation": {}
    },
    {
        "label": "sha256sum",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "peekOfCode": "def sha256sum(file):\n    block_size = 16 * 1024 * 1024  # 16 MB block size\n    b = bytearray(block_size)\n    file_hash = hashlib.sha256()\n    mv = memoryview(b)\n    with open(file, 'rb', buffering=0) as f:\n        while True:\n            n = f.readinto(mv)\n            if not n:\n                break",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "peekOfCode": "logger = logging.getLogger(\"verify-checksum-models\")\ndef sha256sum(file):\n    block_size = 16 * 1024 * 1024  # 16 MB block size\n    b = bytearray(block_size)\n    file_hash = hashlib.sha256()\n    mv = memoryview(b)\n    with open(file, 'rb', buffering=0) as f:\n        while True:\n            n = f.readinto(mv)\n            if not n:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "documentation": {}
    },
    {
        "label": "llama_path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "peekOfCode": "llama_path = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))\n# Define the file with the list of hashes and filenames\nhash_list_file = os.path.join(llama_path, \"SHA256SUMS\")\n# Check if the hash list file exists\nif not os.path.exists(hash_list_file):\n    logger.error(f\"Hash list file not found: {hash_list_file}\")\n    exit(1)\n# Read the hash file content and split it into an array of lines\nwith open(hash_list_file, \"r\") as f:\n    hash_list = f.read().splitlines()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "documentation": {}
    },
    {
        "label": "hash_list_file",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "peekOfCode": "hash_list_file = os.path.join(llama_path, \"SHA256SUMS\")\n# Check if the hash list file exists\nif not os.path.exists(hash_list_file):\n    logger.error(f\"Hash list file not found: {hash_list_file}\")\n    exit(1)\n# Read the hash file content and split it into an array of lines\nwith open(hash_list_file, \"r\") as f:\n    hash_list = f.read().splitlines()\n# Create an array to store the results\nresults = []",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "peekOfCode": "results = []\n# Loop over each line in the hash list\nfor line in hash_list:\n    # Split the line into hash and filename\n    hash_value, filename = line.split(\"  \")\n    # Get the full path of the file by joining the llama path and the filename\n    file_path = os.path.join(llama_path, filename)\n    # Informing user of the progress of the integrity check\n    logger.info(f\"Verifying the checksum of {file_path}\")\n    # Check if the file exists",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.scripts.verify-checksum-models",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument(\"dir_tokenizer\", help=\"directory containing 'tokenizer.model' file\")\nparser.add_argument(\"--fname-tok\",   help=\"path to a text file to tokenize\", required=True)\nargs = parser.parse_args()\ndir_tokenizer = args.dir_tokenizer\nfname_tok = args.fname_tok\ntokenizer = AutoTokenizer.from_pretrained(dir_tokenizer)\nprint('tokenizing file: ', fname_tok) # noqa: NP100\nfname_out = fname_tok + '.tok'\nwith open(fname_tok, 'r', encoding='utf-8') as f:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "peekOfCode": "args = parser.parse_args()\ndir_tokenizer = args.dir_tokenizer\nfname_tok = args.fname_tok\ntokenizer = AutoTokenizer.from_pretrained(dir_tokenizer)\nprint('tokenizing file: ', fname_tok) # noqa: NP100\nfname_out = fname_tok + '.tok'\nwith open(fname_tok, 'r', encoding='utf-8') as f:\n    lines = f.readlines()\n    s = ''.join(lines)\n    t_start = time.time()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "documentation": {}
    },
    {
        "label": "dir_tokenizer",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "peekOfCode": "dir_tokenizer = args.dir_tokenizer\nfname_tok = args.fname_tok\ntokenizer = AutoTokenizer.from_pretrained(dir_tokenizer)\nprint('tokenizing file: ', fname_tok) # noqa: NP100\nfname_out = fname_tok + '.tok'\nwith open(fname_tok, 'r', encoding='utf-8') as f:\n    lines = f.readlines()\n    s = ''.join(lines)\n    t_start = time.time()\n    res = tokenizer.encode(s, add_special_tokens=False)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "documentation": {}
    },
    {
        "label": "fname_tok",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "peekOfCode": "fname_tok = args.fname_tok\ntokenizer = AutoTokenizer.from_pretrained(dir_tokenizer)\nprint('tokenizing file: ', fname_tok) # noqa: NP100\nfname_out = fname_tok + '.tok'\nwith open(fname_tok, 'r', encoding='utf-8') as f:\n    lines = f.readlines()\n    s = ''.join(lines)\n    t_start = time.time()\n    res = tokenizer.encode(s, add_special_tokens=False)\n    t_end = time.time()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(dir_tokenizer)\nprint('tokenizing file: ', fname_tok) # noqa: NP100\nfname_out = fname_tok + '.tok'\nwith open(fname_tok, 'r', encoding='utf-8') as f:\n    lines = f.readlines()\n    s = ''.join(lines)\n    t_start = time.time()\n    res = tokenizer.encode(s, add_special_tokens=False)\n    t_end = time.time()\n    print('\\nmain : tokenized in', \"{:.3f}\".format(1000.0 * (t_end - t_start)), 'ms (py)') # noqa: NP100",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "documentation": {}
    },
    {
        "label": "fname_out",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "peekOfCode": "fname_out = fname_tok + '.tok'\nwith open(fname_tok, 'r', encoding='utf-8') as f:\n    lines = f.readlines()\n    s = ''.join(lines)\n    t_start = time.time()\n    res = tokenizer.encode(s, add_special_tokens=False)\n    t_end = time.time()\n    print('\\nmain : tokenized in', \"{:.3f}\".format(1000.0 * (t_end - t_start)), 'ms (py)') # noqa: NP100\n    with open(fname_out, 'w', encoding='utf-8') as f:\n        for x in res:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-0",
        "documentation": {}
    },
    {
        "label": "LibLlama",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "class LibLlama:\n    DEFAULT_PATH_LLAMA_H = \"./include/llama.h\"\n    DEFAULT_PATH_INCLUDES = [\"./ggml/include/\", \"./include/\"]\n    DEFAULT_PATH_LIBLLAMA = \"./build/src/libllama.so\"  # CMakeLists.txt: BUILD_SHARED_LIBS ON\n    def __init__(self, path_llama_h: str | None = None, path_includes: list[str] = [], path_libllama: str | None = None):\n        path_llama_h = path_llama_h or self.DEFAULT_PATH_LLAMA_H\n        path_includes = path_includes or self.DEFAULT_PATH_INCLUDES\n        path_libllama = path_libllama or self.DEFAULT_PATH_LIBLLAMA\n        (self.ffi, self.lib) = self._load_libllama_cffi(path_llama_h, path_includes, path_libllama)\n        self.lib.llama_backend_init()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "LibLlamaModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "class LibLlamaModel:\n    def __init__(self, libllama: LibLlama, path_model: str, mparams={}, cparams={}):\n        self.lib: Any = libllama.lib\n        self.ffi = libllama.ffi\n        if isinstance(mparams, dict):\n            mparams = libllama.model_default_params(**mparams)\n        self.model = self.lib.llama_model_load_from_file(path_model.encode(), mparams)\n        if not self.model:\n            raise RuntimeError(\"error: failed to load model '%s'\" % path_model)\n        if isinstance(cparams, dict):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "class Tokenizer:\n    def encode(self, text: str) -> list[int]:\n        raise NotImplementedError\n    def decode(self, ids: list[int]) -> str:\n        raise NotImplementedError\nclass TokenizerGroundtruth (Tokenizer):\n    def __init__(self, dir_tokenizer: str):\n        self.model: PreTrainedTokenizer = AutoTokenizer.from_pretrained(dir_tokenizer)\n        # guess BOS and EOS\n        ids = self.encode(\"a\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "TokenizerGroundtrut",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "class TokenizerGroundtruth (Tokenizer):\n    def __init__(self, dir_tokenizer: str):\n        self.model: PreTrainedTokenizer = AutoTokenizer.from_pretrained(dir_tokenizer)\n        # guess BOS and EOS\n        ids = self.encode(\"a\")\n        assert 1 <= len(ids) <= 3\n        add_bos_token = len(ids) > 1 and self.model.bos_token_id == ids[0]\n        add_eos_token = len(ids) > 1 and self.model.eos_token_id == ids[-1]\n        self.add_bos_token = getattr(self.model, \"add_bos_token\", add_bos_token)\n        self.add_eos_token = getattr(self.model, \"add_eos_token\", add_eos_token)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "TokenizerLlamaCp",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "class TokenizerLlamaCpp (Tokenizer):\n    libllama: LibLlama | None = None\n    def __init__(self, vocab_file: str):\n        if not self.libllama:\n            self.libllama = LibLlama()\n        self.model = LibLlamaModel(self.libllama, vocab_file, mparams=dict(vocab_only=True), cparams=dict(n_ctx=4096))\n    def encode(self, text: str) -> list[int]:\n        return self.model.tokenize(text, add_special=True, parse_special=True)\n    def decode(self, ids: list[int]) -> str:\n        return self.model.detokenize(ids, remove_special=False, unparse_special=True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_custom_text",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_custom_text() -> Iterator[str]:\n    \"\"\"General tests\"\"\"\n    yield from [\n        \"\",\n        \" \",\n        \"  \",\n        \"   \",\n        \"\\t\",\n        \"\\n\",\n        \"\\n\\n\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_custom_text_edge_cases",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_custom_text_edge_cases() -> Iterator[str]:\n    \"\"\"Edge cases found while debugging\"\"\"\n    yield from [\n        '\\x1f-a',     # unicode_ranges_control, {0x00001C, 0x00001F}\n        '-a',        # unicode_ranges_digit, 0x00BC\n        '-a',        # unicode_ranges_digit, 0x00BD\n        '-a',        # unicode_ranges_digit, 0x00BE\n        'a b',      # unicode_ranges_digit, 0x3007\n        '-a',       # unicode_ranges_digit, {0x00002150, 0x0000218F} // Number Forms\n        '\\uFEFF//',   # unicode_ranges_control, 0xFEFF (BOM)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_vocab_words",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_vocab_words(tokenizer: TokenizerGroundtruth) -> Iterator[str]:\n    \"\"\"Brute force check all vocab words\"\"\"\n    yield from tokenizer.vocab\ndef generator_ascii_lr_strip() -> Iterator[str]:\n    WHITESPACES = [\"\", \" \", \"  \"]\n    CHARACTERS = list(chr(i) for i in range(1, 0x80)) + [\"\"]\n    for char1 in CHARACTERS:\n        for char2 in CHARACTERS:\n            for lstrip in WHITESPACES:\n                for rstrip in WHITESPACES:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_ascii_lr_strip",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_ascii_lr_strip() -> Iterator[str]:\n    WHITESPACES = [\"\", \" \", \"  \"]\n    CHARACTERS = list(chr(i) for i in range(1, 0x80)) + [\"\"]\n    for char1 in CHARACTERS:\n        for char2 in CHARACTERS:\n            for lstrip in WHITESPACES:\n                for rstrip in WHITESPACES:\n                    yield lstrip + char1 + char2 + rstrip\n                    yield lstrip + char1 + rstrip + char2\n                    yield char1 + lstrip + char2 + rstrip",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_apostrophe",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_apostrophe() -> Iterator[str]:\n    WHITESPACES = [\"\", \" \", \"  \"]\n    CHARACTERS = list(chr(i) for i in range(1, 0x80)) + [\"\"]\n    for char1 in CHARACTERS:\n        for char2 in CHARACTERS:\n            for lstrip in WHITESPACES:\n                for rstrip in WHITESPACES:\n                    yield char1 + lstrip + \"'\" + rstrip + char2\n                    yield char1 + char2 + lstrip + \"'\" + rstrip + \"z\"\n                    yield \"a\" + lstrip + \"'\" + rstrip + char1 + char2",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_added_lr_strip",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_added_lr_strip(tokenizer: TokenizerGroundtruth) -> Iterator[str]:\n    WHITESPACES = [\"\", \" \", \"  \", \"\\n\", \"\\r\\n\", \"\\n\\n\", \"\\t\", \"\\t\\t\"]\n    all_tokens = list(sorted(set(tokenizer.special_tokens + tokenizer.added_tokens)))\n    for token in all_tokens:\n        for lstrip in WHITESPACES:\n            for rstrip in WHITESPACES:\n                yield lstrip + token + rstrip\n                yield \"a\" + lstrip + token + rstrip\n                yield lstrip + token + rstrip + \"z\"\n                yield \"a\" + lstrip + token + rstrip + \"z\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_random_added_tokens",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_random_added_tokens(tokenizer: TokenizerGroundtruth, iterations=100) -> Iterator[str]:\n    separations = [\" \", \"\\n\", \"\\t\", \"-\", \"!\", \"one\", \"1\", \"<s>\", \"</s>\"]\n    all_tokens  = list(sorted(set(tokenizer.special_tokens + tokenizer.added_tokens + separations)))\n    rand = random.Random()\n    for m in range(iterations):\n        rand.seed(m)\n        words = rand.choices(all_tokens, k=500)\n        if words and words[0] == tokenizer.bos_token:  # skip spam warning of double BOS\n            while len(words) > 1 and words[1] == tokenizer.bos_token:  # leave one starting BOS\n                words.pop(0)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_random_chars",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_random_chars(iterations=100) -> Iterator[str]:\n    \"\"\"Brute force random text with simple characters\"\"\"\n    NUM_WORDS = 400\n    WHITESPACES = list(\" \" * 20 + \"\\n\" * 5 + \"\\r\\n\" * 5 + \"\\t\" * 5)\n    CHARS = list(sorted(set(\"\"\"\n        ABCDEFGHIJKLMNOPQRSTUVWXYZ\n        abcdefghijklmnopqrstuvwxyz\n        \n        \n        .-,*/-+!\"$%&/()=?[]{}<>\\\\|@#~~;:_",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_unicodes",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_unicodes() -> Iterator[str]:\n    \"\"\"Iterate unicode characters\"\"\"\n    MAX_CODEPOINTS = 0x30000  # 0x110000\n    def _valid(cpt):\n        if cpt >= 0x30000:  # unassigned and supplementary\n            return False\n        # if cpt == 0x2029:  # deepseek-llm\n        #    return False\n        if unicodedata.category(chr(cpt)) in (\"Cn\", \"Cs\", \"Co\"):  # undefined, surrogates, private\n            return False",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_random_unicodes",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_random_unicodes(iterations=100) -> Iterator[str]:\n    \"\"\"Brute force random text with unicode characters\"\"\"\n    NUM_WORDS = 200\n    WHITESPACES = list(\" \" * 20 + \"\\n\" * 5 + \"\\r\\n\" * 5 + \"\\t\" * 5)\n    characters = list(generator_unicodes())\n    rand = random.Random()\n    for m in range(iterations):\n        rand.seed(m)\n        text = []\n        for _ in range(NUM_WORDS):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_random_vocab_chars",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_random_vocab_chars(tokenizer: TokenizerGroundtruth, iterations=100) -> Iterator[str]:\n    \"\"\"Brute force random text with vocab characters\"\"\"\n    vocab_chars = set()\n    for word in tokenizer.vocab:\n        vocab_chars.update(word)\n    vocab_chars = list(sorted(vocab_chars))\n    rand = random.Random()\n    for m in range(iterations):\n        rand.seed(m)\n        text = rand.choices(vocab_chars, k=1024)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "generator_random_vocab_words",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def generator_random_vocab_words(tokenizer: TokenizerGroundtruth, iterations=100) -> Iterator[str]:\n    \"\"\"Brute force random text from vocab words\"\"\"\n    vocab = [w.strip() for w in tokenizer.vocab]\n    yield from vocab\n    rand = random.Random()\n    for m in range(iterations):\n        rand.seed(m)\n        text = []\n        num_words = rand.randint(300, 400)\n        for i in range(num_words):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "compare_tokenizers",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def compare_tokenizers(tokenizer1: TokenizerGroundtruth, tokenizer2: TokenizerLlamaCpp, generator: Iterator[str]):\n    def find_first_mismatch(ids1: list[int] | str, ids2: list[int] | str):\n        for i, (a, b) in enumerate(zip(ids1, ids2)):\n            if a != b:\n                return i\n        if len(ids1) == len(ids2):\n            return -1\n        return min(len(ids1), len(ids2))\n    def check_detokenizer(text: str, text1: str, text2: str) -> bool:\n        if text1 == text2:  # equal to TokenizerGroundtruth?",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "def main(argv: list[str] | None = None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"vocab_file\", type=str, help=\"path to vocab 'gguf' file\")\n    parser.add_argument(\"dir_tokenizer\", type=str, help=\"directory containing 'tokenizer.model' file\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"increase output verbosity\")\n    args = parser.parse_args(argv)\n    logging.basicConfig(level = logging.DEBUG if args.verbose else logging.INFO)\n    logger.info(f\"VOCABFILE: '{args.vocab_file}'\")\n    tokenizer1 = TokenizerGroundtruth(args.dir_tokenizer)\n    tokenizer2 = TokenizerLlamaCpp(args.vocab_file)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "peekOfCode": "logger = logging.getLogger(\"test-tokenizer-random\")\nclass LibLlama:\n    DEFAULT_PATH_LLAMA_H = \"./include/llama.h\"\n    DEFAULT_PATH_INCLUDES = [\"./ggml/include/\", \"./include/\"]\n    DEFAULT_PATH_LIBLLAMA = \"./build/src/libllama.so\"  # CMakeLists.txt: BUILD_SHARED_LIBS ON\n    def __init__(self, path_llama_h: str | None = None, path_includes: list[str] = [], path_libllama: str | None = None):\n        path_llama_h = path_llama_h or self.DEFAULT_PATH_LLAMA_H\n        path_includes = path_includes or self.DEFAULT_PATH_INCLUDES\n        path_libllama = path_libllama or self.DEFAULT_PATH_LIBLLAMA\n        (self.ffi, self.lib) = self._load_libllama_cffi(path_llama_h, path_includes, path_libllama)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tests.test-tokenizer-random",
        "documentation": {}
    },
    {
        "label": "k",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "def k(raw_key: str, arch: str) -> str:\n    return raw_key.format(arch=arch)\ndef should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_llava: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):\n        return True\n    if has_llava and name in [\"visual_projection.weight\", \"vision_model.post_layernorm.weight\", \"vision_model.post_layernorm.bias\"]:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "should_skip_tensor",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "def should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_llava: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):\n        return True\n    if has_llava and name in [\"visual_projection.weight\", \"vision_model.post_layernorm.weight\", \"vision_model.post_layernorm.bias\"]:\n        return True\n    if name.startswith(\"v\") and not has_vision:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "get_tensor_name",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "def get_tensor_name(name: str) -> str:\n    # Standardize the transformers llava next keys for\n    # image newline / mm projector with the classes in haotian-liu LLaVA\n    if name == \"image_newline\":\n        return \"model.image_newline\"\n    if name.startswith(\"multi_modal_projector\"):\n        name = name.replace(\"multi_modal_projector\", \"mm\")\n        if \"linear_1\" in name:\n            name = name.replace(\"linear_1\", \"0\")\n        if \"linear_2\" in name:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "bytes_to_unicode",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "def bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a significant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "get_non_negative_vision_feature_layers",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "def get_non_negative_vision_feature_layers(v_hparams):\n    \"\"\"\n    Determine the vision feature layer(s) for the llava model, which are indices into the\n    hidden states of the visual encoder. Note that the hidden states array generally takes the\n    form:\n        [<emb input>, <output of enc block 0>, ... <output of enc block num_hidden_layers>]\n    so feature indices should be offset as n+1 to get the output of encoder block n.\n    We convert all vision feature layers to non-negative so that -1 can be used in\n    the model as an unset value. If no vision feature layer is found, we leave it unset.\n    \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "TEXT",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "TEXT = \"clip.text\"\nVISION = \"clip.vision\"\ndef k(raw_key: str, arch: str) -> str:\n    return raw_key.format(arch=arch)\ndef should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_llava: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "VISION",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "VISION = \"clip.vision\"\ndef k(raw_key: str, arch: str) -> str:\n    return raw_key.format(arch=arch)\ndef should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_llava: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):\n        return True",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "ap = argparse.ArgumentParser()\nap.add_argument(\"-m\", \"--model-dir\", help=\"Path to model directory cloned from HF Hub\", required=True)\nap.add_argument(\"--use-f32\", action=\"store_true\", default=False, help=\"Use f32 instead of f16\")\nap.add_argument('--bigendian', action=\"store_true\", default=False, help=\"Model is executed on big-endian machine\")\nap.add_argument(\"--text-only\", action=\"store_true\", required=False,\n                help=\"Save a text-only model. It can't be used to encode images\")\nap.add_argument(\"--vision-only\", action=\"store_true\", required=False,\n                help=\"Save a vision-only model. It can't be used to encode texts\")\nap.add_argument(\"--clip-model-is-vision\", action=\"store_true\", required=False,\n                help=\"The clip model is a pure vision model (ShareGPT4V vision extract for example)\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "encoder_group",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "encoder_group = ap.add_mutually_exclusive_group()\nencoder_group.add_argument(\"--clip-model-is-openclip\", action=\"store_true\", required=False,\n                help=\"The clip model is from openclip (for ViT-SO400M type))\")\nencoder_group.add_argument(\"--clip-model-is-siglip\", action=\"store_true\", required=False,\n                help=\"the visual encoder is Siglip.\")\nap.add_argument(\"--llava-projector\", help=\"Path to llava.projector file. If specified, save an image encoder for LLaVA models.\")\nap.add_argument(\"--projector-type\", help=\"Type of projector. Possible values: mlp, ldp, ldpv2\", choices=[\"mlp\", \"ldp\", \"ldpv2\"], default=\"mlp\")\nap.add_argument(\"-o\", \"--output-dir\", help=\"Directory to save GGUF files. Default is the original model directory\", default=None)\n# Example --image_mean 0.48145466 0.4578275 0.40821073 --image_std 0.26862954 0.26130258 0.27577711\n# Example --image_mean 0.5 0.5 0.5 --image_std 0.5 0.5 0.5",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "default_image_mean",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "default_image_mean = [0.48145466, 0.4578275, 0.40821073]\ndefault_image_std = [0.26862954, 0.26130258, 0.27577711]\nap.add_argument('--image-mean', type=float, nargs='+', help='Mean of the images for normalization (overrides processor) ', default=None)\nap.add_argument('--image-std', type=float, nargs='+', help='Standard deviation of the images for normalization (overrides processor)', default=None)\n# with proper\nargs = ap.parse_args()\nif args.text_only and args.vision_only:\n    print(\"--text-only and --image-only arguments cannot be specified at the same time.\")\n    exit(1)\nif args.use_f32:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "default_image_std",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "default_image_std = [0.26862954, 0.26130258, 0.27577711]\nap.add_argument('--image-mean', type=float, nargs='+', help='Mean of the images for normalization (overrides processor) ', default=None)\nap.add_argument('--image-std', type=float, nargs='+', help='Standard deviation of the images for normalization (overrides processor)', default=None)\n# with proper\nargs = ap.parse_args()\nif args.text_only and args.vision_only:\n    print(\"--text-only and --image-only arguments cannot be specified at the same time.\")\n    exit(1)\nif args.use_f32:\n    print(\"WARNING: Weights for the convolution op is always saved in f16, as the convolution op in GGML does not support 32-bit kernel weights yet.\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "args = ap.parse_args()\nif args.text_only and args.vision_only:\n    print(\"--text-only and --image-only arguments cannot be specified at the same time.\")\n    exit(1)\nif args.use_f32:\n    print(\"WARNING: Weights for the convolution op is always saved in f16, as the convolution op in GGML does not support 32-bit kernel weights yet.\")\n# output in the same directory as the model if output_dir is None\ndir_model = args.model_dir\nif (\n    args.clip_model_is_vision or",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "dir_model",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "dir_model = args.model_dir\nif (\n    args.clip_model_is_vision or\n    not os.path.exists(dir_model + \"/vocab.json\") or\n    args.clip_model_is_openclip or\n    args.clip_model_is_siglip\n):\n    vocab = None\n    tokens = None\nelse:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "ftype_str",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "ftype_str = [\"f32\", \"f16\"]\nftype = 1\nif args.use_f32:\n    ftype = 0\nif args.clip_model_is_siglip:\n    model = SiglipVisionModel.from_pretrained(dir_model)\n    processor = None\nelif args.clip_model_is_vision or args.clip_model_is_openclip:\n    model = CLIPVisionModel.from_pretrained(dir_model)\n    processor = None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "ftype",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "ftype = 1\nif args.use_f32:\n    ftype = 0\nif args.clip_model_is_siglip:\n    model = SiglipVisionModel.from_pretrained(dir_model)\n    processor = None\nelif args.clip_model_is_vision or args.clip_model_is_openclip:\n    model = CLIPVisionModel.from_pretrained(dir_model)\n    processor = None\nelse:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "fname_middle",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "fname_middle = None\nhas_text_encoder = True\nhas_vision_encoder = True\nhas_llava_projector = False\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.llava_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "has_text_encoder",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "has_text_encoder = True\nhas_vision_encoder = True\nhas_llava_projector = False\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.llava_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False\n    has_llava_projector = True",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "has_vision_encoder",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "has_vision_encoder = True\nhas_llava_projector = False\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.llava_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False\n    has_llava_projector = True\nelif args.vision_only:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "has_llava_projector",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "has_llava_projector = False\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.llava_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False\n    has_llava_projector = True\nelif args.vision_only:\n    fname_middle = \"vision-\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "output_dir",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "output_dir = args.output_dir if args.output_dir is not None else dir_model\nos.makedirs(output_dir, exist_ok=True)\noutput_prefix = os.path.basename(output_dir).replace(\"ggml_\", \"\")\nfname_out = os.path.join(output_dir, f\"{fname_middle}model-{ftype_str[ftype]}.gguf\")\nfout = GGUFWriter(path=fname_out, arch=\"clip\", endianess=GGUFEndian.LITTLE if not args.bigendian else GGUFEndian.BIG)\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_llava_projector\", has_llava_projector)\nfout.add_file_type(ftype)\nmodel_name = config[\"_name_or_path\"] if \"_name_or_path\" in config else os.path.basename(dir_model)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "output_prefix",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "output_prefix = os.path.basename(output_dir).replace(\"ggml_\", \"\")\nfname_out = os.path.join(output_dir, f\"{fname_middle}model-{ftype_str[ftype]}.gguf\")\nfout = GGUFWriter(path=fname_out, arch=\"clip\", endianess=GGUFEndian.LITTLE if not args.bigendian else GGUFEndian.BIG)\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_llava_projector\", has_llava_projector)\nfout.add_file_type(ftype)\nmodel_name = config[\"_name_or_path\"] if \"_name_or_path\" in config else os.path.basename(dir_model)\nfout.add_name(model_name)\nif args.text_only:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "fname_out",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "fname_out = os.path.join(output_dir, f\"{fname_middle}model-{ftype_str[ftype]}.gguf\")\nfout = GGUFWriter(path=fname_out, arch=\"clip\", endianess=GGUFEndian.LITTLE if not args.bigendian else GGUFEndian.BIG)\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_llava_projector\", has_llava_projector)\nfout.add_file_type(ftype)\nmodel_name = config[\"_name_or_path\"] if \"_name_or_path\" in config else os.path.basename(dir_model)\nfout.add_name(model_name)\nif args.text_only:\n    fout.add_description(\"text-only CLIP model\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "fout",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "fout = GGUFWriter(path=fname_out, arch=\"clip\", endianess=GGUFEndian.LITTLE if not args.bigendian else GGUFEndian.BIG)\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_llava_projector\", has_llava_projector)\nfout.add_file_type(ftype)\nmodel_name = config[\"_name_or_path\"] if \"_name_or_path\" in config else os.path.basename(dir_model)\nfout.add_name(model_name)\nif args.text_only:\n    fout.add_description(\"text-only CLIP model\")\nelif args.vision_only and not has_llava_projector:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "model_name = config[\"_name_or_path\"] if \"_name_or_path\" in config else os.path.basename(dir_model)\nfout.add_name(model_name)\nif args.text_only:\n    fout.add_description(\"text-only CLIP model\")\nelif args.vision_only and not has_llava_projector:\n    fout.add_description(\"vision-only CLIP model\")\nelif has_llava_projector:\n    fout.add_description(\"image encoder for LLaVA\")\n    # add projector type\n    fout.add_string(\"clip.projector_type\", args.projector_type)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "feature_layers",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "feature_layers = get_non_negative_vision_feature_layers(v_hparams)\nif has_vision_encoder:\n    # Siglip does not have a visual projector; set projection dim to 0\n    if args.clip_model_is_siglip:\n        visual_projection_dim = 0\n    else:\n        visual_projection_dim = v_hparams.get(\"projection_dim\", config[\"projection_dim\"])\n    # set vision_model hparams\n    fout.add_uint32(\"clip.vision.image_size\", v_hparams[\"image_size\"])\n    fout.add_uint32(\"clip.vision.patch_size\", v_hparams[\"patch_size\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "use_gelu",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "use_gelu = v_hparams[\"hidden_act\"] == \"gelu\"\nfout.add_bool(\"clip.use_gelu\", use_gelu)\nif has_llava_projector:\n    # By default, we drop the last layer for llava projector\n    # models unless we have explicitly set vision feature layers\n    if feature_layers is None:\n        model.vision_model.encoder.layers.pop(-1)\n    else:\n        model.vision_model.encoder.layers = model.vision_model.encoder.layers[:max(feature_layers)]\n    projector = torch.load(args.llava_projector)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "state_dict",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "peekOfCode": "state_dict = model.state_dict()\nfor name, data in state_dict.items():\n    if should_skip_tensor(name, has_text_encoder, has_vision_encoder, has_llava_projector):\n        # we don't need this\n        print(f\"skipping parameter: {name}\")\n        continue\n    name = get_tensor_name(name)\n    data = data.squeeze().numpy()\n    n_dims = len(data.shape)\n    # ftype == 0 -> float32, ftype == 1 -> float16",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.convert_image_encoder_to_gguf",
        "documentation": {}
    },
    {
        "label": "k",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "def k(raw_key: str, arch: str) -> str:\n    return raw_key.format(arch=arch)\ndef should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_llava: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):\n        return True\n    if name in (",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "should_skip_tensor",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "def should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_llava: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):\n        return True\n    if name in (\n        \"vision_model.head.probe\",\n        \"vision_model.head.attention.in_proj_weight\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "get_tensor_name",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "def get_tensor_name(name: str) -> str:\n    if \"projection\" in name:\n        return name\n    if \"mm_projector\" in name:\n        name = name.replace(\"model.mm_projector\", \"mm\")\n        name = re.sub(r'mm\\.mlp\\.mlp', 'mm.model.mlp', name, count=1)\n        name = re.sub(r'mm\\.peg\\.peg', 'mm.model.peg', name, count=1)\n        return name\n    return name.replace(\"text_model\", \"t\").replace(\"vision_model\", \"v\").replace(\"encoder.layers\", \"blk\").replace(\"embeddings.\", \"\").replace(\"_proj\", \"\").replace(\"self_attn.\", \"attn_\").replace(\"layer_norm\", \"ln\").replace(\"layernorm\", \"ln\").replace(\"mlp.fc1\", \"ffn_down\").replace(\"mlp.fc2\", \"ffn_up\").replace(\"embedding\", \"embd\").replace(\"final\", \"post\").replace(\"layrnorm\", \"ln\")\ndef bytes_to_unicode():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "bytes_to_unicode",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "def bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a significant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "TEXT",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "TEXT = \"clip.text\"\nVISION = \"clip.vision\"\nfrom transformers import SiglipVisionModel, SiglipVisionConfig\ndef k(raw_key: str, arch: str) -> str:\n    return raw_key.format(arch=arch)\ndef should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_llava: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "VISION",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "VISION = \"clip.vision\"\nfrom transformers import SiglipVisionModel, SiglipVisionConfig\ndef k(raw_key: str, arch: str) -> str:\n    return raw_key.format(arch=arch)\ndef should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_llava: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "ap = argparse.ArgumentParser()\nap.add_argument(\"-m\", \"--model-dir\", help=\"Path to model directory cloned from HF Hub\", required=True)\nap.add_argument(\"--use-f32\", action=\"store_true\", default=False, help=\"Use f32 instead of f16\")\nap.add_argument(\"--text-only\", action=\"store_true\", required=False,\n                help=\"Save a text-only model. It can't be used to encode images\")\nap.add_argument(\"--vision-only\", action=\"store_true\", required=False,\n                help=\"Save a vision-only model. It can't be used to encode texts\")\nap.add_argument(\"--clip-model-is-vision\", action=\"store_true\", required=False,\n                help=\"The clip model is a pure vision model (ShareGPT4V vision extract for example)\")\nap.add_argument(\"--clip-model-is-openclip\", action=\"store_true\", required=False,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "default_image_mean",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "default_image_mean = [0.5, 0.5, 0.5]\ndefault_image_std = [0.5, 0.5, 0.5]\nap.add_argument('--image-mean', type=float, nargs='+', help='Mean of the images for normalization (overrides processor) ', default=None)\nap.add_argument('--image-std', type=float, nargs='+', help='Standard deviation of the images for normalization (overrides processor)', default=None)\n# with proper\nargs = ap.parse_args()\nif args.text_only and args.vision_only:\n    print(\"--text-only and --image-only arguments cannot be specified at the same time.\")\n    exit(1)\nif args.use_f32:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "default_image_std",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "default_image_std = [0.5, 0.5, 0.5]\nap.add_argument('--image-mean', type=float, nargs='+', help='Mean of the images for normalization (overrides processor) ', default=None)\nap.add_argument('--image-std', type=float, nargs='+', help='Standard deviation of the images for normalization (overrides processor)', default=None)\n# with proper\nargs = ap.parse_args()\nif args.text_only and args.vision_only:\n    print(\"--text-only and --image-only arguments cannot be specified at the same time.\")\n    exit(1)\nif args.use_f32:\n    print(\"WARNING: Weights for the convolution op is always saved in f16, as the convolution op in GGML does not support 32-bit kernel weights yet.\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "args = ap.parse_args()\nif args.text_only and args.vision_only:\n    print(\"--text-only and --image-only arguments cannot be specified at the same time.\")\n    exit(1)\nif args.use_f32:\n    print(\"WARNING: Weights for the convolution op is always saved in f16, as the convolution op in GGML does not support 32-bit kernel weights yet.\")\n# output in the same directory as the model if output_dir is None\ndir_model = args.model_dir\nif args.clip_model_is_vision or not os.path.exists(dir_model + \"/vocab.json\") or args.clip_model_is_openclip:\n    vocab = None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "dir_model",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "dir_model = args.model_dir\nif args.clip_model_is_vision or not os.path.exists(dir_model + \"/vocab.json\") or args.clip_model_is_openclip:\n    vocab = None\n    tokens = None\nelse:\n    with open(dir_model + \"/vocab.json\", \"r\", encoding=\"utf-8\") as f:\n        vocab = json.load(f)\n        tokens = [key for key in vocab]\nwith open(dir_model + \"/config.json\", \"r\", encoding=\"utf-8\") as f:\n    config = json.load(f)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "ftype_str",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "ftype_str = [\"f32\", \"f16\"]\nftype = 1\nif args.use_f32:\n    ftype = 0\nvision_config = SiglipVisionConfig(**v_hparams)\nmodel = SiglipVisionModel(vision_config)\nmodel.load_state_dict(torch.load(os.path.join(dir_model, \"glm.clip\")))\nfname_middle = None\nhas_text_encoder = False\nhas_vision_encoder = True",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "ftype",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "ftype = 1\nif args.use_f32:\n    ftype = 0\nvision_config = SiglipVisionConfig(**v_hparams)\nmodel = SiglipVisionModel(vision_config)\nmodel.load_state_dict(torch.load(os.path.join(dir_model, \"glm.clip\")))\nfname_middle = None\nhas_text_encoder = False\nhas_vision_encoder = True\nhas_glm_projector = True",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "vision_config",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "vision_config = SiglipVisionConfig(**v_hparams)\nmodel = SiglipVisionModel(vision_config)\nmodel.load_state_dict(torch.load(os.path.join(dir_model, \"glm.clip\")))\nfname_middle = None\nhas_text_encoder = False\nhas_vision_encoder = True\nhas_glm_projector = True\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "model = SiglipVisionModel(vision_config)\nmodel.load_state_dict(torch.load(os.path.join(dir_model, \"glm.clip\")))\nfname_middle = None\nhas_text_encoder = False\nhas_vision_encoder = True\nhas_glm_projector = True\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.llava_projector is not None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "fname_middle",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "fname_middle = None\nhas_text_encoder = False\nhas_vision_encoder = True\nhas_glm_projector = True\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.llava_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "has_text_encoder",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "has_text_encoder = False\nhas_vision_encoder = True\nhas_glm_projector = True\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.llava_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False\n    has_glm_projector = True",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "has_vision_encoder",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "has_vision_encoder = True\nhas_glm_projector = True\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.llava_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False\n    has_glm_projector = True\nelif args.vision_only:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "has_glm_projector",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "has_glm_projector = True\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.llava_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False\n    has_glm_projector = True\nelif args.vision_only:\n    fname_middle = \"vision-\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "output_dir",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "output_dir = args.output_dir if args.output_dir is not None else dir_model\nos.makedirs(output_dir, exist_ok=True)\noutput_prefix = os.path.basename(output_dir).replace(\"ggml_\", \"\")\nfname_out = os.path.join(output_dir, f\"{fname_middle}model-{ftype_str[ftype]}.gguf\")\nfout = GGUFWriter(path=fname_out, arch=\"clip\")\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_glm_projector\", has_glm_projector)\nfout.add_file_type(ftype)\nmodel_name = config[\"_name_or_path\"] if \"_name_or_path\" in config else os.path.basename(dir_model)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "output_prefix",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "output_prefix = os.path.basename(output_dir).replace(\"ggml_\", \"\")\nfname_out = os.path.join(output_dir, f\"{fname_middle}model-{ftype_str[ftype]}.gguf\")\nfout = GGUFWriter(path=fname_out, arch=\"clip\")\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_glm_projector\", has_glm_projector)\nfout.add_file_type(ftype)\nmodel_name = config[\"_name_or_path\"] if \"_name_or_path\" in config else os.path.basename(dir_model)\nfout.add_name(model_name)\nif has_glm_projector:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "fname_out",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "fname_out = os.path.join(output_dir, f\"{fname_middle}model-{ftype_str[ftype]}.gguf\")\nfout = GGUFWriter(path=fname_out, arch=\"clip\")\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_glm_projector\", has_glm_projector)\nfout.add_file_type(ftype)\nmodel_name = config[\"_name_or_path\"] if \"_name_or_path\" in config else os.path.basename(dir_model)\nfout.add_name(model_name)\nif has_glm_projector:\n    fout.add_description(\"image encoder for glm4v\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "fout",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "fout = GGUFWriter(path=fname_out, arch=\"clip\")\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_glm_projector\", has_glm_projector)\nfout.add_file_type(ftype)\nmodel_name = config[\"_name_or_path\"] if \"_name_or_path\" in config else os.path.basename(dir_model)\nfout.add_name(model_name)\nif has_glm_projector:\n    fout.add_description(\"image encoder for glm4v\")\n    fout.add_string(\"clip.projector_type\", \"adapter\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "model_name = config[\"_name_or_path\"] if \"_name_or_path\" in config else os.path.basename(dir_model)\nfout.add_name(model_name)\nif has_glm_projector:\n    fout.add_description(\"image encoder for glm4v\")\n    fout.add_string(\"clip.projector_type\", \"adapter\")\nelse:\n    fout.add_description(\"two-tower CLIP model\")\nif has_text_encoder:\n    assert t_hparams is not None\n    assert tokens is not None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "state_dict",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "peekOfCode": "state_dict = model.state_dict()  # pyright: ignore[reportAttributeAccessIssue]\nfor name, data in state_dict.items():\n    if should_skip_tensor(name, has_text_encoder, has_vision_encoder, has_glm_projector):\n        # we don't need this\n        print(f\"skipping parameter: {name}\")\n        continue\n    name = get_tensor_name(name)\n    data = data.squeeze().numpy()\n    n_dims = len(data.shape)\n    # ftype == 0 -> float32, ftype == 1 -> float16",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "peekOfCode": "ap = argparse.ArgumentParser()\nap.add_argument(\"-m\", \"--model\", help=\"Path to GLM model\")\nargs = ap.parse_args()\n# find the model part that includes the the multimodal projector weights\nmodel = AutoModel.from_pretrained(args.model, trust_remote_code=True, local_files_only=True)\ncheckpoint = model.state_dict()\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.adapter.\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "peekOfCode": "args = ap.parse_args()\n# find the model part that includes the the multimodal projector weights\nmodel = AutoModel.from_pretrained(args.model, trust_remote_code=True, local_files_only=True)\ncheckpoint = model.state_dict()\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.adapter.\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\ntorch.save(projector, f\"{args.model}/glm.projector\")\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.vit.model.vision_model.\")]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "peekOfCode": "model = AutoModel.from_pretrained(args.model, trust_remote_code=True, local_files_only=True)\ncheckpoint = model.state_dict()\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.adapter.\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\ntorch.save(projector, f\"{args.model}/glm.projector\")\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.vit.model.vision_model.\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vision.vit.model.\", \"\"): checkpoint[name].float() for name in clip_tensors}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "peekOfCode": "checkpoint = model.state_dict()\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.adapter.\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\ntorch.save(projector, f\"{args.model}/glm.projector\")\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.vit.model.vision_model.\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vision.vit.model.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n    torch.save(clip, f\"{args.model}/glm.clip\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "documentation": {}
    },
    {
        "label": "mm_tensors",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "peekOfCode": "mm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.adapter.\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\ntorch.save(projector, f\"{args.model}/glm.projector\")\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.vit.model.vision_model.\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vision.vit.model.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n    torch.save(clip, f\"{args.model}/glm.clip\")\n    # added tokens should be removed to be able to convert Mistral models\n    if os.path.exists(f\"{args.model}/added_tokens.json\"):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "documentation": {}
    },
    {
        "label": "projector",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "peekOfCode": "projector = {name: checkpoint[name].float() for name in mm_tensors}\ntorch.save(projector, f\"{args.model}/glm.projector\")\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.vit.model.vision_model.\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vision.vit.model.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n    torch.save(clip, f\"{args.model}/glm.clip\")\n    # added tokens should be removed to be able to convert Mistral models\n    if os.path.exists(f\"{args.model}/added_tokens.json\"):\n        with open(f\"{args.model}/added_tokens.json\", \"w\") as f:\n            f.write(\"{}\\n\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "documentation": {}
    },
    {
        "label": "clip_tensors",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "peekOfCode": "clip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vision.vit.model.vision_model.\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vision.vit.model.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n    torch.save(clip, f\"{args.model}/glm.clip\")\n    # added tokens should be removed to be able to convert Mistral models\n    if os.path.exists(f\"{args.model}/added_tokens.json\"):\n        with open(f\"{args.model}/added_tokens.json\", \"w\") as f:\n            f.write(\"{}\\n\")\nprint(\"Done!\")\nprint(f\"Now you can convert {args.model} to a regular LLaMA GGUF file.\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.glmedge-surgery",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "peekOfCode": "ap = argparse.ArgumentParser()\nap.add_argument(\"-m\", \"--model\", help=\"Path to LLaVA v1.5 model\")\nargs = ap.parse_args()\n# find the model part that includes the the multimodal projector weights\npath = sorted(glob.glob(f\"{args.model}/pytorch_model*.bin\"))[-1]\ncheckpoint = torch.load(path)\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"model.mm_projector\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "peekOfCode": "args = ap.parse_args()\n# find the model part that includes the the multimodal projector weights\npath = sorted(glob.glob(f\"{args.model}/pytorch_model*.bin\"))[-1]\ncheckpoint = torch.load(path)\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"model.mm_projector\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\ntorch.save(projector, f\"{args.model}/llava.projector\")\n# BakLLaVA models contain CLIP tensors in it",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "peekOfCode": "path = sorted(glob.glob(f\"{args.model}/pytorch_model*.bin\"))[-1]\ncheckpoint = torch.load(path)\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"model.mm_projector\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\ntorch.save(projector, f\"{args.model}/llava.projector\")\n# BakLLaVA models contain CLIP tensors in it\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"model.vision_tower\")]\nif len(clip_tensors) > 0:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "peekOfCode": "checkpoint = torch.load(path)\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"model.mm_projector\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\ntorch.save(projector, f\"{args.model}/llava.projector\")\n# BakLLaVA models contain CLIP tensors in it\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"model.vision_tower\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vision_tower.vision_tower.\", \"\"): checkpoint[name].float() for name in clip_tensors}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "documentation": {}
    },
    {
        "label": "mm_tensors",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "peekOfCode": "mm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"model.mm_projector\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\ntorch.save(projector, f\"{args.model}/llava.projector\")\n# BakLLaVA models contain CLIP tensors in it\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"model.vision_tower\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vision_tower.vision_tower.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n    torch.save(clip, f\"{args.model}/llava.clip\")\n    # added tokens should be removed to be able to convert Mistral models",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "documentation": {}
    },
    {
        "label": "projector",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "peekOfCode": "projector = {name: checkpoint[name].float() for name in mm_tensors}\ntorch.save(projector, f\"{args.model}/llava.projector\")\n# BakLLaVA models contain CLIP tensors in it\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"model.vision_tower\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vision_tower.vision_tower.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n    torch.save(clip, f\"{args.model}/llava.clip\")\n    # added tokens should be removed to be able to convert Mistral models\n    if os.path.exists(f\"{args.model}/added_tokens.json\"):\n        with open(f\"{args.model}/added_tokens.json\", \"w\") as f:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "documentation": {}
    },
    {
        "label": "clip_tensors",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "peekOfCode": "clip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"model.vision_tower\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vision_tower.vision_tower.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n    torch.save(clip, f\"{args.model}/llava.clip\")\n    # added tokens should be removed to be able to convert Mistral models\n    if os.path.exists(f\"{args.model}/added_tokens.json\"):\n        with open(f\"{args.model}/added_tokens.json\", \"w\") as f:\n            f.write(\"{}\\n\")\nprint(\"Done!\")\nprint(f\"Now you can convert {args.model} to a regular LLaMA GGUF file.\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery",
        "documentation": {}
    },
    {
        "label": "is_safetensor_file",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "def is_safetensor_file(file_path):\n    return file_path.endswith('.safetensors')\n# Unified loading function\ndef load_model(file_path):\n    if is_safetensor_file(file_path):\n        tensors = {}\n        with cast(ContextManager[Any], safe_open(file_path, framework=\"pt\", device=\"cpu\")) as f:\n            for key in f.keys():\n                tensors[key] = f.get_tensor(key).clone()\n                # output shape",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "def load_model(file_path):\n    if is_safetensor_file(file_path):\n        tensors = {}\n        with cast(ContextManager[Any], safe_open(file_path, framework=\"pt\", device=\"cpu\")) as f:\n            for key in f.keys():\n                tensors[key] = f.get_tensor(key).clone()\n                # output shape\n                print(f\"{key} : {tensors[key].shape}\")\n        return tensors, 'safetensor'\n    else:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "save_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "def save_model(model, file_path, file_type):\n    if file_type == 'safetensor':\n        # safe_save(model, file_path)\n        save_file(model, file_path)\n    else:\n        torch.save(model, file_path)\n# Helpers to match weight names from specific components or\n# determine if a saved shard contains that component\ndef is_vision_tower(weight_name):\n    return (",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "is_vision_tower",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "def is_vision_tower(weight_name):\n    return (\n        weight_name.startswith(\"model.vision_tower\") or\n        weight_name.startswith(\"vit.\") or\n        weight_name.startswith(\"vision_tower\")\n    )\ndef is_newline(weight_name):\n    return (\n        weight_name.startswith(\"model.image_newline\") or\n        weight_name.startswith(\"image_newline\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "is_newline",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "def is_newline(weight_name):\n    return (\n        weight_name.startswith(\"model.image_newline\") or\n        weight_name.startswith(\"image_newline\")\n    )\ndef is_mm_projector(weight_name):\n    return (\n        weight_name.startswith(\"model.mm_projector\") or\n        weight_name.startswith(\"vision_proj.\") or\n        weight_name.startswith(\"multi_modal_projector\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "is_mm_projector",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "def is_mm_projector(weight_name):\n    return (\n        weight_name.startswith(\"model.mm_projector\") or\n        weight_name.startswith(\"vision_proj.\") or\n        weight_name.startswith(\"multi_modal_projector\")\n    )\ndef newline_criteria(checkpoint):\n    return any(is_newline(k) for k in checkpoint.keys())\ndef proj_criteria(checkpoint):\n    return any(is_mm_projector(k) for k in checkpoint.keys())",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "newline_criteria",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "def newline_criteria(checkpoint):\n    return any(is_newline(k) for k in checkpoint.keys())\ndef proj_criteria(checkpoint):\n    return any(is_mm_projector(k) for k in checkpoint.keys())\n# Adapted function to clean vision tower from checkpoint\ndef clean_vision_tower_from_checkpoint(checkpoint_path):\n    checkpoint, file_type = load_model(checkpoint_path)\n    # file_type = 'pytorch'\n    model_path = os.path.dirname(checkpoint_path)\n    print(f\"Searching for vision tower tensors in {checkpoint_path}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "proj_criteria",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "def proj_criteria(checkpoint):\n    return any(is_mm_projector(k) for k in checkpoint.keys())\n# Adapted function to clean vision tower from checkpoint\ndef clean_vision_tower_from_checkpoint(checkpoint_path):\n    checkpoint, file_type = load_model(checkpoint_path)\n    # file_type = 'pytorch'\n    model_path = os.path.dirname(checkpoint_path)\n    print(f\"Searching for vision tower tensors in {checkpoint_path}\")\n    clip_tensors = [k for k, v in checkpoint.items() if is_vision_tower(k)]\n    if len(clip_tensors) > 0:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "clean_vision_tower_from_checkpoint",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "def clean_vision_tower_from_checkpoint(checkpoint_path):\n    checkpoint, file_type = load_model(checkpoint_path)\n    # file_type = 'pytorch'\n    model_path = os.path.dirname(checkpoint_path)\n    print(f\"Searching for vision tower tensors in {checkpoint_path}\")\n    clip_tensors = [k for k, v in checkpoint.items() if is_vision_tower(k)]\n    if len(clip_tensors) > 0:\n        print(f\"Found {len(clip_tensors)} tensors to extract from {checkpoint_path}\")\n        # Adapted for file type\n        clip_path = os.path.join(model_path, \"llava.clip\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "find_relevant_checkpoints",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "def find_relevant_checkpoints(checkpoint_paths, newline_criteria, projector):\n    newline_checkpoint_path = None\n    projector_checkpoint_path = None\n    for path in checkpoint_paths:\n        checkpoint, _ = load_model(path)\n        if newline_criteria(checkpoint) and newline_checkpoint_path is None:\n            newline_checkpoint_path = path\n        if projector(checkpoint):\n            projector_checkpoint_path = path\n    return newline_checkpoint_path, projector_checkpoint_path",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "ap = argparse.ArgumentParser()\nap.add_argument(\"-m\", \"--model\", required=True, help=\"Path to LLaVA v1.5+ model\")\nap.add_argument(\"-C\", \"--clean-vision-tower\", action=\"store_true\", help=\"Remove any vision tower from the model files\")\nargs = ap.parse_args()\nif args.clean_vision_tower:\n    # Generalized to handle both PyTorch and SafeTensors models\n    model_files = sorted(glob.glob(f\"{args.model}/*\"), key=os.path.getmtime, reverse=True)\n    # checkpoint_paths = [path for path in model_files if (path.endswith('.bin') and path.startswith('pytorch')) or (path.endswith('.safetensors') and path.startswith('model'))]\n    checkpoint_paths = [path for path in model_files if (path.endswith('.bin') and 'pytorch' in path.split('/')[-1].split('\\\\')[-1]) or (path.endswith('.safetensors') and 'model' in path.split('/')[-1].split('\\\\')[-1])]\n    for projector_checkpoint_path in checkpoint_paths:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "args = ap.parse_args()\nif args.clean_vision_tower:\n    # Generalized to handle both PyTorch and SafeTensors models\n    model_files = sorted(glob.glob(f\"{args.model}/*\"), key=os.path.getmtime, reverse=True)\n    # checkpoint_paths = [path for path in model_files if (path.endswith('.bin') and path.startswith('pytorch')) or (path.endswith('.safetensors') and path.startswith('model'))]\n    checkpoint_paths = [path for path in model_files if (path.endswith('.bin') and 'pytorch' in path.split('/')[-1].split('\\\\')[-1]) or (path.endswith('.safetensors') and 'model' in path.split('/')[-1].split('\\\\')[-1])]\n    for projector_checkpoint_path in checkpoint_paths:\n        print(f\"Cleaning {projector_checkpoint_path}\")\n        if not clean_vision_tower_from_checkpoint(projector_checkpoint_path):\n            print(f\"No vision tower found in {projector_checkpoint_path}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "model_files",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "model_files = sorted(glob.glob(f\"{args.model}/*\"), key=os.path.getmtime, reverse=True)\ncheckpoint_paths = [path for path in model_files if (path.endswith('.bin') and 'pytorch' in path.split('/')[-1].split('\\\\')[-1]) or (path.endswith('.safetensors') and 'model' in path.split('/')[-1].split('\\\\')[-1])]\n# last_checkpoint_path = checkpoint_paths[0]\n# first_checkpoint_path = checkpoint_paths[-1]\nnewline_checkpoint_path, projector_checkpoint_path = find_relevant_checkpoints(checkpoint_paths, newline_criteria, proj_criteria)\nprint(f\"Taking projector from {projector_checkpoint_path}\")\nfirst_mm_tensors = []\nfirst_checkpoint = None\nif newline_checkpoint_path is not None:\n    print(f\"Taking newline from {newline_checkpoint_path}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "checkpoint_paths",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "checkpoint_paths = [path for path in model_files if (path.endswith('.bin') and 'pytorch' in path.split('/')[-1].split('\\\\')[-1]) or (path.endswith('.safetensors') and 'model' in path.split('/')[-1].split('\\\\')[-1])]\n# last_checkpoint_path = checkpoint_paths[0]\n# first_checkpoint_path = checkpoint_paths[-1]\nnewline_checkpoint_path, projector_checkpoint_path = find_relevant_checkpoints(checkpoint_paths, newline_criteria, proj_criteria)\nprint(f\"Taking projector from {projector_checkpoint_path}\")\nfirst_mm_tensors = []\nfirst_checkpoint = None\nif newline_checkpoint_path is not None:\n    print(f\"Taking newline from {newline_checkpoint_path}\")\n    first_checkpoint, file_type = load_model(newline_checkpoint_path)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "first_mm_tensors",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "first_mm_tensors = []\nfirst_checkpoint = None\nif newline_checkpoint_path is not None:\n    print(f\"Taking newline from {newline_checkpoint_path}\")\n    first_checkpoint, file_type = load_model(newline_checkpoint_path)\n    first_mm_tensors = [k for k, v in first_checkpoint.items() if is_newline(k)]\n# Load the checkpoint\nmm_tensors = []\nlast_checkpoint = None\nif projector_checkpoint_path is not None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "first_checkpoint",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "first_checkpoint = None\nif newline_checkpoint_path is not None:\n    print(f\"Taking newline from {newline_checkpoint_path}\")\n    first_checkpoint, file_type = load_model(newline_checkpoint_path)\n    first_mm_tensors = [k for k, v in first_checkpoint.items() if is_newline(k)]\n# Load the checkpoint\nmm_tensors = []\nlast_checkpoint = None\nif projector_checkpoint_path is not None:\n    last_checkpoint, file_type = load_model(projector_checkpoint_path)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "mm_tensors",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "mm_tensors = []\nlast_checkpoint = None\nif projector_checkpoint_path is not None:\n    last_checkpoint, file_type = load_model(projector_checkpoint_path)\n    mm_tensors = [k for k, v in last_checkpoint.items() if is_mm_projector(k)]\nif len(mm_tensors) == 0:\n    if last_checkpoint is not None:\n        for k, v in last_checkpoint.items():\n            print(k)\n    print(f\"Found {len(mm_tensors)} tensors to extract out of {len(last_checkpoint) if last_checkpoint is not None else 0} tensors.\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "last_checkpoint",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "last_checkpoint = None\nif projector_checkpoint_path is not None:\n    last_checkpoint, file_type = load_model(projector_checkpoint_path)\n    mm_tensors = [k for k, v in last_checkpoint.items() if is_mm_projector(k)]\nif len(mm_tensors) == 0:\n    if last_checkpoint is not None:\n        for k, v in last_checkpoint.items():\n            print(k)\n    print(f\"Found {len(mm_tensors)} tensors to extract out of {len(last_checkpoint) if last_checkpoint is not None else 0} tensors.\")\n    print(\"No tensors found. Is this a LLaVA model?\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "projector",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "peekOfCode": "projector = {}\nfor name in mm_tensors:\n    assert last_checkpoint is not None\n    projector[name] = last_checkpoint[name].float()\nfor name in first_mm_tensors:\n    assert first_checkpoint is not None\n    projector[name] = first_checkpoint[name].float()\nif len(projector) > 0:\n    save_model(projector, f\"{args.model}/llava.projector\", 'pytorch')\nprint(\"Done!\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.llava_surgery_v2",
        "documentation": {}
    },
    {
        "label": "SiglipVisionConfig",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "class SiglipVisionConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`SiglipVisionModel`]. It is used to instantiate a\n    Siglip vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the vision encoder of the Siglip\n    [google/siglip-base-patch16-224](https://huggingface.co/google/siglip-base-patch16-224) architecture.\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n    Args:\n        hidden_size (`int`, *optional*, defaults to 768):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "SiglipVisionEmbeddings",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "class SiglipVisionEmbeddings(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n        self.patch_embedding = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=self.embed_dim,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "SiglipAttention",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "class SiglipAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n    # Copied from transformers.models.clip.modeling_clip.CLIPAttention.__init__\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        if self.head_dim * self.num_heads != self.embed_dim:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "SiglipMLP",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "class SiglipMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.activation_fn = ACT2FN[config.hidden_act]\n        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n# Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->Siglip\nclass SiglipEncoderLayer(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "SiglipEncoderLayer",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "class SiglipEncoderLayer(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n        self.self_attn = (\n            SiglipAttention(config)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.mlp = SiglipMLP(config)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "SiglipPreTrainedModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "class SiglipPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n    config_class = SiglipVisionConfig\n    base_model_prefix = \"siglip\"\n    supports_gradient_checkpointing = True\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "SiglipEncoder",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "class SiglipEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n    [`SiglipEncoderLayer`].\n    Args:\n        config: SiglipConfig\n    \"\"\"\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "SiglipVisionTransformer",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "class SiglipVisionTransformer(SiglipPreTrainedModel):\n    config_class = SiglipVisionConfig\n    main_input_name = \"pixel_values\"\n    _supports_flash_attn_2 = True\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__(config)\n        self.config = config\n        embed_dim = config.hidden_size\n        self.embeddings = SiglipVisionEmbeddings(config)\n        self.encoder = SiglipEncoder(config)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "trunc_normal_tf_",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def trunc_normal_tf_(\n    tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0\n):\n    \"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\\\leq \\text{mean} \\\\leq b`.\n    NOTE: this 'tf' variant behaves closer to Tensorflow / JAX impl where the",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "variance_scaling_",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def variance_scaling_(tensor, scale=1.0, mode=\"fan_in\", distribution=\"normal\"):\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    denom = fan_in\n    if mode == \"fan_in\":\n        denom = fan_in\n    elif mode == \"fan_out\":\n        denom = fan_out\n    elif mode == \"fan_avg\":\n        denom = (fan_in + fan_out) / 2\n    variance = scale / denom",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "lecun_normal_",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def lecun_normal_(tensor):\n    variance_scaling_(tensor, mode=\"fan_in\", distribution=\"truncated_normal\")\ndef default_flax_embed_init(tensor):\n    variance_scaling_(tensor, mode=\"fan_in\", distribution=\"normal\")\nclass SiglipVisionEmbeddings(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "default_flax_embed_init",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def default_flax_embed_init(tensor):\n    variance_scaling_(tensor, mode=\"fan_in\", distribution=\"normal\")\nclass SiglipVisionEmbeddings(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n        self.patch_embedding = nn.Conv2d(",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "add_key_str",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def add_key_str(raw_key: str, arch: str) -> str:\n    return raw_key.format(arch=arch)\ndef should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_minicpmv: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):\n        return True\n    if has_minicpmv and name in [\"visual_projection.weight\"]:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "should_skip_tensor",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_minicpmv: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):\n        return True\n    if has_minicpmv and name in [\"visual_projection.weight\"]:\n        return True\n    if name.startswith(\"v\") and not has_vision:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "get_tensor_name",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def get_tensor_name(name: str) -> str:\n    if \"projection\" in name:\n        return name\n    if \"mm_projector\" in name:\n        name = name.replace(\"model.mm_projector\", \"mm\")\n        name = re.sub(r'mm\\.mlp\\.mlp', 'mm.model.mlp', name, count=1)\n        name = re.sub(r'mm\\.peg\\.peg', 'mm.model.peg', name, count=1)\n        return name\n    return name.replace(\"text_model\", \"t\").replace(\"vision_model\", \"v\").replace(\"encoder.layers\", \"blk\").replace(\"embeddings.\", \"\").replace(\"_proj\", \"\").replace(\"self_attn.\", \"attn_\").replace(\"layer_norm\", \"ln\").replace(\"layernorm\", \"ln\").replace(\"mlp.fc1\", \"ffn_down\").replace(\"mlp.fc2\", \"ffn_up\").replace(\"embedding\", \"embd\").replace(\"final\", \"post\").replace(\"layrnorm\", \"ln\")\ndef bytes_to_unicode():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "bytes_to_unicode",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a significant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "get_1d_sincos_pos_embed_from_grid",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float32)\n    omega /= embed_dim / 2.\n    omega = 1. / 10000 ** omega  # (D/2,)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "get_2d_sincos_pos_embed_from_grid",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n    return emb\n# https://github.com/facebookresearch/mae/blob/efb2a8062c206524e35e47d04501ed4f544c0ae8/util/pos_embed.py#L20\ndef get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "get_2d_sincos_pos_embed",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    if isinstance(grid_size, int):\n        grid_h_size, grid_w_size = grid_size, grid_size\n    else:\n        grid_h_size, grid_w_size = grid_size[0], grid_size[1]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass SiglipVisionConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`SiglipVisionModel`]. It is used to instantiate a\n    Siglip vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the vision encoder of the Siglip\n    [google/siglip-base-patch16-224](https://huggingface.co/google/siglip-base-patch16-224) architecture.\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n    Args:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "_CHECKPOINT_FOR_DOC",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "_CHECKPOINT_FOR_DOC = \"google/siglip-base-patch16-224\"\nSIGLIP_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"google/siglip-base-patch16-224\",\n    # See all SigLIP models at https://huggingface.co/models?filter=siglip\n]\n# Copied from transformers.models.llama.modeling_llama._get_unpad_data\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "SIGLIP_PRETRAINED_MODEL_ARCHIVE_LIST",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "SIGLIP_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"google/siglip-base-patch16-224\",\n    # See all SigLIP models at https://huggingface.co/models?filter=siglip\n]\n# Copied from transformers.models.llama.modeling_llama._get_unpad_data\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "SIGLIP_START_DOCSTRING",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "SIGLIP_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n    Parameters:\n        config ([`SiglipVisionConfig`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "SIGLIP_VISION_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "SIGLIP_VISION_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "TEXT",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "TEXT = \"clip.text\"\nVISION = \"clip.vision\"\ndef add_key_str(raw_key: str, arch: str) -> str:\n    return raw_key.format(arch=arch)\ndef should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_minicpmv: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "VISION",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "VISION = \"clip.vision\"\ndef add_key_str(raw_key: str, arch: str) -> str:\n    return raw_key.format(arch=arch)\ndef should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_minicpmv: bool) -> bool:\n    if name in (\n        \"logit_scale\",\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    ):\n        return True",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "ap = argparse.ArgumentParser()\nap.add_argument(\"-m\", \"--model-dir\", help=\"Path to model directory cloned from HF Hub\", required=True)\nap.add_argument(\"--use-f32\", action=\"store_true\", default=False, help=\"Use f32 instead of f16\")\nap.add_argument(\"--text-only\", action=\"store_true\", required=False,\n                help=\"Save a text-only model. It can't be used to encode images\")\nap.add_argument(\"--vision-only\", action=\"store_true\", required=False,\n                help=\"Save a vision-only model. It can't be used to encode texts\")\nap.add_argument(\"--clip-model-is-vision\", action=\"store_true\", required=False,\n                help=\"The clip model is a pure vision model (ShareGPT4V vision extract for example)\")\nap.add_argument(\"--clip-model-is-openclip\", action=\"store_true\", required=False,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "default_image_mean",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "default_image_mean = [0.5, 0.5, 0.5]\ndefault_image_std = [0.5, 0.5, 0.5]\nap.add_argument('--image-mean', type=float, nargs='+', help='Mean of the images for normalization (overrides processor) ', default=None)\nap.add_argument('--image-std', type=float, nargs='+', help='Standard deviation of the images for normalization (overrides processor)', default=None)\nap.add_argument('--minicpmv_version', type=int, help='minicpmv_version: MiniCPM-V-2 use 1; MiniCPM-V-2.5 use 2; MiniCPM-V-2.6 use 3; MiniCPM-o-2.6 use 4; MiniCPM-V 4.0 use 5; MiniCPM-o-4.0 use 6', default=2)\n# with proper\nargs = ap.parse_args()\nif args.text_only and args.vision_only:\n    print(\"--text-only and --image-only arguments cannot be specified at the same time.\")\n    exit(1)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "default_image_std",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "default_image_std = [0.5, 0.5, 0.5]\nap.add_argument('--image-mean', type=float, nargs='+', help='Mean of the images for normalization (overrides processor) ', default=None)\nap.add_argument('--image-std', type=float, nargs='+', help='Standard deviation of the images for normalization (overrides processor)', default=None)\nap.add_argument('--minicpmv_version', type=int, help='minicpmv_version: MiniCPM-V-2 use 1; MiniCPM-V-2.5 use 2; MiniCPM-V-2.6 use 3; MiniCPM-o-2.6 use 4; MiniCPM-V 4.0 use 5; MiniCPM-o-4.0 use 6', default=2)\n# with proper\nargs = ap.parse_args()\nif args.text_only and args.vision_only:\n    print(\"--text-only and --image-only arguments cannot be specified at the same time.\")\n    exit(1)\nif args.use_f32:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "args = ap.parse_args()\nif args.text_only and args.vision_only:\n    print(\"--text-only and --image-only arguments cannot be specified at the same time.\")\n    exit(1)\nif args.use_f32:\n    print(\"WARNING: Weights for the convolution op is always saved in f16, as the convolution op in GGML does not support 32-bit kernel weights yet.\")\n# output in the same directory as the model if output_dir is None\ndir_model = args.model_dir\n# Read config.json to get actual model configuration\nconfig_path = os.path.join(dir_model, \"config.json\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "dir_model",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "dir_model = args.model_dir\n# Read config.json to get actual model configuration\nconfig_path = os.path.join(dir_model, \"config.json\")\nmodel_config = {}\nif os.path.isfile(config_path):\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        model_config = json.load(f)\n    print(f\"Loaded config from {config_path}\")\nelse:\n    print(f\"Warning: config.json not found at {config_path}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "config_path = os.path.join(dir_model, \"config.json\")\nmodel_config = {}\nif os.path.isfile(config_path):\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        model_config = json.load(f)\n    print(f\"Loaded config from {config_path}\")\nelse:\n    print(f\"Warning: config.json not found at {config_path}\")\n# If minicpmv_projector is not specified but the default path exists, use the default path\nif args.minicpmv_projector is None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "model_config",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "model_config = {}\nif os.path.isfile(config_path):\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        model_config = json.load(f)\n    print(f\"Loaded config from {config_path}\")\nelse:\n    print(f\"Warning: config.json not found at {config_path}\")\n# If minicpmv_projector is not specified but the default path exists, use the default path\nif args.minicpmv_projector is None:\n    default_projector_path = os.path.join(dir_model, \"minicpmv.projector\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "ftype_str",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "ftype_str = [\"f32\", \"f16\"]\nftype = 1\nif args.use_f32:\n    ftype = 0\n# if args.clip_model_is_vision or args.clip_model_is_openclip:\n#     model = CLIPVisionModel.from_pretrained(dir_model)\n#     processor = None\n# else:\n#     model = CLIPModel.from_pretrained(dir_model)\n#     processor = CLIPProcessor.from_pretrained(dir_model)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "ftype",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "ftype = 1\nif args.use_f32:\n    ftype = 0\n# if args.clip_model_is_vision or args.clip_model_is_openclip:\n#     model = CLIPVisionModel.from_pretrained(dir_model)\n#     processor = None\n# else:\n#     model = CLIPModel.from_pretrained(dir_model)\n#     processor = CLIPProcessor.from_pretrained(dir_model)\nminicpmv_version = args.minicpmv_version",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "minicpmv_version",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "minicpmv_version = args.minicpmv_version\n# Use actual config values instead of hardcoded ones\nif model_config:\n    # For the projector/resampler, use the main model's hidden_size\n    emb_dim = model_config.get(\"hidden_size\", 1536)\n    # For the vision model, use vision_config values\n    vision_config_dict = model_config.get(\"vision_config\", {})\n    default_vision_config = {\n        \"hidden_size\": vision_config_dict.get(\"hidden_size\", 1152),\n        \"image_size\": vision_config_dict.get(\"image_size\", 980),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "vision_config",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "vision_config = Idefics2VisionConfig(**default_vision_config)\nmodel = Idefics2VisionTransformer(vision_config)\nif minicpmv_version == 3 or (model_config and model_config.get(\"vision_config\", {}).get(\"model_type\") == \"siglip\"):\n    vision_config = SiglipVisionConfig(**default_vision_config)\n    model = SiglipVisionTransformer(vision_config)\nelif minicpmv_version == 4:\n    vision_config = SiglipVisionConfig(**default_vision_config)\n    model = SiglipVisionTransformer(vision_config)\nelif minicpmv_version == 5:\n    default_vision_config[\"model_type\"] = \"siglip_vision_model\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "model = Idefics2VisionTransformer(vision_config)\nif minicpmv_version == 3 or (model_config and model_config.get(\"vision_config\", {}).get(\"model_type\") == \"siglip\"):\n    vision_config = SiglipVisionConfig(**default_vision_config)\n    model = SiglipVisionTransformer(vision_config)\nelif minicpmv_version == 4:\n    vision_config = SiglipVisionConfig(**default_vision_config)\n    model = SiglipVisionTransformer(vision_config)\nelif minicpmv_version == 5:\n    default_vision_config[\"model_type\"] = \"siglip_vision_model\"\n    vision_config = SiglipVisionConfig(**default_vision_config)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "processor",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "processor = None\n# if model.attn_pool is not None:\n#     model.attn_pool = torch.nn.Identity()\n# model.blocks = model.blocks[:-1]\nmodel.load_state_dict(torch.load(os.path.join(dir_model, \"minicpmv.clip\")))\nfname_middle = None\nhas_text_encoder = True\nhas_vision_encoder = True\nhas_minicpmv_projector = False\nif args.text_only:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "fname_middle",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "fname_middle = None\nhas_text_encoder = True\nhas_vision_encoder = True\nhas_minicpmv_projector = False\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.minicpmv_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "has_text_encoder",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "has_text_encoder = True\nhas_vision_encoder = True\nhas_minicpmv_projector = False\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.minicpmv_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False\n    has_minicpmv_projector = True",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "has_vision_encoder",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "has_vision_encoder = True\nhas_minicpmv_projector = False\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.minicpmv_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False\n    has_minicpmv_projector = True\nelif args.vision_only:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "has_minicpmv_projector",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "has_minicpmv_projector = False\nif args.text_only:\n    fname_middle = \"text-\"\n    has_vision_encoder = False\nelif args.minicpmv_projector is not None:\n    fname_middle = \"mmproj-\"\n    has_text_encoder = False\n    has_minicpmv_projector = True\nelif args.vision_only:\n    fname_middle = \"vision-\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "output_dir",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "output_dir = args.output_dir\nos.makedirs(output_dir, exist_ok=True)\noutput_prefix = os.path.basename(output_dir).replace(\"ggml_\", \"\")\nfname_out = os.path.join(output_dir, f\"{fname_middle}model-{ftype_str[ftype]}.gguf\")\nfout = GGUFWriter(path=fname_out, arch=\"clip\")\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_minicpmv_projector\", has_minicpmv_projector)\nfout.add_file_type(ftype)\nif args.text_only:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "output_prefix",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "output_prefix = os.path.basename(output_dir).replace(\"ggml_\", \"\")\nfname_out = os.path.join(output_dir, f\"{fname_middle}model-{ftype_str[ftype]}.gguf\")\nfout = GGUFWriter(path=fname_out, arch=\"clip\")\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_minicpmv_projector\", has_minicpmv_projector)\nfout.add_file_type(ftype)\nif args.text_only:\n    fout.add_description(\"text-only CLIP model\")\nelif args.vision_only and not has_minicpmv_projector:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "fname_out",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "fname_out = os.path.join(output_dir, f\"{fname_middle}model-{ftype_str[ftype]}.gguf\")\nfout = GGUFWriter(path=fname_out, arch=\"clip\")\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_minicpmv_projector\", has_minicpmv_projector)\nfout.add_file_type(ftype)\nif args.text_only:\n    fout.add_description(\"text-only CLIP model\")\nelif args.vision_only and not has_minicpmv_projector:\n    fout.add_description(\"vision-only CLIP model\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "fout",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "fout = GGUFWriter(path=fname_out, arch=\"clip\")\nfout.add_bool(\"clip.has_text_encoder\", has_text_encoder)\nfout.add_bool(\"clip.has_vision_encoder\", has_vision_encoder)\nfout.add_bool(\"clip.has_minicpmv_projector\", has_minicpmv_projector)\nfout.add_file_type(ftype)\nif args.text_only:\n    fout.add_description(\"text-only CLIP model\")\nelif args.vision_only and not has_minicpmv_projector:\n    fout.add_description(\"vision-only CLIP model\")\nelif has_minicpmv_projector:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "use_gelu",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "use_gelu = True\nfout.add_bool(\"clip.use_gelu\", use_gelu)\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float32)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "state_dict",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "state_dict = model.state_dict()\nnew_state_dict = {}\nfor k, v in state_dict.items():\n    kvs = _replace_name(k, v)\n    for nk, nv in kvs.items():\n        new_state_dict[nk] = nv\nstate_dict = new_state_dict\nfor name, data in state_dict.items():\n    if should_skip_tensor(name, has_text_encoder, has_vision_encoder, has_minicpmv_projector):\n        # we don't need this",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "new_state_dict",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "new_state_dict = {}\nfor k, v in state_dict.items():\n    kvs = _replace_name(k, v)\n    for nk, nv in kvs.items():\n        new_state_dict[nk] = nv\nstate_dict = new_state_dict\nfor name, data in state_dict.items():\n    if should_skip_tensor(name, has_text_encoder, has_vision_encoder, has_minicpmv_projector):\n        # we don't need this\n        print(f\"skipping parameter: {name}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "state_dict",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "peekOfCode": "state_dict = new_state_dict\nfor name, data in state_dict.items():\n    if should_skip_tensor(name, has_text_encoder, has_vision_encoder, has_minicpmv_projector):\n        # we don't need this\n        print(f\"skipping parameter: {name}\")\n        continue\n    name = get_tensor_name(name)\n    data = data.squeeze().numpy()\n    n_dims = len(data.shape)\n    # ftype == 0 -> float32, ftype == 1 -> float16",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-convert-image-encoder-to-gguf",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "peekOfCode": "ap = argparse.ArgumentParser()\nap.add_argument(\"-m\", \"--model\", help=\"Path to MiniCPM-V model\")\nargs = ap.parse_args()\n# find the model part that includes the the multimodal projector weights\nmodel = AutoModel.from_pretrained(args.model, trust_remote_code=True, local_files_only=True, torch_dtype=torch.bfloat16)\ncheckpoint = model.state_dict()\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"resampler\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "peekOfCode": "args = ap.parse_args()\n# find the model part that includes the the multimodal projector weights\nmodel = AutoModel.from_pretrained(args.model, trust_remote_code=True, local_files_only=True, torch_dtype=torch.bfloat16)\ncheckpoint = model.state_dict()\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"resampler\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\nif 'resampler.proj' in projector.keys() and hasattr(model.llm.config,'scale_emb') is True:\n    projector['resampler.proj'] = projector['resampler.proj'] / model.llm.config.scale_emb",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "peekOfCode": "model = AutoModel.from_pretrained(args.model, trust_remote_code=True, local_files_only=True, torch_dtype=torch.bfloat16)\ncheckpoint = model.state_dict()\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"resampler\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\nif 'resampler.proj' in projector.keys() and hasattr(model.llm.config,'scale_emb') is True:\n    projector['resampler.proj'] = projector['resampler.proj'] / model.llm.config.scale_emb\ntorch.save(projector, f\"{args.model}/minicpmv.projector\")\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vpm\")]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "peekOfCode": "checkpoint = model.state_dict()\n# get a list of mm tensor names\nmm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"resampler\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\nif 'resampler.proj' in projector.keys() and hasattr(model.llm.config,'scale_emb') is True:\n    projector['resampler.proj'] = projector['resampler.proj'] / model.llm.config.scale_emb\ntorch.save(projector, f\"{args.model}/minicpmv.projector\")\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vpm\")]\nif len(clip_tensors) > 0:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "documentation": {}
    },
    {
        "label": "mm_tensors",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "peekOfCode": "mm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"resampler\")]\n# store these tensors in a new dictionary and torch.save them\nprojector = {name: checkpoint[name].float() for name in mm_tensors}\nif 'resampler.proj' in projector.keys() and hasattr(model.llm.config,'scale_emb') is True:\n    projector['resampler.proj'] = projector['resampler.proj'] / model.llm.config.scale_emb\ntorch.save(projector, f\"{args.model}/minicpmv.projector\")\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vpm\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vpm.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n    torch.save(clip, f\"{args.model}/minicpmv.clip\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "documentation": {}
    },
    {
        "label": "projector",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "peekOfCode": "projector = {name: checkpoint[name].float() for name in mm_tensors}\nif 'resampler.proj' in projector.keys() and hasattr(model.llm.config,'scale_emb') is True:\n    projector['resampler.proj'] = projector['resampler.proj'] / model.llm.config.scale_emb\ntorch.save(projector, f\"{args.model}/minicpmv.projector\")\nclip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vpm\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vpm.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n    torch.save(clip, f\"{args.model}/minicpmv.clip\")\n    # added tokens should be removed to be able to convert Mistral models\n    if os.path.exists(f\"{args.model}/added_tokens.json\"):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "documentation": {}
    },
    {
        "label": "clip_tensors",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "peekOfCode": "clip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vpm\")]\nif len(clip_tensors) > 0:\n    clip = {name.replace(\"vpm.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n    torch.save(clip, f\"{args.model}/minicpmv.clip\")\n    # added tokens should be removed to be able to convert Mistral models\n    if os.path.exists(f\"{args.model}/added_tokens.json\"):\n        with open(f\"{args.model}/added_tokens.json\", \"w\") as f:\n            f.write(\"{}\\n\")\nconfig = model.llm.config\nconfig.auto_map = {",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "peekOfCode": "config = model.llm.config\nconfig.auto_map = {\n    \"AutoConfig\": \"configuration_minicpm.MiniCPMConfig\",\n    \"AutoModel\": \"modeling_minicpm.MiniCPMModel\",\n    \"AutoModelForCausalLM\": \"modeling_minicpm.MiniCPMForCausalLM\",\n    \"AutoModelForSeq2SeqLM\": \"modeling_minicpm.MiniCPMForCausalLM\",\n    \"AutoModelForSequenceClassification\": \"modeling_minicpm.MiniCPMForSequenceClassification\"\n}\nmodel.llm.save_pretrained(f\"{args.model}/model\")\ntok = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "documentation": {}
    },
    {
        "label": "config.auto_map",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "peekOfCode": "config.auto_map = {\n    \"AutoConfig\": \"configuration_minicpm.MiniCPMConfig\",\n    \"AutoModel\": \"modeling_minicpm.MiniCPMModel\",\n    \"AutoModelForCausalLM\": \"modeling_minicpm.MiniCPMForCausalLM\",\n    \"AutoModelForSeq2SeqLM\": \"modeling_minicpm.MiniCPMForCausalLM\",\n    \"AutoModelForSequenceClassification\": \"modeling_minicpm.MiniCPMForSequenceClassification\"\n}\nmodel.llm.save_pretrained(f\"{args.model}/model\")\ntok = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)\ntok.save_pretrained(f\"{args.model}/model\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "documentation": {}
    },
    {
        "label": "tok",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "peekOfCode": "tok = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)\ntok.save_pretrained(f\"{args.model}/model\")\nprint(\"Done!\")\nprint(f\"Now you can convert {args.model} to a regular LLaMA GGUF file.\")\nprint(f\"Also, use {args.model}/minicpmv.projector to prepare a minicpmv-encoder.gguf file.\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.mtmd.legacy-models.minicpmv-surgery",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "peekOfCode": "def main(args_in: list[str] | None = None) -> None:\n    parser = argparse.ArgumentParser(description=\"Start server benchmark scenario\")\n    parser.add_argument(\"--name\", type=str, help=\"Bench name\", required=True)\n    parser.add_argument(\"--runner-label\", type=str, help=\"Runner label\", required=True)\n    parser.add_argument(\"--branch\", type=str, help=\"Branch name\", default=\"detached\")\n    parser.add_argument(\"--commit\", type=str, help=\"Commit name\", default=\"dirty\")\n    parser.add_argument(\"--host\", type=str, help=\"Server listen host\", default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int, help=\"Server listen host\", default=\"8080\")\n    parser.add_argument(\"--model-path-prefix\", type=str, help=\"Prefix where to store the model files\", default=\"models\")\n    parser.add_argument(\"--n-prompts\", type=int,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "documentation": {}
    },
    {
        "label": "start_benchmark",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "peekOfCode": "def start_benchmark(args):\n    k6_path = './k6'\n    if 'BENCH_K6_BIN_PATH' in os.environ:\n        k6_path = os.environ['BENCH_K6_BIN_PATH']\n    k6_args = [\n        'run', args.scenario,\n        '--no-color',\n        '--no-connection-reuse',\n        '--no-vu-connection-reuse',\n    ]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "documentation": {}
    },
    {
        "label": "start_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "peekOfCode": "def start_server(args):\n    server_process = start_server_background(args)\n    attempts = 0\n    max_attempts = 600\n    if 'GITHUB_ACTIONS' in os.environ:\n        max_attempts *= 2\n    while not is_server_listening(args.host, args.port):\n        attempts += 1\n        if attempts > max_attempts:\n            assert False, \"server not started\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "documentation": {}
    },
    {
        "label": "start_server_background",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "peekOfCode": "def start_server_background(args):\n    # Start the server\n    server_path = '../../../build/bin/llama-server'\n    if 'LLAMA_SERVER_BIN_PATH' in os.environ:\n        server_path = os.environ['LLAMA_SERVER_BIN_PATH']\n    server_args = [\n        '--host', args.host,\n        '--port', args.port,\n    ]\n    server_args.extend(['--hf-repo', args.hf_repo])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "documentation": {}
    },
    {
        "label": "is_server_listening",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "peekOfCode": "def is_server_listening(server_fqdn, server_port):\n    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:\n        result = sock.connect_ex((server_fqdn, server_port))\n        _is_server_listening = result == 0\n        if _is_server_listening:\n            print(f\"server is listening on {server_fqdn}:{server_port}...\")\n        return _is_server_listening\ndef is_server_ready(server_fqdn, server_port):\n    url = f\"http://{server_fqdn}:{server_port}/health\"\n    response = requests.get(url)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "documentation": {}
    },
    {
        "label": "is_server_ready",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "peekOfCode": "def is_server_ready(server_fqdn, server_port):\n    url = f\"http://{server_fqdn}:{server_port}/health\"\n    response = requests.get(url)\n    return response.status_code == 200\ndef escape_metric_name(metric_name):\n    return re.sub('[^A-Z0-9]', '_', metric_name.upper())\nif __name__ == '__main__':\n    main()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "documentation": {}
    },
    {
        "label": "escape_metric_name",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "peekOfCode": "def escape_metric_name(metric_name):\n    return re.sub('[^A-Z0-9]', '_', metric_name.upper())\nif __name__ == '__main__':\n    main()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.bench.bench",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\ndef test_server_start_simple():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/health\")\n    assert res.status_code == 200\ndef test_server_props():\n    global server",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "documentation": {}
    },
    {
        "label": "test_server_start_simple",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "peekOfCode": "def test_server_start_simple():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/health\")\n    assert res.status_code == 200\ndef test_server_props():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/props\")\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "documentation": {}
    },
    {
        "label": "test_server_props",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "peekOfCode": "def test_server_props():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/props\")\n    assert res.status_code == 200\n    assert \".gguf\" in res.body[\"model_path\"]\n    assert res.body[\"total_slots\"] == server.n_slots\n    default_val = res.body[\"default_generation_settings\"]\n    assert server.n_ctx is not None and server.n_slots is not None\n    assert default_val[\"n_ctx\"] == server.n_ctx / server.n_slots",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "documentation": {}
    },
    {
        "label": "test_server_models",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "peekOfCode": "def test_server_models():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/models\")\n    assert res.status_code == 200\n    assert len(res.body[\"data\"]) == 1\n    assert res.body[\"data\"][0][\"id\"] == server.model_alias\ndef test_server_slots():\n    global server\n    # without slots endpoint enabled, this should return error",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "documentation": {}
    },
    {
        "label": "test_server_slots",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "peekOfCode": "def test_server_slots():\n    global server\n    # without slots endpoint enabled, this should return error\n    server.server_slots = False\n    server.start()\n    res = server.make_request(\"GET\", \"/slots\")\n    assert res.status_code == 501 # ERROR_TYPE_NOT_SUPPORTED\n    assert \"error\" in res.body\n    server.stop()\n    # with slots endpoint enabled, this should return slots info",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "documentation": {}
    },
    {
        "label": "test_load_split_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "peekOfCode": "def test_load_split_model():\n    global server\n    server.offline = False\n    server.model_hf_repo = \"ggml-org/models\"\n    server.model_hf_file = \"tinyllamas/split/stories15M-q8_0-00001-of-00003.gguf\"\n    server.model_alias = \"tinyllama-split\"\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"n_predict\": 16,\n        \"prompt\": \"Hello\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "documentation": {}
    },
    {
        "label": "test_no_webui",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "peekOfCode": "def test_no_webui():\n    global server\n    # default: webui enabled\n    server.start()\n    url = f\"http://{server.server_host}:{server.server_port}\"\n    res = requests.get(url)\n    assert res.status_code == 200\n    assert \"<!doctype html>\" in res.text\n    server.stop()\n    # with --no-webui",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "peekOfCode": "server = ServerPreset.tinyllama2()\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\ndef test_server_start_simple():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/health\")\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_basic",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n@pytest.mark.parametrize(\n    \"model,system_prompt,user_prompt,max_tokens,re_content,n_prompt,n_predicted,finish_reason,jinja,chat_template\",\n    [\n        (None, \"Book\", \"Hey\", 8, \"But she couldn't\", 69, 8, \"length\", False, None),\n        (None, \"Book\", \"Hey\", 8, \"But she couldn't\", 69, 8, \"length\", True, None),\n        (None, \"Book\", \"What is the best book\", 8, \"(Suddenly)+|\\\\{ \\\" Sarax.\", 77, 8, \"length\", False, None),\n        (None, \"Book\", \"What is the best book\", 8, \"(Suddenly)+|\\\\{ \\\" Sarax.\", 77, 8, \"length\", True,  None),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_chat_completion",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_chat_completion(model, system_prompt, user_prompt, max_tokens, re_content, n_prompt, n_predicted, finish_reason, jinja, chat_template):\n    global server\n    server.jinja = jinja\n    server.chat_template = chat_template\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"model\": model,\n        \"max_tokens\": max_tokens,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_chat_completion_stream",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_chat_completion_stream(system_prompt, user_prompt, max_tokens, re_content, n_prompt, n_predicted, finish_reason):\n    global server\n    server.model_alias = \"llama-test-model\"\n    server.start()\n    res = server.make_stream_request(\"POST\", \"/chat/completions\", data={\n        \"max_tokens\": max_tokens,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_chat_completion_with_openai_library",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_chat_completion_with_openai_library():\n    global server\n    server.start()\n    client = OpenAI(api_key=\"dummy\", base_url=f\"http://{server.server_host}:{server.server_port}/v1\")\n    res = client.chat.completions.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Book\"},\n            {\"role\": \"user\", \"content\": \"What is the best book\"},\n        ],",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_chat_template",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_chat_template():\n    global server\n    server.chat_template = \"llama3\"\n    server.debug = True  # to get the \"__verbose\" object in the response\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"max_tokens\": 8,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"Book\"},\n            {\"role\": \"user\", \"content\": \"What is the best book\"},",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_chat_template_assistant_prefill",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_chat_template_assistant_prefill(prefill, re_prefill):\n    global server\n    server.chat_template = \"llama3\"\n    server.debug = True  # to get the \"__verbose\" object in the response\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"max_tokens\": 8,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"Book\"},\n            {\"role\": \"user\", \"content\": \"What is the best book\"},",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_apply_chat_template",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_apply_chat_template():\n    global server\n    server.chat_template = \"command-r\"\n    server.start()\n    res = server.make_request(\"POST\", \"/apply-template\", data={\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a test.\"},\n            {\"role\": \"user\", \"content\":\"Hi there\"},\n        ]\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_with_response_format",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_completion_with_response_format(response_format: dict, n_predicted: int, re_content: str | None):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"max_tokens\": n_predicted,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a coding assistant.\"},\n            {\"role\": \"user\", \"content\": \"Write an example\"},\n        ],\n        \"response_format\": response_format,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_with_json_schema",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_completion_with_json_schema(jinja: bool, json_schema: dict, n_predicted: int, re_content: str):\n    global server\n    server.jinja = jinja\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"max_tokens\": n_predicted,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a coding assistant.\"},\n            {\"role\": \"user\", \"content\": \"Write an example\"},\n        ],",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_with_grammar",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_completion_with_grammar(jinja: bool, grammar: str, n_predicted: int, re_content: str):\n    global server\n    server.jinja = jinja\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"max_tokens\": n_predicted,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Does not matter what I say, does it?\"},\n        ],\n        \"grammar\": grammar,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_invalid_chat_completion_req",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_invalid_chat_completion_req(messages):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"messages\": messages,\n    })\n    assert res.status_code == 400 or res.status_code == 500\n    assert \"error\" in res.body\ndef test_chat_completion_with_timings_per_token():\n    global server",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_chat_completion_with_timings_per_token",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_chat_completion_with_timings_per_token():\n    global server\n    server.start()\n    res = server.make_stream_request(\"POST\", \"/chat/completions\", data={\n        \"max_tokens\": 10,\n        \"messages\": [{\"role\": \"user\", \"content\": \"test\"}],\n        \"stream\": True,\n        \"stream_options\": {\"include_usage\": True},\n        \"timings_per_token\": True,\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_logprobs",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_logprobs():\n    global server\n    server.start()\n    client = OpenAI(api_key=\"dummy\", base_url=f\"http://{server.server_host}:{server.server_port}/v1\")\n    res = client.chat.completions.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        temperature=0.0,\n        messages=[\n            {\"role\": \"system\", \"content\": \"Book\"},\n            {\"role\": \"user\", \"content\": \"What is the best book\"},",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_logprobs_stream",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_logprobs_stream():\n    global server\n    server.start()\n    client = OpenAI(api_key=\"dummy\", base_url=f\"http://{server.server_host}:{server.server_port}/v1\")\n    res = client.chat.completions.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        temperature=0.0,\n        messages=[\n            {\"role\": \"system\", \"content\": \"Book\"},\n            {\"role\": \"user\", \"content\": \"What is the best book\"},",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_logit_bias",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_logit_bias():\n    global server\n    server.start()\n    exclude = [\"i\", \"I\", \"the\", \"The\", \"to\", \"a\", \"an\", \"be\", \"is\", \"was\", \"but\", \"But\", \"and\", \"And\", \"so\", \"So\", \"you\", \"You\", \"he\", \"He\", \"she\", \"She\", \"we\", \"We\", \"they\", \"They\", \"it\", \"It\", \"his\", \"His\", \"her\", \"Her\", \"book\", \"Book\"]\n    res = server.make_request(\"POST\", \"/tokenize\", data={\n        \"content\": \" \" + \" \".join(exclude) + \" \",\n    })\n    assert res.status_code == 200\n    tokens = res.body[\"tokens\"]\n    logit_bias = {tok: -100 for tok in tokens}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_context_size_exceeded",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_context_size_exceeded():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"Book\"},\n            {\"role\": \"user\", \"content\": \"What is the best book\"},\n        ] * 100, # make the prompt too long\n    })\n    assert res.status_code == 400",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_context_size_exceeded_stream",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_context_size_exceeded_stream():\n    global server\n    server.start()\n    try:\n        for _ in server.make_stream_request(\"POST\", \"/chat/completions\", data={\n            \"messages\": [\n                {\"role\": \"system\", \"content\": \"Book\"},\n                {\"role\": \"user\", \"content\": \"What is the best book\"},\n            ] * 100, # make the prompt too long\n            \"stream\": True}):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_return_progress",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_return_progress(n_batch, batch_count, reuse_cache):\n    global server\n    server.n_batch = n_batch\n    server.n_ctx = 256\n    server.n_slots = 1\n    server.start()\n    def make_cmpl_request():\n        return server.make_stream_request(\"POST\", \"/chat/completions\", data={\n            \"max_tokens\": 10,\n            \"messages\": [",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "test_chat_completions_multiple_choices",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "peekOfCode": "def test_chat_completions_multiple_choices():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"max_tokens\": 8,\n        \"n\": 2,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"Book\"},\n            {\"role\": \"user\", \"content\": \"What is the best book\"},\n        ],",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_chat_completion",
        "documentation": {}
    },
    {
        "label": "get_test_image_base64",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def get_test_image_base64() -> str:\n    \"\"\"Get a test image in base64 format\"\"\"\n    # Use the same test image as test_vision_api.py\n    IMG_URL = \"https://huggingface.co/ggml-org/tinygemma3-GGUF/resolve/main/test/11_truck.png\"\n    response = requests.get(IMG_URL)\n    response.raise_for_status()\n    return base64.b64encode(response.content).decode(\"utf-8\")\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.model_alias = \"tinyllama-2-anthropic\"\n    server.server_port = 8082\n    server.n_slots = 1\n    server.n_ctx = 8192\n    server.n_batch = 2048\n@pytest.fixture\ndef vision_server():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "vision_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def vision_server():\n    \"\"\"Separate fixture for vision tests that require multimodal support\"\"\"\n    global server\n    server = ServerPreset.tinygemma3()\n    server.offline = False  # Allow downloading the model\n    server.model_alias = \"tinygemma3-anthropic\"\n    server.server_port = 8083  # Different port to avoid conflicts\n    server.n_slots = 1\n    return server\n# Basic message tests",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_messages_basic",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_messages_basic():\n    \"\"\"Test basic Anthropic messages endpoint\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Say hello\"}\n        ]\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_messages_with_system",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_messages_with_system():\n    \"\"\"Test messages with system prompt\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50,\n        \"system\": \"You are a helpful assistant.\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello\"}\n        ]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_messages_multipart_content",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_messages_multipart_content():\n    \"\"\"Test messages with multipart content blocks\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50,\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_messages_conversation",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_messages_conversation():\n    \"\"\"Test multi-turn conversation\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n            {\"role\": \"user\", \"content\": \"How are you?\"}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_messages_streaming",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_messages_streaming():\n    \"\"\"Test streaming messages\"\"\"\n    server.start()\n    res = server.make_stream_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 30,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Say hello\"}\n        ],\n        \"stream\": True",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_count_tokens",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_count_tokens():\n    \"\"\"Test token counting endpoint\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages/count_tokens\", data={\n        \"model\": \"test\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello world\"}\n        ]\n    })\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_count_tokens_with_system",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_count_tokens_with_system():\n    \"\"\"Test token counting with system prompt\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages/count_tokens\", data={\n        \"model\": \"test\",\n        \"system\": \"You are a helpful assistant.\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello\"}\n        ]\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_count_tokens_no_max_tokens",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_count_tokens_no_max_tokens():\n    \"\"\"Test that count_tokens doesn't require max_tokens\"\"\"\n    server.start()\n    # max_tokens is NOT required for count_tokens\n    res = server.make_request(\"POST\", \"/v1/messages/count_tokens\", data={\n        \"model\": \"test\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello\"}\n        ]\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_tool_use_basic",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_tool_use_basic():\n    \"\"\"Test basic tool use\"\"\"\n    server.jinja = True\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 200,\n        \"tools\": [{\n            \"name\": \"get_weather\",\n            \"description\": \"Get the current weather in a location\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_tool_result",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_tool_result():\n    \"\"\"Test sending tool results back\n    This test verifies that tool_result blocks are properly converted to\n    role=\"tool\" messages internally. Without proper conversion, this would\n    fail with a 500 error: \"unsupported content[].type\" because tool_result\n    blocks would remain in the user message content array.\n    \"\"\"\n    server.jinja = True\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_tool_result_with_text",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_tool_result_with_text():\n    \"\"\"Test tool result mixed with text content\n    This tests the edge case where a user message contains both text and\n    tool_result blocks. The server must properly split these into separate\n    messages: a user message with text, followed by tool messages.\n    Without proper handling, this would fail with 500: \"unsupported content[].type\"\n    \"\"\"\n    server.jinja = True\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_tool_result_error",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_tool_result_error():\n    \"\"\"Test tool result with error flag\"\"\"\n    server.jinja = True\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 100,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Get the weather\"},\n            {",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_tool_streaming",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_tool_streaming():\n    \"\"\"Test streaming with tool use\"\"\"\n    server.jinja = True\n    server.start()\n    res = server.make_stream_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 200,\n        \"stream\": True,\n        \"tools\": [{\n            \"name\": \"calculator\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_vision_format_accepted",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_vision_format_accepted():\n    \"\"\"Test that Anthropic vision format is accepted (format validation only)\"\"\"\n    server.start()\n    # Small 1x1 red PNG image in base64\n    red_pixel_png = \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8z8DwHwAFBQIAX8jx0gAAAABJRU5ErkJggg==\"\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 10,\n        \"messages\": [\n            {",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_vision_base64_with_multimodal_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_vision_base64_with_multimodal_model(vision_server):\n    \"\"\"Test vision with base64 image using Anthropic format with multimodal model\"\"\"\n    global server\n    server = vision_server\n    server.start()\n    # Get test image in base64 format\n    image_base64 = get_test_image_base64()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 10,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_stop_sequences",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_stop_sequences():\n    \"\"\"Test stop_sequences parameter\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 100,\n        \"stop_sequences\": [\"\\n\", \"END\"],\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Count to 10\"}\n        ]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_temperature",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_temperature():\n    \"\"\"Test temperature parameter\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50,\n        \"temperature\": 0.5,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello\"}\n        ]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_top_p",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_top_p():\n    \"\"\"Test top_p parameter\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50,\n        \"top_p\": 0.9,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello\"}\n        ]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_top_k",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_top_k():\n    \"\"\"Test top_k parameter (llama.cpp specific)\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50,\n        \"top_k\": 40,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello\"}\n        ]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_missing_messages",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_missing_messages():\n    \"\"\"Test error when messages are missing\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50\n        # missing \"messages\" field\n    })\n    # Should return an error (400 or 500)\n    assert res.status_code >= 400",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_empty_messages",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_empty_messages():\n    \"\"\"Test permissive handling of empty messages array\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50,\n        \"messages\": []\n    })\n    # Server is permissive and accepts empty messages (provides defaults)\n    # This matches the permissive validation design choice",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_streaming_content_block_indices",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_streaming_content_block_indices():\n    \"\"\"Test that content block indices are correct in streaming\"\"\"\n    server.jinja = True\n    server.start()\n    # Request that might produce both text and tool use\n    res = server.make_stream_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 400,\n        \"stream\": True,\n        \"tools\": [{",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_thinking",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_thinking():\n    \"\"\"Test extended thinking parameter\"\"\"\n    server.jinja = True\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 100,\n        \"thinking\": {\n            \"type\": \"enabled\",\n            \"budget_tokens\": 50",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_metadata",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_metadata():\n    \"\"\"Test metadata parameter\"\"\"\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/messages\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50,\n        \"metadata\": {\n            \"user_id\": \"test_user_123\"\n        },\n        \"messages\": [",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_vs_openai_different_response_format",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_vs_openai_different_response_format():\n    \"\"\"Verify Anthropic format is different from OpenAI format\"\"\"\n    server.start()\n    # Make OpenAI request\n    openai_res = server.make_request(\"POST\", \"/v1/chat/completions\", data={\n        \"model\": \"test\",\n        \"max_tokens\": 50,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello\"}\n        ]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "test_anthropic_thinking_with_reasoning_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "peekOfCode": "def test_anthropic_thinking_with_reasoning_model(stream):\n    \"\"\"Test that thinking content blocks are properly returned for reasoning models\"\"\"\n    global server\n    server = ServerProcess()\n    server.model_hf_repo = \"bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF\"\n    server.model_hf_file = \"DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf\"\n    server.reasoning_format = \"deepseek\"\n    server.jinja = True\n    server.n_ctx = 8192\n    server.n_predict = 1024",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_compat_anthropic",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n@pytest.mark.parametrize(\"prompt,n_predict,re_content,n_prompt,n_predicted,truncated,return_tokens\", [\n    (\"I believe the meaning of life is\", 8, \"(going|bed)+\", 18, 8, False, False),\n    (\"Write a joke about AI from a very long prompt which will not be truncated\", 64, \"(princesses|everyone|kids|Anna|forest)+\", 46, 64, False, True),\n])\ndef test_completion(prompt: str, n_predict: int, re_content: str, n_prompt: int, n_predicted: int, truncated: bool, return_tokens: bool):\n    global server\n    server.start()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion(prompt: str, n_predict: int, re_content: str, n_prompt: int, n_predicted: int, truncated: bool, return_tokens: bool):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"n_predict\": n_predict,\n        \"prompt\": prompt,\n        \"return_tokens\": return_tokens,\n    })\n    assert res.status_code == 200\n    assert res.body[\"timings\"][\"prompt_n\"] == n_prompt",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_stream",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion_stream(prompt: str, n_predict: int, re_content: str, n_prompt: int, n_predicted: int, truncated: bool):\n    global server\n    server.start()\n    res = server.make_stream_request(\"POST\", \"/completion\", data={\n        \"n_predict\": n_predict,\n        \"prompt\": prompt,\n        \"stream\": True,\n    })\n    content = \"\"\n    for data in res:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_stream_vs_non_stream",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion_stream_vs_non_stream():\n    global server\n    server.start()\n    res_stream = server.make_stream_request(\"POST\", \"/completion\", data={\n        \"n_predict\": 8,\n        \"prompt\": \"I believe the meaning of life is\",\n        \"stream\": True,\n    })\n    res_non_stream = server.make_request(\"POST\", \"/completion\", data={\n        \"n_predict\": 8,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_with_openai_library",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion_with_openai_library():\n    global server\n    server.start()\n    client = OpenAI(api_key=\"dummy\", base_url=f\"http://{server.server_host}:{server.server_port}/v1\")\n    res = client.completions.create(\n        model=\"davinci-002\",\n        prompt=\"I believe the meaning of life is\",\n        max_tokens=8,\n    )\n    assert res.system_fingerprint is not None and res.system_fingerprint.startswith(\"b\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_stream_with_openai_library",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion_stream_with_openai_library():\n    global server\n    server.start()\n    client = OpenAI(api_key=\"dummy\", base_url=f\"http://{server.server_host}:{server.server_port}/v1\")\n    res = client.completions.create(\n        model=\"davinci-002\",\n        prompt=\"I believe the meaning of life is\",\n        max_tokens=8,\n        stream=True,\n    )",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_stream_with_openai_library_stops",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion_stream_with_openai_library_stops():\n    global server\n    server.model_hf_repo = \"bartowski/Phi-3.5-mini-instruct-GGUF:Q4_K_M\"\n    server.model_hf_file = None\n    server.start()\n    client = OpenAI(api_key=\"dummy\", base_url=f\"http://{server.server_host}:{server.server_port}/v1\")\n    res = client.completions.create(\n        model=\"davinci-002\",\n        prompt=\"System: You are helpfull assistant.\\nAssistant:\\nHey! How could I help?\\nUser:\\nTell me a joke.\\nAssistant:\\n\",\n        stop=[\"User:\\n\", \"Assistant:\\n\"],",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_consistent_result_same_seed",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_consistent_result_same_seed(n_slots: int):\n    global server\n    server.n_slots = n_slots\n    server.start()\n    last_res = None\n    for _ in range(4):\n        res = server.make_request(\"POST\", \"/completion\", data={\n            \"prompt\": \"I believe the meaning of life is\",\n            \"seed\": 42,\n            \"temperature\": 0.0,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_different_result_different_seed",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_different_result_different_seed(n_slots: int):\n    global server\n    server.n_slots = n_slots\n    server.start()\n    last_res = None\n    for seed in range(4):\n        res = server.make_request(\"POST\", \"/completion\", data={\n            \"prompt\": \"I believe the meaning of life is\",\n            \"seed\": seed,\n            \"temperature\": 1.0,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_consistent_result_different_batch_size",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_consistent_result_different_batch_size(n_batch: int, temperature: float):\n    global server\n    server.n_batch = n_batch\n    server.start()\n    last_res = None\n    for _ in range(4):\n        res = server.make_request(\"POST\", \"/completion\", data={\n            \"prompt\": \"I believe the meaning of life is\",\n            \"seed\": 42,\n            \"temperature\": temperature,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_cache_vs_nocache_prompt",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_cache_vs_nocache_prompt():\n    global server\n    server.start()\n    res_cache = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"I believe the meaning of life is\",\n        \"seed\": 42,\n        \"temperature\": 1.0,\n        \"cache_prompt\": True,\n    })\n    res_no_cache = server.make_request(\"POST\", \"/completion\", data={",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_nocache_long_input_prompt",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_nocache_long_input_prompt():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"I believe the meaning of life is\"*32,\n        \"seed\": 42,\n        \"temperature\": 1.0,\n        \"cache_prompt\": False,\n    })\n    assert res.status_code == 400",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_json_prompt_no_mtmd",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_json_prompt_no_mtmd():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": { JSON_PROMPT_STRING_KEY: \"I believe the meaning of life is\" },\n        \"seed\": 42,\n        \"temperature\": 1.0,\n        \"cache_prompt\": False,\n    })\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_json_prompt_mtm_error_when_not_supported",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_json_prompt_mtm_error_when_not_supported():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": { JSON_PROMPT_STRING_KEY: \"I believe the meaning of life is <__media__>\", JSON_MULTIMODAL_KEY: \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=\" },\n        \"seed\": 42,\n        \"temperature\": 1.0,\n        \"cache_prompt\": False,\n    })\n    # MTMD is disabled on this model, so this should fail.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_with_tokens_input",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion_with_tokens_input():\n    global server\n    server.temperature = 0.0\n    server.start()\n    prompt_str = \"I believe the meaning of life is\"\n    res = server.make_request(\"POST\", \"/tokenize\", data={\n        \"content\": prompt_str,\n        \"add_special\": True,\n    })\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_parallel_slots",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion_parallel_slots(n_slots: int, n_requests: int):\n    global server\n    server.n_slots = n_slots\n    server.temperature = 0.0\n    server.start()\n    PROMPTS = [\n        (\"Write a very long book.\", \"(very|special|big)+\"),\n        (\"Write another a poem.\", \"(small|house)+\"),\n        (\"What is LLM?\", \"(Dad|said)+\"),\n        (\"The sky is blue and I love it.\", \"(climb|leaf)+\"),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_unified",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion_unified(n_ctx, n_slots, n_predict_vals, expected_success):\n    global server\n    server.n_slots = n_slots\n    server.kv_unified = True\n    server.n_ctx = n_ctx\n    server.start()\n    prompt = \"A\"\n    tasks = []\n    for n_predict in n_predict_vals:\n        tasks.append((server.make_request, (\"POST\", \"/completion\", {\"prompt\": prompt, \"n_predict\": n_predict})))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_response_fields",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion_response_fields(\n    prompt: str, n_predict: int, response_fields: list[str]\n):\n    global server\n    server.start()\n    res = server.make_request(\n        \"POST\",\n        \"/completion\",\n        data={\n            \"n_predict\": n_predict,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_n_probs",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_n_probs():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"I believe the meaning of life is\",\n        \"n_probs\": 10,\n        \"temperature\": 0.0,\n        \"n_predict\": 5,\n    })\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_n_probs_stream",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_n_probs_stream():\n    global server\n    server.start()\n    res = server.make_stream_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"I believe the meaning of life is\",\n        \"n_probs\": 10,\n        \"temperature\": 0.0,\n        \"n_predict\": 5,\n        \"stream\": True,\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_n_probs_post_sampling",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_n_probs_post_sampling():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"I believe the meaning of life is\",\n        \"n_probs\": 10,\n        \"temperature\": 0.0,\n        \"n_predict\": 5,\n        \"post_sampling_probs\": True,\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_logit_bias",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_logit_bias(tokenize, openai_style):\n    global server\n    server.start()\n    exclude = [\"i\", \"I\", \"the\", \"The\", \"to\", \"a\", \"an\", \"be\", \"is\", \"was\", \"but\", \"But\", \"and\", \"And\", \"so\", \"So\", \"you\", \"You\", \"he\", \"He\", \"she\", \"She\", \"we\", \"We\", \"they\", \"They\", \"it\", \"It\", \"his\", \"His\", \"her\", \"Her\", \"book\", \"Book\"]\n    logit_bias = []\n    if tokenize:\n        res = server.make_request(\"POST\", \"/tokenize\", data={\n            \"content\": \" \" + \" \".join(exclude) + \" \",\n        })\n        assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_cancel_request",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_cancel_request():\n    global server\n    server.n_ctx = 4096\n    server.n_predict = -1\n    server.n_slots = 1\n    server.server_slots = True\n    server.start()\n    # send a request that will take a long time, but cancel it before it finishes\n    try:\n        server.make_request(\"POST\", \"/completion\", data={",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "test_completion_prompt_cache",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "def test_completion_prompt_cache():\n    global server\n    server.n_slots = 2\n    server.kv_unified = True\n    server.start()\n    for _ in range(16):\n        # generate alternating random prompts with variable lengths in order to get them in and out of the cache\n        r = random.randint(0, 4)\n        prompt = (\" Hello \" +  str(r)) * (40 + r)\n        n_prompt = (40 + r)*5 + 2",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "server = ServerPreset.tinyllama2()\nJSON_MULTIMODAL_KEY = \"multimodal_data\"\nJSON_PROMPT_STRING_KEY = \"prompt_string\"\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n@pytest.mark.parametrize(\"prompt,n_predict,re_content,n_prompt,n_predicted,truncated,return_tokens\", [\n    (\"I believe the meaning of life is\", 8, \"(going|bed)+\", 18, 8, False, False),\n    (\"Write a joke about AI from a very long prompt which will not be truncated\", 64, \"(princesses|everyone|kids|Anna|forest)+\", 46, 64, False, True),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "JSON_MULTIMODAL_KEY",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "JSON_MULTIMODAL_KEY = \"multimodal_data\"\nJSON_PROMPT_STRING_KEY = \"prompt_string\"\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n@pytest.mark.parametrize(\"prompt,n_predict,re_content,n_prompt,n_predicted,truncated,return_tokens\", [\n    (\"I believe the meaning of life is\", 8, \"(going|bed)+\", 18, 8, False, False),\n    (\"Write a joke about AI from a very long prompt which will not be truncated\", 64, \"(princesses|everyone|kids|Anna|forest)+\", 46, 64, False, True),\n])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "JSON_PROMPT_STRING_KEY",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "peekOfCode": "JSON_PROMPT_STRING_KEY = \"prompt_string\"\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n@pytest.mark.parametrize(\"prompt,n_predict,re_content,n_prompt,n_predicted,truncated,return_tokens\", [\n    (\"I believe the meaning of life is\", 8, \"(going|bed)+\", 18, 8, False, False),\n    (\"Write a joke about AI from a very long prompt which will not be truncated\", 64, \"(princesses|everyone|kids|Anna|forest)+\", 46, 64, False, True),\n])\ndef test_completion(prompt: str, n_predict: int, re_content: str, n_prompt: int, n_predicted: int, truncated: bool, return_tokens: bool):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_completion",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.n_ctx = 512\n    server.n_slots = 2\n    server.n_predict = 128\ndef test_ctx_shift_enabled():\n    # the prompt is 226 tokens\n    # the slot context is 512/2 = 256 tokens\n    # 96 tokens are generated thanks to shifting the context when it gets full",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "documentation": {}
    },
    {
        "label": "test_ctx_shift_enabled",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "peekOfCode": "def test_ctx_shift_enabled():\n    # the prompt is 226 tokens\n    # the slot context is 512/2 = 256 tokens\n    # 96 tokens are generated thanks to shifting the context when it gets full\n    global server\n    server.enable_ctx_shift = True\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"n_predict\": 96,\n        \"prompt\": SHORT_TEXT,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "documentation": {}
    },
    {
        "label": "test_ctx_shift_disabled_short_prompt",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "peekOfCode": "def test_ctx_shift_disabled_short_prompt(n_predict: int, n_token_output: int, truncated: bool):\n    global server\n    server.n_predict = -1\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"n_predict\": n_predict,\n        \"prompt\": \"Hi how are you\",\n    })\n    assert res.status_code == 200\n    assert res.body[\"timings\"][\"predicted_n\"] == n_token_output",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "documentation": {}
    },
    {
        "label": "test_ctx_shift_disabled_long_prompt",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "peekOfCode": "def test_ctx_shift_disabled_long_prompt():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"n_predict\": 64,\n        \"prompt\": LONG_TEXT,\n    })\n    assert res.status_code != 200\n    assert \"error\" in res.body\n    assert \"exceeds the available context size\" in res.body[\"error\"][\"message\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "documentation": {}
    },
    {
        "label": "test_ctx_shift_disabled_stream",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "peekOfCode": "def test_ctx_shift_disabled_stream():\n    global server\n    server.start()\n    res = server.make_stream_request(\"POST\", \"/v1/completions\", data={\n        \"n_predict\": 256,\n        \"prompt\": \"Once\",\n        \"stream\": True,\n    })\n    content = \"\"\n    for data in res:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "peekOfCode": "server = ServerPreset.tinyllama2()\nSHORT_TEXT = \"\"\"\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nUt enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n\"\"\".strip()\nLONG_TEXT = \"\"\"\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nUt enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "documentation": {}
    },
    {
        "label": "SHORT_TEXT",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "peekOfCode": "SHORT_TEXT = \"\"\"\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nUt enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n\"\"\".strip()\nLONG_TEXT = \"\"\"\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nUt enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\nExcepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "documentation": {}
    },
    {
        "label": "LONG_TEXT",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "peekOfCode": "LONG_TEXT = \"\"\"\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nUt enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\nExcepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\"\"\".strip()\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_ctx_shift",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.bert_bge_small()\ndef test_embedding_single():\n    global server\n    server.pooling = 'last'\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\n        \"input\": \"I believe the meaning of life is\",\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_single",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_single():\n    global server\n    server.pooling = 'last'\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\n        \"input\": \"I believe the meaning of life is\",\n    })\n    assert res.status_code == 200\n    assert len(res.body['data']) == 1\n    assert 'embedding' in res.body['data'][0]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_multiple",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_multiple():\n    global server\n    server.pooling = 'last'\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\n        \"input\": [\n            \"I believe the meaning of life is\",\n            \"Write a joke about AI from a very long prompt which will not be truncated\",\n            \"This is a test\",\n            \"This is another test\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_multiple_with_fa",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_multiple_with_fa():\n    server = ServerPreset.bert_bge_small_with_fa()\n    server.pooling = 'last'\n    server.start()\n    # one of these should trigger the FA branch (i.e. context size % 256 == 0)\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\n        \"input\": [\n            \"a \"*253,\n            \"b \"*254,\n            \"c \"*255,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_mixed_input",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_mixed_input(input, is_multi_prompt: bool):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\"input\": input})\n    assert res.status_code == 200\n    data = res.body['data']\n    if is_multi_prompt:\n        assert len(data) == len(input)\n        for d in data:\n            assert 'embedding' in d",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_pooling_none",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_pooling_none():\n    global server\n    server.pooling = 'none'\n    server.start()\n    res = server.make_request(\"POST\", \"/embeddings\", data={\n        \"input\": \"hello hello hello\",\n    })\n    assert res.status_code == 200\n    assert 'embedding' in res.body[0]\n    assert len(res.body[0]['embedding']) == 5 # 3 text tokens + 2 special",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_pooling_none_oai",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_pooling_none_oai():\n    global server\n    server.pooling = 'none'\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\n        \"input\": \"hello hello hello\",\n    })\n    # /v1/embeddings does not support pooling type 'none'\n    assert res.status_code == 400\n    assert \"error\" in res.body",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_openai_library_single",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_openai_library_single():\n    global server\n    server.pooling = 'last'\n    server.start()\n    client = OpenAI(api_key=\"dummy\", base_url=f\"http://{server.server_host}:{server.server_port}/v1\")\n    res = client.embeddings.create(model=\"text-embedding-3-small\", input=\"I believe the meaning of life is\")\n    assert len(res.data) == 1\n    assert len(res.data[0].embedding) > 1\ndef test_embedding_openai_library_multiple():\n    global server",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_openai_library_multiple",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_openai_library_multiple():\n    global server\n    server.pooling = 'last'\n    server.start()\n    client = OpenAI(api_key=\"dummy\", base_url=f\"http://{server.server_host}:{server.server_port}/v1\")\n    res = client.embeddings.create(model=\"text-embedding-3-small\", input=[\n        \"I believe the meaning of life is\",\n        \"Write a joke about AI from a very long prompt which will not be truncated\",\n        \"This is a test\",\n        \"This is another test\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_error_prompt_too_long",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_error_prompt_too_long():\n    global server\n    server.pooling = 'last'\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\n        \"input\": \"This is a test \" * 512,\n    })\n    assert res.status_code != 200\n    assert \"too large\" in res.body[\"error\"][\"message\"]\ndef test_same_prompt_give_same_result():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_same_prompt_give_same_result",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_same_prompt_give_same_result():\n    server.pooling = 'last'\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\n        \"input\": [\n            \"I believe the meaning of life is\",\n            \"I believe the meaning of life is\",\n            \"I believe the meaning of life is\",\n            \"I believe the meaning of life is\",\n            \"I believe the meaning of life is\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_usage_single",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_usage_single(content, n_tokens):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\"input\": content})\n    assert res.status_code == 200\n    assert res.body['usage']['prompt_tokens'] == res.body['usage']['total_tokens']\n    assert res.body['usage']['prompt_tokens'] == n_tokens\ndef test_embedding_usage_multiple():\n    global server\n    server.start()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_usage_multiple",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_usage_multiple():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\n        \"input\": [\n            \"I believe the meaning of life is\",\n            \"I believe the meaning of life is\",\n        ],\n    })\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "test_embedding_openai_library_base64",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "def test_embedding_openai_library_base64():\n    server.start()\n    test_input = \"Test base64 embedding output\"\n    # get embedding in default format\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={\n        \"input\": test_input\n    })\n    assert res.status_code == 200\n    vec0 = res.body[\"data\"][0][\"embedding\"]\n    # get embedding in base64 format",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "server = ServerPreset.bert_bge_small()\nEPSILON = 1e-3\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.bert_bge_small()\ndef test_embedding_single():\n    global server\n    server.pooling = 'last'\n    server.start()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "EPSILON",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "peekOfCode": "EPSILON = 1e-3\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.bert_bge_small()\ndef test_embedding_single():\n    global server\n    server.pooling = 'last'\n    server.start()\n    res = server.make_request(\"POST\", \"/v1/embeddings\", data={",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_embedding",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama_infill()\ndef test_infill_without_input_extra():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/infill\", data={\n        \"input_prefix\": \"#include <cstdio>\\n#include \\\"llama.h\\\"\\n\\nint main() {\\n\",\n        \"prompt\": \"    int n_threads = llama_\",\n        \"input_suffix\": \"}\\n\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "documentation": {}
    },
    {
        "label": "test_infill_without_input_extra",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "peekOfCode": "def test_infill_without_input_extra():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/infill\", data={\n        \"input_prefix\": \"#include <cstdio>\\n#include \\\"llama.h\\\"\\n\\nint main() {\\n\",\n        \"prompt\": \"    int n_threads = llama_\",\n        \"input_suffix\": \"}\\n\",\n    })\n    assert res.status_code == 200\n    assert match_regex(\"(Ann|small|shiny|Daddy|Jimmy)+\", res.body[\"content\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "documentation": {}
    },
    {
        "label": "test_infill_with_input_extra",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "peekOfCode": "def test_infill_with_input_extra():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/infill\", data={\n        \"input_extra\": [{\n            \"filename\": \"llama.h\",\n            \"text\": \"LLAMA_API int32_t llama_n_threads();\\n\"\n        }],\n        \"input_prefix\": \"#include <cstdio>\\n#include \\\"llama.h\\\"\\n\\nint main() {\\n\",\n        \"prompt\": \"    int n_threads = llama_\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "documentation": {}
    },
    {
        "label": "test_invalid_input_extra_req",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "peekOfCode": "def test_invalid_input_extra_req(input_extra):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/infill\", data={\n        \"input_extra\": [input_extra],\n        \"input_prefix\": \"#include <cstdio>\\n#include \\\"llama.h\\\"\\n\\nint main() {\\n\",\n        \"prompt\": \"    int n_threads = llama_\",\n        \"input_suffix\": \"}\\n\",\n    })\n    assert res.status_code == 400",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "documentation": {}
    },
    {
        "label": "test_with_qwen_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "peekOfCode": "def test_with_qwen_model():\n    global server\n    server.model_file = None\n    server.model_hf_repo = \"ggml-org/Qwen2.5-Coder-1.5B-IQ3_XXS-GGUF\"\n    server.model_hf_file = \"qwen2.5-coder-1.5b-iq3_xxs-imat.gguf\"\n    server.start(timeout_seconds=600)\n    res = server.make_request(\"POST\", \"/infill\", data={\n        \"input_extra\": [{\n            \"filename\": \"llama.h\",\n            \"text\": \"LLAMA_API int32_t llama_n_threads();\\n\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "peekOfCode": "server = ServerPreset.tinyllama_infill()\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama_infill()\ndef test_infill_without_input_extra():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/infill\", data={\n        \"input_prefix\": \"#include <cstdio>\\n#include \\\"llama.h\\\"\\n\\nint main() {\\n\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_infill",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.stories15m_moe()\n    server.lora_files = [download_file(LORA_FILE_URL)]\n@pytest.mark.parametrize(\"scale,re_content\", [\n    # without applying lora, the model should behave like a bedtime story generator\n    (0.0, \"(little|girl|three|years|old)+\"),\n    # with lora, the model should behave like a Shakespearean text generator\n    (1.0, \"(eye|love|glass|sun)+\"),\n])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "test_lora",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "peekOfCode": "def test_lora(scale: float, re_content: str):\n    global server\n    server.start()\n    res_lora_control = server.make_request(\"POST\", \"/lora-adapters\", data=[\n        {\"id\": 0, \"scale\": scale}\n    ])\n    assert res_lora_control.status_code == 200\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"Look in thy glass\",\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "test_lora_per_request",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "peekOfCode": "def test_lora_per_request():\n    global server\n    server.n_slots = 4\n    server.start()\n    # running the same prompt with different lora scales, all in parallel\n    # each prompt will be processed by a different slot\n    prompt = \"Look in thy glass\"\n    lora_config = [\n        ( [{\"id\": 0, \"scale\": 0.0}], \"(bright|day|many|happy)+\" ),\n        ( [{\"id\": 0, \"scale\": 0.0}], \"(bright|day|many|happy)+\" ),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "test_with_big_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "peekOfCode": "def test_with_big_model():\n    server = ServerProcess()\n    server.model_hf_repo = \"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"\n    server.model_hf_file = \"Meta-Llama-3.1-8B-Instruct-IQ2_M.gguf\"\n    server.model_alias = \"Llama-3.2-8B-Instruct\"\n    server.n_slots = 4\n    server.n_ctx = server.n_slots * 1024\n    server.n_predict = 64\n    server.temperature = 0.0\n    server.seed = 42",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "peekOfCode": "server = ServerPreset.stories15m_moe()\nLORA_FILE_URL = \"https://huggingface.co/ggml-org/stories15M_MOE/resolve/main/moe_shakespeare15M.gguf\"\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.stories15m_moe()\n    server.lora_files = [download_file(LORA_FILE_URL)]\n@pytest.mark.parametrize(\"scale,re_content\", [\n    # without applying lora, the model should behave like a bedtime story generator\n    (0.0, \"(little|girl|three|years|old)+\"),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "LORA_FILE_URL",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "peekOfCode": "LORA_FILE_URL = \"https://huggingface.co/ggml-org/stories15M_MOE/resolve/main/moe_shakespeare15M.gguf\"\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.stories15m_moe()\n    server.lora_files = [download_file(LORA_FILE_URL)]\n@pytest.mark.parametrize(\"scale,re_content\", [\n    # without applying lora, the model should behave like a bedtime story generator\n    (0.0, \"(little|girl|three|years|old)+\"),\n    # with lora, the model should behave like a Shakespearean text generator",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.jina_reranker_tiny()\nTEST_DOCUMENTS = [\n    \"A machine is a physical system that uses power to apply forces and control movement to perform an action. The term is commonly applied to artificial devices, such as those employing engines or motors, but also to natural biological macromolecules, such as molecular machines.\",\n    \"Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, non-human animals, and some machines; there is also evidence for some kind of learning in certain plants.\",\n    \"Machine learning is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.\",\n    \"Paris, capitale de la France, est une grande ville europenne et un centre mondial de l'art, de la mode, de la gastronomie et de la culture. Son paysage urbain du XIXe sicle est travers par de larges boulevards et la Seine.\"\n]\ndef test_rerank():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "documentation": {}
    },
    {
        "label": "test_rerank",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "peekOfCode": "def test_rerank():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/rerank\", data={\n        \"query\": \"Machine learning is\",\n        \"documents\": TEST_DOCUMENTS,\n    })\n    assert res.status_code == 200\n    assert len(res.body[\"results\"]) == 4\n    most_relevant = res.body[\"results\"][0]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "documentation": {}
    },
    {
        "label": "test_rerank_tei_format",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "peekOfCode": "def test_rerank_tei_format():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/rerank\", data={\n        \"query\": \"Machine learning is\",\n        \"texts\": TEST_DOCUMENTS,\n    })\n    assert res.status_code == 200\n    assert len(res.body) == 4\n    most_relevant = res.body[0]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "documentation": {}
    },
    {
        "label": "test_invalid_rerank_req",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "peekOfCode": "def test_invalid_rerank_req(documents):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/rerank\", data={\n        \"query\": \"Machine learning is\",\n        \"documents\": documents,\n    })\n    assert res.status_code == 400\n    assert \"error\" in res.body\n@pytest.mark.parametrize(",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "documentation": {}
    },
    {
        "label": "test_rerank_usage",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "peekOfCode": "def test_rerank_usage(query, doc1, doc2, n_tokens):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/rerank\", data={\n        \"query\": query,\n        \"documents\": [\n            doc1,\n            doc2,\n        ]\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "documentation": {}
    },
    {
        "label": "test_rerank_top_n",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "peekOfCode": "def test_rerank_top_n(top_n, expected_len):\n    global server\n    server.start()\n    data = {\n        \"query\": \"Machine learning is\",\n        \"documents\": TEST_DOCUMENTS,\n    }\n    if top_n is not None:\n        data[\"top_n\"] = top_n\n    res = server.make_request(\"POST\", \"/rerank\", data=data)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "documentation": {}
    },
    {
        "label": "test_rerank_tei_top_n",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "peekOfCode": "def test_rerank_tei_top_n(top_n, expected_len):\n    global server\n    server.start()\n    data = {\n        \"query\": \"Machine learning is\",\n        \"texts\": TEST_DOCUMENTS,\n    }\n    if top_n is not None:\n        data[\"top_n\"] = top_n\n    res = server.make_request(\"POST\", \"/rerank\", data=data)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "peekOfCode": "server = ServerPreset.jina_reranker_tiny()\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.jina_reranker_tiny()\nTEST_DOCUMENTS = [\n    \"A machine is a physical system that uses power to apply forces and control movement to perform an action. The term is commonly applied to artificial devices, such as those employing engines or motors, but also to natural biological macromolecules, such as molecular machines.\",\n    \"Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, non-human animals, and some machines; there is also evidence for some kind of learning in certain plants.\",\n    \"Machine learning is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.\",\n    \"Paris, capitale de la France, est une grande ville europenne et un centre mondial de l'art, de la mode, de la gastronomie et de la culture. Son paysage urbain du XIXe sicle est travers par de larges boulevards et la Seine.\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "documentation": {}
    },
    {
        "label": "TEST_DOCUMENTS",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "peekOfCode": "TEST_DOCUMENTS = [\n    \"A machine is a physical system that uses power to apply forces and control movement to perform an action. The term is commonly applied to artificial devices, such as those employing engines or motors, but also to natural biological macromolecules, such as molecular machines.\",\n    \"Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, non-human animals, and some machines; there is also evidence for some kind of learning in certain plants.\",\n    \"Machine learning is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.\",\n    \"Paris, capitale de la France, est une grande ville europenne et un centre mondial de l'art, de la mode, de la gastronomie et de la culture. Son paysage urbain du XIXe sicle est travers par de larges boulevards et la Seine.\"\n]\ndef test_rerank():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/rerank\", data={",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_rerank",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.router()\n@pytest.mark.parametrize(\n    \"model,success\",\n    [\n        (\"ggml-org/tinygemma3-GGUF:Q8_0\", True),\n        (\"non-existent/model\", False),\n    ]\n)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "documentation": {}
    },
    {
        "label": "test_router_chat_completion_stream",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "peekOfCode": "def test_router_chat_completion_stream(model: str, success: bool):\n    global server\n    server.start()\n    content = \"\"\n    ex: ServerError | None = None\n    try:\n        res = server.make_stream_request(\"POST\", \"/chat/completions\", data={\n            \"model\": model,\n            \"max_tokens\": 16,\n            \"messages\": [",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "documentation": {}
    },
    {
        "label": "test_router_unload_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "peekOfCode": "def test_router_unload_model():\n    global server\n    server.start()\n    model_id = \"ggml-org/tinygemma3-GGUF:Q8_0\"\n    _load_model_and_wait(model_id)\n    unload_res = server.make_request(\"POST\", \"/models/unload\", data={\"model\": model_id})\n    assert unload_res.status_code == 200\n    assert unload_res.body.get(\"success\") is True\n    _wait_for_model_status(model_id, {\"unloaded\"})\ndef test_router_models_max_evicts_lru():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "documentation": {}
    },
    {
        "label": "test_router_models_max_evicts_lru",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "peekOfCode": "def test_router_models_max_evicts_lru():\n    global server\n    server.models_max = 2\n    server.start()\n    candidate_models = [\n        \"ggml-org/tinygemma3-GGUF:Q8_0\",\n        \"ggml-org/test-model-stories260K\",\n        \"ggml-org/test-model-stories260K-infill\",\n    ]\n    # Load only the first 2 models to fill the cache",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "documentation": {}
    },
    {
        "label": "test_router_no_models_autoload",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "peekOfCode": "def test_router_no_models_autoload():\n    global server\n    server.no_models_autoload = True\n    server.start()\n    model_id = \"ggml-org/tinygemma3-GGUF:Q8_0\"\n    res = server.make_request(\n        \"POST\",\n        \"/v1/chat/completions\",\n        data={\n            \"model\": model_id,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "documentation": {}
    },
    {
        "label": "test_router_api_key_required",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "peekOfCode": "def test_router_api_key_required():\n    global server\n    server.api_key = \"sk-router-secret\"\n    server.start()\n    model_id = \"ggml-org/tinygemma3-GGUF:Q8_0\"\n    auth_headers = {\"Authorization\": f\"Bearer {server.api_key}\"}\n    res = server.make_request(\n        \"POST\",\n        \"/v1/chat/completions\",\n        data={",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_router",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.api_key = TEST_API_KEY\n@pytest.mark.parametrize(\"endpoint\", [\"/health\", \"/models\"])\ndef test_access_public_endpoint(endpoint: str):\n    global server\n    server.start()\n    res = server.make_request(\"GET\", endpoint)\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "documentation": {}
    },
    {
        "label": "test_access_public_endpoint",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "peekOfCode": "def test_access_public_endpoint(endpoint: str):\n    global server\n    server.start()\n    res = server.make_request(\"GET\", endpoint)\n    assert res.status_code == 200\n    assert \"error\" not in res.body\n@pytest.mark.parametrize(\"api_key\", [None, \"invalid-key\"])\ndef test_incorrect_api_key(api_key: str):\n    global server\n    server.start()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "documentation": {}
    },
    {
        "label": "test_incorrect_api_key",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "peekOfCode": "def test_incorrect_api_key(api_key: str):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completions\", data={\n        \"prompt\": \"I believe the meaning of life is\",\n    }, headers={\n        \"Authorization\": f\"Bearer {api_key}\" if api_key else None,\n    })\n    assert res.status_code == 401\n    assert \"error\" in res.body",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "documentation": {}
    },
    {
        "label": "test_correct_api_key",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "peekOfCode": "def test_correct_api_key():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completions\", data={\n        \"prompt\": \"I believe the meaning of life is\",\n    }, headers={\n        \"Authorization\": f\"Bearer {TEST_API_KEY}\",\n    })\n    assert res.status_code == 200\n    assert \"error\" not in res.body",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "documentation": {}
    },
    {
        "label": "test_correct_api_key_anthropic_header",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "peekOfCode": "def test_correct_api_key_anthropic_header():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completions\", data={\n        \"prompt\": \"I believe the meaning of life is\",\n    }, headers={\n        \"X-Api-Key\": TEST_API_KEY,\n    })\n    assert res.status_code == 200\n    assert \"error\" not in res.body",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "documentation": {}
    },
    {
        "label": "test_openai_library_correct_api_key",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "peekOfCode": "def test_openai_library_correct_api_key():\n    global server\n    server.start()\n    client = OpenAI(api_key=TEST_API_KEY, base_url=f\"http://{server.server_host}:{server.server_port}\")\n    res = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a chatbot.\"},\n            {\"role\": \"user\", \"content\": \"What is the meaning of life?\"},\n        ],",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "documentation": {}
    },
    {
        "label": "test_cors_options",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "peekOfCode": "def test_cors_options(origin: str, cors_header: str, cors_header_value: str):\n    global server\n    server.start()\n    res = server.make_request(\"OPTIONS\", \"/completions\", headers={\n        \"Origin\": origin,\n        \"Access-Control-Request-Method\": \"POST\",\n        \"Access-Control-Request-Headers\": \"Authorization\",\n    })\n    assert res.status_code == 200\n    assert cors_header in res.headers",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "documentation": {}
    },
    {
        "label": "test_local_media_file",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "peekOfCode": "def test_local_media_file(media_path, image_url, success,):\n    server = ServerPreset.tinygemma3()\n    server.media_path = media_path\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"max_tokens\": 1,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": \"test\"},\n                {\"type\": \"image_url\", \"image_url\": {",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "peekOfCode": "server = ServerPreset.tinyllama2()\nTEST_API_KEY = \"sk-this-is-the-secret-key\"\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.api_key = TEST_API_KEY\n@pytest.mark.parametrize(\"endpoint\", [\"/health\", \"/models\"])\ndef test_access_public_endpoint(endpoint: str):\n    global server",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "documentation": {}
    },
    {
        "label": "TEST_API_KEY",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "peekOfCode": "TEST_API_KEY = \"sk-this-is-the-secret-key\"\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.api_key = TEST_API_KEY\n@pytest.mark.parametrize(\"endpoint\", [\"/health\", \"/models\"])\ndef test_access_public_endpoint(endpoint: str):\n    global server\n    server.start()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_security",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_sleep",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_sleep",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\ndef test_server_sleep():\n    global server\n    server.sleep_idle_seconds = 1\n    server.start()\n    # wait a bit so that server can go to sleep\n    time.sleep(2)\n    # make sure these endpoints are still responsive after sleep",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_sleep",
        "documentation": {}
    },
    {
        "label": "test_server_sleep",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_sleep",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_sleep",
        "peekOfCode": "def test_server_sleep():\n    global server\n    server.sleep_idle_seconds = 1\n    server.start()\n    # wait a bit so that server can go to sleep\n    time.sleep(2)\n    # make sure these endpoints are still responsive after sleep\n    res = server.make_request(\"GET\", \"/health\")\n    assert res.status_code == 200\n    res = server.make_request(\"GET\", \"/props\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_sleep",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_sleep",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_sleep",
        "peekOfCode": "server = ServerPreset.tinyllama2()\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\ndef test_server_sleep():\n    global server\n    server.sleep_idle_seconds = 1\n    server.start()\n    # wait a bit so that server can go to sleep",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_sleep",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.slot_save_path = \"./tmp\"\n    server.temperature = 0.0\ndef test_slot_save_restore():\n    global server\n    server.start()\n    # First prompt in slot 1 should be fully processed\n    res = server.make_request(\"POST\", \"/completion\", data={",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "documentation": {}
    },
    {
        "label": "test_slot_save_restore",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "peekOfCode": "def test_slot_save_restore():\n    global server\n    server.start()\n    # First prompt in slot 1 should be fully processed\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"What is the capital of France?\",\n        \"id_slot\": 1,\n        \"cache_prompt\": True,\n    })\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "documentation": {}
    },
    {
        "label": "test_slot_erase",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "peekOfCode": "def test_slot_erase():\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"What is the capital of France?\",\n        \"id_slot\": 1,\n        \"cache_prompt\": True,\n    })\n    assert res.status_code == 200\n    assert match_regex(\"(Whiskers|Flana)+\", res.body[\"content\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "peekOfCode": "server = ServerPreset.tinyllama2()\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.slot_save_path = \"./tmp\"\n    server.temperature = 0.0\ndef test_slot_save_restore():\n    global server\n    server.start()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_slot_save",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.stories15m_moe()\n    # set default values\n    server.model_draft = download_file(MODEL_DRAFT_FILE_URL)\n    server.draft_min = 4\n    server.draft_max = 8\n    server.fa = \"off\"\n@pytest.fixture(autouse=True)\ndef fixture_create_server():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "documentation": {}
    },
    {
        "label": "fixture_create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "peekOfCode": "def fixture_create_server():\n    return create_server()\ndef test_with_and_without_draft():\n    global server\n    server.model_draft = None  # disable draft model\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"I believe the meaning of life is\",\n        \"temperature\": 0.0,\n        \"top_k\": 1,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "documentation": {}
    },
    {
        "label": "test_with_and_without_draft",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "peekOfCode": "def test_with_and_without_draft():\n    global server\n    server.model_draft = None  # disable draft model\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"I believe the meaning of life is\",\n        \"temperature\": 0.0,\n        \"top_k\": 1,\n    })\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "documentation": {}
    },
    {
        "label": "test_different_draft_min_draft_max",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "peekOfCode": "def test_different_draft_min_draft_max():\n    global server\n    test_values = [\n        (1, 2),\n        (1, 4),\n        (4, 8),\n        (4, 12),\n        (8, 16),\n    ]\n    last_content = None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "documentation": {}
    },
    {
        "label": "test_slot_ctx_not_exceeded",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "peekOfCode": "def test_slot_ctx_not_exceeded():\n    global server\n    server.n_ctx = 256\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"Hello \" * 248,\n        \"temperature\": 0.0,\n        \"top_k\": 1,\n        \"speculative.p_min\": 0.0,\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "documentation": {}
    },
    {
        "label": "test_with_ctx_shift",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "peekOfCode": "def test_with_ctx_shift():\n    global server\n    server.n_ctx = 256\n    server.enable_ctx_shift = True\n    server.start()\n    res = server.make_request(\"POST\", \"/completion\", data={\n        \"prompt\": \"Hello \" * 248,\n        \"temperature\": 0.0,\n        \"top_k\": 1,\n        \"n_predict\": 256,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "documentation": {}
    },
    {
        "label": "test_multi_requests_parallel",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "peekOfCode": "def test_multi_requests_parallel(n_slots: int, n_requests: int):\n    global server\n    server.n_slots = n_slots\n    server.start()\n    tasks = []\n    for _ in range(n_requests):\n        tasks.append((server.make_request, (\"POST\", \"/completion\", {\n            \"prompt\": \"I believe the meaning of life is\",\n            \"temperature\": 0.0,\n            \"top_k\": 1,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "peekOfCode": "server = ServerPreset.stories15m_moe()\nMODEL_DRAFT_FILE_URL = \"https://huggingface.co/ggml-org/models/resolve/main/tinyllamas/stories15M-q4_0.gguf\"\ndef create_server():\n    global server\n    server = ServerPreset.stories15m_moe()\n    # set default values\n    server.model_draft = download_file(MODEL_DRAFT_FILE_URL)\n    server.draft_min = 4\n    server.draft_max = 8\n    server.fa = \"off\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "documentation": {}
    },
    {
        "label": "MODEL_DRAFT_FILE_URL",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "peekOfCode": "MODEL_DRAFT_FILE_URL = \"https://huggingface.co/ggml-org/models/resolve/main/tinyllamas/stories15M-q4_0.gguf\"\ndef create_server():\n    global server\n    server = ServerPreset.stories15m_moe()\n    # set default values\n    server.model_draft = download_file(MODEL_DRAFT_FILE_URL)\n    server.draft_min = 4\n    server.draft_max = 8\n    server.fa = \"off\"\n@pytest.fixture(autouse=True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_speculative",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.model_alias = \"tinyllama-2\"\n    server.n_slots = 1\n@pytest.mark.parametrize(\"tools\", [None, [], [TEST_TOOL]])\n@pytest.mark.parametrize(\"template_name,reasoning_budget,expected_end\", [\n    (\"deepseek-ai-DeepSeek-R1-Distill-Qwen-32B\", None, \"<think>\\n\"),\n    (\"deepseek-ai-DeepSeek-R1-Distill-Qwen-32B\",   -1, \"<think>\\n\"),\n    (\"deepseek-ai-DeepSeek-R1-Distill-Qwen-32B\",    0, \"<think>\\n</think>\"),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "documentation": {}
    },
    {
        "label": "test_reasoning_budget",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "peekOfCode": "def test_reasoning_budget(template_name: str, reasoning_budget: int | None, expected_end: str, tools: list[dict]):\n    global server\n    server.jinja = True\n    server.reasoning_budget = reasoning_budget\n    server.chat_template_file = f'../../../models/templates/{template_name}.jinja'\n    server.start()\n    res = server.make_request(\"POST\", \"/apply-template\", data={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is today?\"},\n        ],",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "documentation": {}
    },
    {
        "label": "test_date_inside_prompt",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "peekOfCode": "def test_date_inside_prompt(template_name: str, format: str, tools: list[dict]):\n    global server\n    server.jinja = True\n    server.chat_template_file = f'../../../models/templates/{template_name}.jinja'\n    server.start()\n    res = server.make_request(\"POST\", \"/apply-template\", data={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is today?\"},\n        ],\n        \"tools\": tools,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "documentation": {}
    },
    {
        "label": "test_add_generation_prompt",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "peekOfCode": "def test_add_generation_prompt(template_name: str, expected_generation_prompt: str, add_generation_prompt: bool):\n    global server\n    server.jinja = True\n    server.chat_template_file = f'../../../models/templates/{template_name}.jinja'\n    server.start()\n    res = server.make_request(\"POST\", \"/apply-template\", data={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is today?\"},\n        ],\n        \"add_generation_prompt\": add_generation_prompt,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "peekOfCode": "path = Path(__file__).resolve().parents[1]\nsys.path.insert(0, str(path))\nimport datetime\nfrom utils import *\nserver: ServerProcess\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.model_alias = \"tinyllama-2\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_template",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\ndef test_tokenize_detokenize():\n    global server\n    server.start()\n    # tokenize\n    content = \"What is the capital of France ?\"\n    res_tok = server.make_request(\"POST\", \"/tokenize\", data={\n        \"content\": content",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "documentation": {}
    },
    {
        "label": "test_tokenize_detokenize",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "peekOfCode": "def test_tokenize_detokenize():\n    global server\n    server.start()\n    # tokenize\n    content = \"What is the capital of France ?\"\n    res_tok = server.make_request(\"POST\", \"/tokenize\", data={\n        \"content\": content\n    })\n    assert res_tok.status_code == 200\n    assert len(res_tok.body[\"tokens\"]) > 5",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "documentation": {}
    },
    {
        "label": "test_tokenize_with_bos",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "peekOfCode": "def test_tokenize_with_bos():\n    global server\n    server.start()\n    # tokenize\n    content = \"What is the capital of France ?\"\n    bosId = 1\n    res_tok = server.make_request(\"POST\", \"/tokenize\", data={\n        \"content\": content,\n        \"add_special\": True,\n    })",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "documentation": {}
    },
    {
        "label": "test_tokenize_with_pieces",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "peekOfCode": "def test_tokenize_with_pieces():\n    global server\n    server.start()\n    # tokenize\n    content = \"This is a test string with unicode  and emoji \"\n    res_tok = server.make_request(\"POST\", \"/tokenize\", data={\n        \"content\": content,\n        \"with_pieces\": True,\n    })\n    assert res_tok.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "peekOfCode": "server = ServerPreset.tinyllama2()\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\ndef test_tokenize_detokenize():\n    global server\n    server.start()\n    # tokenize\n    content = \"What is the capital of France ?\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tokenize",
        "documentation": {}
    },
    {
        "label": "CompletionMode",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "class CompletionMode(Enum):\n    NORMAL = \"normal\"\n    STREAMED = \"streamed\"\nTEST_TOOL = {\n    \"type\":\"function\",\n    \"function\": {\n        \"name\": \"test\",\n        \"description\": \"\",\n        \"parameters\": {\n            \"type\": \"object\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.model_alias = \"tinyllama-2-tool-call\"\n    server.server_port = 8081\n    server.n_slots = 1\n    server.n_ctx = 8192\n    server.n_batch = 2048\nclass CompletionMode(Enum):\n    NORMAL = \"normal\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "do_test_completion_with_required_tool_tiny",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def do_test_completion_with_required_tool_tiny(server: ServerProcess, tool: dict, argument_key: str | None, n_predict, **kwargs):\n    body = server.make_any_request(\"POST\", \"/v1/chat/completions\", data={\n        \"max_tokens\": n_predict,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a coding assistant.\"},\n            {\"role\": \"user\", \"content\": \"Write an example\"},\n        ],\n        \"tool_choice\": \"required\",\n        \"tools\": [tool],\n        \"parallel_tool_calls\": False,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "test_completion_with_required_tool_tiny_fast",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def test_completion_with_required_tool_tiny_fast(template_name: str, tool: dict, argument_key: str | None, stream: CompletionMode):\n    global server\n    n_predict = 1024\n    # server = ServerPreset.stories15m_moe()\n    server.jinja = True\n    server.n_predict = n_predict\n    server.chat_template_file = f'../../../models/templates/{template_name}.jinja'\n    server.start()\n    do_test_completion_with_required_tool_tiny(server, tool, argument_key, n_predict, stream=stream == CompletionMode.STREAMED, temperature=0.0, top_k=1, top_p=1.0)\n@pytest.mark.slow",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "test_completion_with_required_tool_tiny_slow",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def test_completion_with_required_tool_tiny_slow(template_name: str, tool: dict, argument_key: str | None, stream: CompletionMode):\n    global server\n    n_predict = 512\n    # server = ServerPreset.stories15m_moe()\n    server.jinja = True\n    server.n_predict = n_predict\n    server.chat_template_file = f'../../../models/templates/{template_name}.jinja'\n    server.start(timeout_seconds=TIMEOUT_START_SLOW)\n    do_test_completion_with_required_tool_tiny(server, tool, argument_key, n_predict, stream=stream == CompletionMode.STREAMED)\n@pytest.mark.slow",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "test_completion_with_required_tool_real_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def test_completion_with_required_tool_real_model(tool: dict, argument_key: str | None, hf_repo: str, template_override: str | Tuple[str, str | None] | None, stream: CompletionMode):\n    global server\n    n_predict = 512\n    server.jinja = True\n    server.n_ctx = 8192\n    server.n_predict = n_predict\n    server.model_hf_repo = hf_repo\n    server.model_hf_file = None\n    if isinstance(template_override, tuple):\n        (template_hf_repo, template_variant) = template_override",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "do_test_completion_without_tool_call",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def do_test_completion_without_tool_call(server: ServerProcess, n_predict: int, tools: list[dict], tool_choice: str | None, **kwargs):\n    body = server.make_any_request(\"POST\", \"/v1/chat/completions\", data={\n        \"max_tokens\": n_predict,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a coding assistant.\"},\n            {\"role\": \"user\", \"content\": \"say hello world with python\"},\n        ],\n        \"tools\": tools if tools else None,\n        \"tool_choice\": tool_choice,\n        **kwargs,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "test_completion_without_tool_call_fast",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def test_completion_without_tool_call_fast(template_name: str, n_predict: int, tools: list[dict], tool_choice: str | None, stream: CompletionMode):\n    global server\n    server.n_predict = n_predict\n    server.jinja = True\n    server.chat_template_file = f'../../../models/templates/{template_name}.jinja'\n    server.start()\n    do_test_completion_without_tool_call(server, n_predict, tools, tool_choice, stream=stream == CompletionMode.STREAMED)\n@pytest.mark.slow\n@pytest.mark.parametrize(\"stream\", [CompletionMode.NORMAL, CompletionMode.STREAMED])\n@pytest.mark.parametrize(\"template_name,n_predict,tools,tool_choice\", [",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "test_completion_without_tool_call_slow",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def test_completion_without_tool_call_slow(template_name: str, n_predict: int, tools: list[dict], tool_choice: str | None, stream: CompletionMode):\n    global server\n    server.n_predict = n_predict\n    server.jinja = True\n    server.chat_template_file = f'../../../models/templates/{template_name}.jinja'\n    server.start(timeout_seconds=TIMEOUT_START_SLOW)\n    do_test_completion_without_tool_call(server, n_predict, tools, tool_choice, stream=stream == CompletionMode.STREAMED)\n@pytest.mark.slow\n@pytest.mark.parametrize(\"stream\", [CompletionMode.NORMAL, CompletionMode.STREAMED])\n@pytest.mark.parametrize(\"hf_repo,template_override\", [",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "test_weather",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def test_weather(hf_repo: str, template_override: str | Tuple[str, str | None] | None, stream: CompletionMode):\n    global server\n    n_predict = 512\n    server.jinja = True\n    server.n_ctx = 8192\n    server.n_predict = n_predict\n    server.model_hf_repo = hf_repo\n    server.model_hf_file = None\n    if isinstance(template_override, tuple):\n        (template_hf_repo, template_variant) = template_override",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "do_test_weather",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def do_test_weather(server: ServerProcess, **kwargs):\n    body = server.make_any_request(\"POST\", \"/v1/chat/completions\", data={\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a chatbot that uses tools/functions. Dont overthink things.\"},\n            {\"role\": \"user\", \"content\": \"What is the weather in Istanbul?\"},\n        ],\n        \"tools\": [WEATHER_TOOL],\n        **kwargs,\n    }, timeout=TIMEOUT_HTTP_REQUEST)\n    choice = body[\"choices\"][0]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "test_calc_result",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def test_calc_result(result_override: str | None, n_predict: int, hf_repo: str, template_override: str | Tuple[str, str | None] | None, stream: CompletionMode):\n    global server\n    server.jinja = True\n    server.n_ctx = 8192 * 2\n    server.n_predict = n_predict\n    server.model_hf_repo = hf_repo\n    server.model_hf_file = None\n    if isinstance(template_override, tuple):\n        (template_hf_repo, template_variant) = template_override\n        server.chat_template_file = f\"../../../models/templates/{template_hf_repo.replace('/', '-') + ('-' + template_variant if template_variant else '')}.jinja\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "do_test_calc_result",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def do_test_calc_result(server: ServerProcess, result_override: str | None, n_predict: int, **kwargs):\n    body = server.make_any_request(\"POST\", \"/v1/chat/completions\", data={\n        \"max_tokens\": n_predict,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a tools-calling assistant. You express numerical values with at most two decimals.\"},\n            {\"role\": \"user\", \"content\": \"What's the y coordinate of a point on the unit sphere at angle 30 degrees?\"},\n            {\n                \"role\": \"assistant\",\n                \"content\": None,\n                \"tool_calls\": [",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "test_thoughts",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def test_thoughts(n_predict: int, reasoning_format: Literal['deepseek', 'none'] | None, expect_content: str | None, expect_reasoning_content: str | None, hf_repo: str, template_override: str | Tuple[str, str | None] | None, stream: CompletionMode):\n    global server\n    server.reasoning_format = reasoning_format\n    server.jinja = True\n    server.n_ctx = 8192 * 2\n    server.n_predict = n_predict\n    server.model_hf_repo = hf_repo\n    server.model_hf_file = None\n    if isinstance(template_override, tuple):\n        (template_hf_repo, template_variant) = template_override",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "test_hello_world",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def test_hello_world(hf_repo: str, template_override: str | Tuple[str, str | None] | None, stream: CompletionMode):\n    global server\n    n_predict = 512 # High because of DeepSeek R1\n    server.jinja = True\n    server.n_ctx = 8192\n    server.n_predict = n_predict\n    server.model_hf_repo = hf_repo\n    server.model_hf_file = None\n    if isinstance(template_override, tuple):\n        (template_hf_repo, template_variant) = template_override",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "do_test_hello_world",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "def do_test_hello_world(server: ServerProcess, **kwargs):\n    body = server.make_any_request(\"POST\", \"/v1/chat/completions\", data={\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a tool-calling agent.\"},\n            {\"role\": \"user\", \"content\": \"say hello world with python\"},\n        ],\n        \"tools\": [PYTHON_TOOL],\n        **kwargs,\n    }, timeout=TIMEOUT_HTTP_REQUEST)\n    choice = body[\"choices\"][0]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "path = Path(__file__).resolve().parents[1]\nsys.path.insert(0, str(path))\nfrom utils import *\nfrom enum import Enum\nserver: ServerProcess\nTIMEOUT_START_SLOW = 15 * 60 # this is needed for real model tests\nTIMEOUT_HTTP_REQUEST = 60\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "TIMEOUT_START_SLOW",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "TIMEOUT_START_SLOW = 15 * 60 # this is needed for real model tests\nTIMEOUT_HTTP_REQUEST = 60\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.model_alias = \"tinyllama-2-tool-call\"\n    server.server_port = 8081\n    server.n_slots = 1\n    server.n_ctx = 8192",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "TIMEOUT_HTTP_REQUEST",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "TIMEOUT_HTTP_REQUEST = 60\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinyllama2()\n    server.model_alias = \"tinyllama-2-tool-call\"\n    server.server_port = 8081\n    server.n_slots = 1\n    server.n_ctx = 8192\n    server.n_batch = 2048",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "TEST_TOOL",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "TEST_TOOL = {\n    \"type\":\"function\",\n    \"function\": {\n        \"name\": \"test\",\n        \"description\": \"\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"success\": {\"type\": \"boolean\", \"const\": True},\n            },",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "PYTHON_TOOL",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "PYTHON_TOOL = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"python\",\n        \"description\": \"Runs code in an ipython interpreter and returns the result of the execution after 60 seconds.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"code\": {\n                    \"type\": \"string\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "WEATHER_TOOL",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "peekOfCode": "WEATHER_TOOL = {\n  \"type\":\"function\",\n  \"function\":{\n    \"name\":\"get_current_weather\",\n    \"description\":\"Get the current weather in a given location\",\n    \"parameters\":{\n      \"type\":\"object\",\n      \"properties\":{\n        \"location\":{\n          \"type\":\"string\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_tool_call",
        "documentation": {}
    },
    {
        "label": "get_img_url",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "peekOfCode": "def get_img_url(id: str) -> str:\n    IMG_URL_0 = \"https://huggingface.co/ggml-org/tinygemma3-GGUF/resolve/main/test/11_truck.png\"\n    IMG_URL_1 = \"https://huggingface.co/ggml-org/tinygemma3-GGUF/resolve/main/test/91_cat.png\"\n    if id == \"IMG_URL_0\":\n        return IMG_URL_0\n    elif id == \"IMG_URL_1\":\n        return IMG_URL_1\n    elif id == \"IMG_BASE64_URI_0\":\n        response = requests.get(IMG_URL_0)\n        response.raise_for_status() # Raise an exception for bad status codes",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "documentation": {}
    },
    {
        "label": "create_server",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "peekOfCode": "def create_server():\n    global server\n    server = ServerPreset.tinygemma3()\ndef test_models_supports_multimodal_capability():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/models\", data={})\n    assert res.status_code == 200\n    model_info = res.body[\"models\"][0]\n    print(model_info)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "documentation": {}
    },
    {
        "label": "test_models_supports_multimodal_capability",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "peekOfCode": "def test_models_supports_multimodal_capability():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/models\", data={})\n    assert res.status_code == 200\n    model_info = res.body[\"models\"][0]\n    print(model_info)\n    assert \"completion\" in model_info[\"capabilities\"]\n    assert \"multimodal\" in model_info[\"capabilities\"]\ndef test_v1_models_supports_multimodal_capability():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "documentation": {}
    },
    {
        "label": "test_v1_models_supports_multimodal_capability",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "peekOfCode": "def test_v1_models_supports_multimodal_capability():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/v1/models\", data={})\n    assert res.status_code == 200\n    model_info = res.body[\"models\"][0]\n    print(model_info)\n    assert \"completion\" in model_info[\"capabilities\"]\n    assert \"multimodal\" in model_info[\"capabilities\"]\n@pytest.mark.parametrize(",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "documentation": {}
    },
    {
        "label": "test_vision_chat_completion",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "peekOfCode": "def test_vision_chat_completion(prompt, image_url, success, re_content):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/chat/completions\", data={\n        \"temperature\": 0.0,\n        \"top_k\": 1,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n                {\"type\": \"image_url\", \"image_url\": {",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "documentation": {}
    },
    {
        "label": "test_vision_completion",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "peekOfCode": "def test_vision_completion(prompt, image_data, success, re_content):\n    global server\n    server.start()\n    res = server.make_request(\"POST\", \"/completions\", data={\n        \"temperature\": 0.0,\n        \"top_k\": 1,\n        \"prompt\": {\n            JSON_PROMPT_STRING_KEY: prompt,\n            JSON_MULTIMODAL_KEY: [ get_img_url(image_data) ],\n        },",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "documentation": {}
    },
    {
        "label": "test_vision_embeddings",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "peekOfCode": "def test_vision_embeddings(prompt, image_data, success):\n    global server\n    server.server_embeddings = True\n    server.n_batch = 512\n    server.start()\n    image_data = get_img_url(image_data)\n    res = server.make_request(\"POST\", \"/embeddings\", data={\n        \"content\": [\n            { JSON_PROMPT_STRING_KEY: prompt, JSON_MULTIMODAL_KEY: [ image_data ] },\n            { JSON_PROMPT_STRING_KEY: prompt, JSON_MULTIMODAL_KEY: [ image_data ] },",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "documentation": {}
    },
    {
        "label": "JSON_MULTIMODAL_KEY",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "peekOfCode": "JSON_MULTIMODAL_KEY = \"multimodal_data\"\nJSON_PROMPT_STRING_KEY = \"prompt_string\"\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinygemma3()\ndef test_models_supports_multimodal_capability():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/models\", data={})",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "documentation": {}
    },
    {
        "label": "JSON_PROMPT_STRING_KEY",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "peekOfCode": "JSON_PROMPT_STRING_KEY = \"prompt_string\"\n@pytest.fixture(autouse=True)\ndef create_server():\n    global server\n    server = ServerPreset.tinygemma3()\ndef test_models_supports_multimodal_capability():\n    global server\n    server.start()\n    res = server.make_request(\"GET\", \"/models\", data={})\n    assert res.status_code == 200",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.unit.test_vision_api",
        "documentation": {}
    },
    {
        "label": "stop_server_after_each_test",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.conftest",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.conftest",
        "peekOfCode": "def stop_server_after_each_test():\n    # do nothing before each test\n    yield\n    # stop all servers after each test\n    instances = set(\n        server_instances\n    )  # copy the set to prevent 'Set changed size during iteration'\n    for server in instances:\n        server.stop()\n@pytest.fixture(scope=\"module\", autouse=True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.conftest",
        "documentation": {}
    },
    {
        "label": "do_something",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.conftest",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.conftest",
        "peekOfCode": "def do_something():\n    # this will be run once per test session, before any tests\n    ServerPreset.load_all()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.conftest",
        "documentation": {}
    },
    {
        "label": "ServerResponse",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "peekOfCode": "class ServerResponse:\n    headers: dict\n    status_code: int\n    body: dict | Any\nclass ServerError(Exception):\n    def __init__(self, code, body):\n        self.code = code\n        self.body = body\nclass ServerProcess:\n    # default options",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "documentation": {}
    },
    {
        "label": "ServerError",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "peekOfCode": "class ServerError(Exception):\n    def __init__(self, code, body):\n        self.code = code\n        self.body = body\nclass ServerProcess:\n    # default options\n    debug: bool = False\n    server_port: int = 8080\n    server_host: str = \"127.0.0.1\"\n    model_hf_repo: str | None = \"ggml-org/models\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "documentation": {}
    },
    {
        "label": "ServerProcess",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "peekOfCode": "class ServerProcess:\n    # default options\n    debug: bool = False\n    server_port: int = 8080\n    server_host: str = \"127.0.0.1\"\n    model_hf_repo: str | None = \"ggml-org/models\"\n    model_hf_file: str | None = \"tinyllamas/stories260K.gguf\"\n    model_alias: str = \"tinyllama-2\"\n    temperature: float = 0.8\n    seed: int = 42",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "documentation": {}
    },
    {
        "label": "ServerPreset",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "peekOfCode": "class ServerPreset:\n    @staticmethod\n    def load_all() -> None:\n        \"\"\" Load all server presets to ensure model files are cached. \"\"\"\n        servers: List[ServerProcess] = [\n            method()\n            for name, method in ServerPreset.__dict__.items()\n            if callable(method) and name != \"load_all\"\n        ]\n        for server in servers:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "documentation": {}
    },
    {
        "label": "parallel_function_calls",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "peekOfCode": "def parallel_function_calls(function_list: List[Tuple[Callable[..., Any], Tuple[Any, ...]]]) -> List[Any]:\n    \"\"\"\n    Run multiple functions in parallel and return results in the same order as calls. Equivalent to Promise.all in JS.\n    Example usage:\n    results = parallel_function_calls([\n        (func1, (arg1, arg2)),\n        (func2, (arg3, arg4)),\n    ])\n    \"\"\"\n    results = [None] * len(function_list)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "documentation": {}
    },
    {
        "label": "match_regex",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "peekOfCode": "def match_regex(regex: str, text: str) -> bool:\n    return (\n        re.compile(\n            regex, flags=RegexFlag.IGNORECASE | RegexFlag.MULTILINE | RegexFlag.DOTALL\n        ).search(text)\n        is not None\n    )\ndef download_file(url: str, output_file_path: str | None = None) -> str:\n    \"\"\"\n    Download a file from a URL to a local path. If the file already exists, it will not be downloaded again.",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "documentation": {}
    },
    {
        "label": "download_file",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "peekOfCode": "def download_file(url: str, output_file_path: str | None = None) -> str:\n    \"\"\"\n    Download a file from a URL to a local path. If the file already exists, it will not be downloaded again.\n    output_file_path is the local path to save the downloaded file. If not provided, the file will be saved in the root directory.\n    Returns the local path of the downloaded file.\n    \"\"\"\n    file_name = url.split('/').pop()\n    output_file = f'./tmp/{file_name}' if output_file_path is None else output_file_path\n    if not os.path.exists(output_file):\n        print(f\"Downloading {url} to {output_file}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "documentation": {}
    },
    {
        "label": "is_slow_test_allowed",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "peekOfCode": "def is_slow_test_allowed():\n    return os.environ.get(\"SLOW_TESTS\") == \"1\" or os.environ.get(\"SLOW_TESTS\") == \"ON\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HTTP_TIMEOUT",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "peekOfCode": "DEFAULT_HTTP_TIMEOUT = 60\nclass ServerResponse:\n    headers: dict\n    status_code: int\n    body: dict | Any\nclass ServerError(Exception):\n    def __init__(self, code, body):\n        self.code = code\n        self.body = body\nclass ServerProcess:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.server.tests.utils",
        "documentation": {}
    },
    {
        "label": "flatten_state_dict",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "def flatten_state_dict(state_dict, parent_key='', sep='.'):\n    items = []\n    items_new = []\n    for k, v in state_dict.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, torch.Tensor):\n            items.append((new_key, v))\n        elif isinstance(v, dict):\n            items.extend(flatten_state_dict(v, new_key, sep=sep).items())\n            return dict(items)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "model_path = './model.pt'\n# read from CLI\nif len(sys.argv) > 1:\n    model_path = sys.argv[1]\n# get the directory of the input model\npath_dst = os.path.dirname(model_path)\nprint(f\"Loading model from {model_path}\")\nmodel = torch.load(model_path, map_location='cpu')\n#print(model)\n# print all keys",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "path_dst",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "path_dst = os.path.dirname(model_path)\nprint(f\"Loading model from {model_path}\")\nmodel = torch.load(model_path, map_location='cpu')\n#print(model)\n# print all keys\nfor key in model.keys():\n    print(key)\n    if key == 'hyper_parameters':\n        #print(model[key])\n        # dump as json pretty",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "model = torch.load(model_path, map_location='cpu')\n#print(model)\n# print all keys\nfor key in model.keys():\n    print(key)\n    if key == 'hyper_parameters':\n        #print(model[key])\n        # dump as json pretty\n        print(json.dumps(model[key], indent=4))\n    #if key != 'state_dict' and key != 'optimizer_states':",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "flattened_state_dict",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "flattened_state_dict = flatten_state_dict(state_dict)\n# Convert the model to the safetensors format\noutput_path = path_dst + '/model.safetensors'\nsave_file(flattened_state_dict, output_path)\nprint(f\"Model has been successfully converted and saved to {output_path}\")\n# Calculate the total size of the .safetensors file\ntotal_size = os.path.getsize(output_path)\n# Create the weight map\nweight_map = {\n    \"model.safetensors\": [\"*\"]  # Assuming all weights are in one file",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "output_path = path_dst + '/model.safetensors'\nsave_file(flattened_state_dict, output_path)\nprint(f\"Model has been successfully converted and saved to {output_path}\")\n# Calculate the total size of the .safetensors file\ntotal_size = os.path.getsize(output_path)\n# Create the weight map\nweight_map = {\n    \"model.safetensors\": [\"*\"]  # Assuming all weights are in one file\n}\n# Create metadata for the index.json file",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "total_size",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "total_size = os.path.getsize(output_path)\n# Create the weight map\nweight_map = {\n    \"model.safetensors\": [\"*\"]  # Assuming all weights are in one file\n}\n# Create metadata for the index.json file\nmetadata = {\n    \"total_size\": total_size,\n    \"weight_map\": weight_map\n}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "weight_map",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "weight_map = {\n    \"model.safetensors\": [\"*\"]  # Assuming all weights are in one file\n}\n# Create metadata for the index.json file\nmetadata = {\n    \"total_size\": total_size,\n    \"weight_map\": weight_map\n}\n# Save the metadata to index.json\nindex_path = path_dst + '/index.json'",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "metadata",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "metadata = {\n    \"total_size\": total_size,\n    \"weight_map\": weight_map\n}\n# Save the metadata to index.json\nindex_path = path_dst + '/index.json'\nwith open(index_path, 'w') as f:\n    json.dump(metadata, f, indent=4)\nprint(f\"Metadata has been saved to {index_path}\")\nconfig = {",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "index_path",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "index_path = path_dst + '/index.json'\nwith open(index_path, 'w') as f:\n    json.dump(metadata, f, indent=4)\nprint(f\"Metadata has been saved to {index_path}\")\nconfig = {\n    \"architectures\": [\n        \"WavTokenizerDec\"\n    ],\n    \"hidden_size\": 1282,\n    \"n_embd_features\": 512,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "peekOfCode": "config = {\n    \"architectures\": [\n        \"WavTokenizerDec\"\n    ],\n    \"hidden_size\": 1282,\n    \"n_embd_features\": 512,\n    \"n_ff\": 2304,\n    \"vocab_size\": 4096,\n    \"n_head\": 1,\n    \"layer_norm_epsilon\": 1e-6,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.convert_pt_to_hf",
        "documentation": {}
    },
    {
        "label": "fill_hann_window",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "def fill_hann_window(size, periodic=True):\n    if periodic:\n        return np.hanning(size + 1)[:-1]\n    return np.hanning(size)\ndef irfft(n_fft, complex_input):\n    return np.fft.irfft(complex_input, n=n_fft)\ndef fold(buffer, n_out, n_win, n_hop, n_pad):\n    result = np.zeros(n_out)\n    n_frames = len(buffer) // n_win\n    for i in range(n_frames):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "irfft",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "def irfft(n_fft, complex_input):\n    return np.fft.irfft(complex_input, n=n_fft)\ndef fold(buffer, n_out, n_win, n_hop, n_pad):\n    result = np.zeros(n_out)\n    n_frames = len(buffer) // n_win\n    for i in range(n_frames):\n        start = i * n_hop\n        end = start + n_win\n        result[start:end] += buffer[i * n_win:(i + 1) * n_win]\n    return result[n_pad:-n_pad] if n_pad > 0 else result",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "fold",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "def fold(buffer, n_out, n_win, n_hop, n_pad):\n    result = np.zeros(n_out)\n    n_frames = len(buffer) // n_win\n    for i in range(n_frames):\n        start = i * n_hop\n        end = start + n_win\n        result[start:end] += buffer[i * n_win:(i + 1) * n_win]\n    return result[n_pad:-n_pad] if n_pad > 0 else result\ndef process_frame(args):\n    l, n_fft, ST, hann = args",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "process_frame",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "def process_frame(args):\n    l, n_fft, ST, hann = args\n    frame = irfft(n_fft, ST[l])\n    frame = frame * hann\n    hann2 = hann * hann\n    return frame, hann2\ndef embd_to_audio(embd, n_codes, n_embd, n_thread=4):\n    embd = np.asarray(embd, dtype=np.float32).reshape(n_codes, n_embd)\n    n_fft = 1280\n    n_hop = 320",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "embd_to_audio",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "def embd_to_audio(embd, n_codes, n_embd, n_thread=4):\n    embd = np.asarray(embd, dtype=np.float32).reshape(n_codes, n_embd)\n    n_fft = 1280\n    n_hop = 320\n    n_win = 1280\n    n_pad = (n_win - n_hop) // 2\n    n_out = (n_codes - 1) * n_hop + n_win\n    hann = fill_hann_window(n_fft, True)\n    E = np.zeros((n_embd, n_codes), dtype=np.float32)\n    for l in range(n_codes):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "save_wav",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "def save_wav(filename, audio_data, sample_rate):\n    num_channels = 1\n    bits_per_sample = 16\n    bytes_per_sample = bits_per_sample // 8\n    data_size = len(audio_data) * bytes_per_sample\n    byte_rate = sample_rate * num_channels * bytes_per_sample\n    block_align = num_channels * bytes_per_sample\n    chunk_size = 36 + data_size  # 36 = size of header minus first 8 bytes\n    header = struct.pack(\n        '<4sI4s4sIHHIIHH4sI',",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "process_text",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "def process_text(text: str):\n    text = re.sub(r'\\d+(\\.\\d+)?', lambda x: x.group(), text.lower()) # TODO this needs to be fixed\n    text = re.sub(r'[-_/,\\.\\\\]', ' ', text)\n    text = re.sub(r'[^a-z\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text.split()\n# usage:\n# python tts-outetts.py http://server-llm:port http://server-dec:port \"text\"\nif len(sys.argv) <= 3:\n    print(\"usage: python tts-outetts.py http://server-llm:port http://server-dec:port \\\"text\\\"\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "host_llm",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "host_llm = sys.argv[1]\nhost_dec = sys.argv[2]\ntext = sys.argv[3]\nprefix = \"\"\"<|im_start|>\n<|text_start|>the<|text_sep|>overall<|text_sep|>package<|text_sep|>from<|text_sep|>just<|text_sep|>two<|text_sep|>people<|text_sep|>is<|text_sep|>pretty<|text_sep|>remarkable<|text_sep|>sure<|text_sep|>i<|text_sep|>have<|text_sep|>some<|text_sep|>critiques<|text_sep|>about<|text_sep|>some<|text_sep|>of<|text_sep|>the<|text_sep|>gameplay<|text_sep|>aspects<|text_sep|>but<|text_sep|>its<|text_sep|>still<|text_sep|>really<|text_sep|>enjoyable<|text_sep|>and<|text_sep|>it<|text_sep|>looks<|text_sep|>lovely<|text_sep|>\"\"\"\nwords = process_text(text)\nwords = \"<|text_sep|>\".join([i.strip() for i in words])\nwords += \"<|text_end|>\\n\"\n# voice data\n# TODO: load from json",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "host_dec",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "host_dec = sys.argv[2]\ntext = sys.argv[3]\nprefix = \"\"\"<|im_start|>\n<|text_start|>the<|text_sep|>overall<|text_sep|>package<|text_sep|>from<|text_sep|>just<|text_sep|>two<|text_sep|>people<|text_sep|>is<|text_sep|>pretty<|text_sep|>remarkable<|text_sep|>sure<|text_sep|>i<|text_sep|>have<|text_sep|>some<|text_sep|>critiques<|text_sep|>about<|text_sep|>some<|text_sep|>of<|text_sep|>the<|text_sep|>gameplay<|text_sep|>aspects<|text_sep|>but<|text_sep|>its<|text_sep|>still<|text_sep|>really<|text_sep|>enjoyable<|text_sep|>and<|text_sep|>it<|text_sep|>looks<|text_sep|>lovely<|text_sep|>\"\"\"\nwords = process_text(text)\nwords = \"<|text_sep|>\".join([i.strip() for i in words])\nwords += \"<|text_end|>\\n\"\n# voice data\n# TODO: load from json\n#suffix = \"\"\"<|audio_start|>",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "text = sys.argv[3]\nprefix = \"\"\"<|im_start|>\n<|text_start|>the<|text_sep|>overall<|text_sep|>package<|text_sep|>from<|text_sep|>just<|text_sep|>two<|text_sep|>people<|text_sep|>is<|text_sep|>pretty<|text_sep|>remarkable<|text_sep|>sure<|text_sep|>i<|text_sep|>have<|text_sep|>some<|text_sep|>critiques<|text_sep|>about<|text_sep|>some<|text_sep|>of<|text_sep|>the<|text_sep|>gameplay<|text_sep|>aspects<|text_sep|>but<|text_sep|>its<|text_sep|>still<|text_sep|>really<|text_sep|>enjoyable<|text_sep|>and<|text_sep|>it<|text_sep|>looks<|text_sep|>lovely<|text_sep|>\"\"\"\nwords = process_text(text)\nwords = \"<|text_sep|>\".join([i.strip() for i in words])\nwords += \"<|text_end|>\\n\"\n# voice data\n# TODO: load from json\n#suffix = \"\"\"<|audio_start|>\n#the<|t_0.08|><|code_start|><|257|><|740|><|636|><|913|><|788|><|1703|><|code_end|>",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "prefix = \"\"\"<|im_start|>\n<|text_start|>the<|text_sep|>overall<|text_sep|>package<|text_sep|>from<|text_sep|>just<|text_sep|>two<|text_sep|>people<|text_sep|>is<|text_sep|>pretty<|text_sep|>remarkable<|text_sep|>sure<|text_sep|>i<|text_sep|>have<|text_sep|>some<|text_sep|>critiques<|text_sep|>about<|text_sep|>some<|text_sep|>of<|text_sep|>the<|text_sep|>gameplay<|text_sep|>aspects<|text_sep|>but<|text_sep|>its<|text_sep|>still<|text_sep|>really<|text_sep|>enjoyable<|text_sep|>and<|text_sep|>it<|text_sep|>looks<|text_sep|>lovely<|text_sep|>\"\"\"\nwords = process_text(text)\nwords = \"<|text_sep|>\".join([i.strip() for i in words])\nwords += \"<|text_end|>\\n\"\n# voice data\n# TODO: load from json\n#suffix = \"\"\"<|audio_start|>\n#the<|t_0.08|><|code_start|><|257|><|740|><|636|><|913|><|788|><|1703|><|code_end|>\n#overall<|t_0.36|><|code_start|><|127|><|201|><|191|><|774|><|700|><|532|><|1056|><|557|><|798|><|298|><|1741|><|747|><|1662|><|1617|><|1702|><|1527|><|368|><|1588|><|1049|><|1008|><|1625|><|747|><|1576|><|728|><|1019|><|1696|><|1765|><|code_end|>",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "words",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "words = process_text(text)\nwords = \"<|text_sep|>\".join([i.strip() for i in words])\nwords += \"<|text_end|>\\n\"\n# voice data\n# TODO: load from json\n#suffix = \"\"\"<|audio_start|>\n#the<|t_0.08|><|code_start|><|257|><|740|><|636|><|913|><|788|><|1703|><|code_end|>\n#overall<|t_0.36|><|code_start|><|127|><|201|><|191|><|774|><|700|><|532|><|1056|><|557|><|798|><|298|><|1741|><|747|><|1662|><|1617|><|1702|><|1527|><|368|><|1588|><|1049|><|1008|><|1625|><|747|><|1576|><|728|><|1019|><|1696|><|1765|><|code_end|>\n#package<|t_0.56|><|code_start|><|935|><|584|><|1319|><|627|><|1016|><|1491|><|1344|><|1117|><|1526|><|1040|><|239|><|1435|><|951|><|498|><|723|><|1180|><|535|><|789|><|1649|><|1637|><|78|><|465|><|1668|><|901|><|595|><|1675|><|117|><|1009|><|1667|><|320|><|840|><|79|><|507|><|1762|><|1508|><|1228|><|1768|><|802|><|1450|><|1457|><|232|><|639|><|code_end|>\n#from<|t_0.19|><|code_start|><|604|><|782|><|1682|><|872|><|1532|><|1600|><|1036|><|1761|><|647|><|1554|><|1371|><|653|><|1595|><|950|><|code_end|>",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "words",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "words = \"<|text_sep|>\".join([i.strip() for i in words])\nwords += \"<|text_end|>\\n\"\n# voice data\n# TODO: load from json\n#suffix = \"\"\"<|audio_start|>\n#the<|t_0.08|><|code_start|><|257|><|740|><|636|><|913|><|788|><|1703|><|code_end|>\n#overall<|t_0.36|><|code_start|><|127|><|201|><|191|><|774|><|700|><|532|><|1056|><|557|><|798|><|298|><|1741|><|747|><|1662|><|1617|><|1702|><|1527|><|368|><|1588|><|1049|><|1008|><|1625|><|747|><|1576|><|728|><|1019|><|1696|><|1765|><|code_end|>\n#package<|t_0.56|><|code_start|><|935|><|584|><|1319|><|627|><|1016|><|1491|><|1344|><|1117|><|1526|><|1040|><|239|><|1435|><|951|><|498|><|723|><|1180|><|535|><|789|><|1649|><|1637|><|78|><|465|><|1668|><|901|><|595|><|1675|><|117|><|1009|><|1667|><|320|><|840|><|79|><|507|><|1762|><|1508|><|1228|><|1768|><|802|><|1450|><|1457|><|232|><|639|><|code_end|>\n#from<|t_0.19|><|code_start|><|604|><|782|><|1682|><|872|><|1532|><|1600|><|1036|><|1761|><|647|><|1554|><|1371|><|653|><|1595|><|950|><|code_end|>\n#just<|t_0.25|><|code_start|><|1782|><|1670|><|317|><|786|><|1748|><|631|><|599|><|1155|><|1364|><|1524|><|36|><|1591|><|889|><|1535|><|541|><|440|><|1532|><|50|><|870|><|code_end|>",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "#suffix",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "#suffix = \"\"\"<|audio_start|>\n#the<|t_0.08|><|code_start|><|257|><|740|><|636|><|913|><|788|><|1703|><|code_end|>\n#overall<|t_0.36|><|code_start|><|127|><|201|><|191|><|774|><|700|><|532|><|1056|><|557|><|798|><|298|><|1741|><|747|><|1662|><|1617|><|1702|><|1527|><|368|><|1588|><|1049|><|1008|><|1625|><|747|><|1576|><|728|><|1019|><|1696|><|1765|><|code_end|>\n#package<|t_0.56|><|code_start|><|935|><|584|><|1319|><|627|><|1016|><|1491|><|1344|><|1117|><|1526|><|1040|><|239|><|1435|><|951|><|498|><|723|><|1180|><|535|><|789|><|1649|><|1637|><|78|><|465|><|1668|><|901|><|595|><|1675|><|117|><|1009|><|1667|><|320|><|840|><|79|><|507|><|1762|><|1508|><|1228|><|1768|><|802|><|1450|><|1457|><|232|><|639|><|code_end|>\n#from<|t_0.19|><|code_start|><|604|><|782|><|1682|><|872|><|1532|><|1600|><|1036|><|1761|><|647|><|1554|><|1371|><|653|><|1595|><|950|><|code_end|>\n#just<|t_0.25|><|code_start|><|1782|><|1670|><|317|><|786|><|1748|><|631|><|599|><|1155|><|1364|><|1524|><|36|><|1591|><|889|><|1535|><|541|><|440|><|1532|><|50|><|870|><|code_end|>\n#two<|t_0.24|><|code_start|><|1681|><|1510|><|673|><|799|><|805|><|1342|><|330|><|519|><|62|><|640|><|1138|><|565|><|1552|><|1497|><|1552|><|572|><|1715|><|1732|><|code_end|>\n#people<|t_0.39|><|code_start|><|593|><|274|><|136|><|740|><|691|><|633|><|1484|><|1061|><|1138|><|1485|><|344|><|428|><|397|><|1562|><|645|><|917|><|1035|><|1449|><|1669|><|487|><|442|><|1484|><|1329|><|1832|><|1704|><|600|><|761|><|653|><|269|><|code_end|>\n#is<|t_0.16|><|code_start|><|566|><|583|><|1755|><|646|><|1337|><|709|><|802|><|1008|><|485|><|1583|><|652|><|10|><|code_end|>\n#pretty<|t_0.32|><|code_start|><|1818|><|1747|><|692|><|733|><|1010|><|534|><|406|><|1697|><|1053|><|1521|><|1355|><|1274|><|816|><|1398|><|211|><|1218|><|817|><|1472|><|1703|><|686|><|13|><|822|><|445|><|1068|><|code_end|>",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "suffix = [ 151667, 198, 1782, 155780, 151669, 151929, 152412, 152308, 152585, 152460, 153375, 151670, 198, 74455,\n          155808, 151669, 151799, 151873, 151863, 152446, 152372, 152204, 152728, 152229, 152470, 151970, 153413,\n          152419, 153334, 153289, 153374, 153199, 152040, 153260, 152721, 152680, 153297, 152419, 153248, 152400,\n          152691, 153368, 153437, 151670, 198, 1722, 155828, 151669, 152607, 152256, 152991, 152299, 152688, 153163,\n          153016, 152789, 153198, 152712, 151911, 153107, 152623, 152170, 152395, 152852, 152207, 152461, 153321,\n          153309, 151750, 152137, 153340, 152573, 152267, 153347, 151789, 152681, 153339, 151992, 152512, 151751,\n          152179, 153434, 153180, 152900, 153440, 152474, 153122, 153129, 151904, 152311, 151670, 198, 1499, 155791,\n          151669, 152276, 152454, 153354, 152544, 153204, 153272, 152708, 153433, 152319, 153226, 153043, 152325,\n          153267, 152622, 151670, 198, 4250, 155797, 151669, 153454, 153342, 151989, 152458, 153420, 152303, 152271,\n          152827, 153036, 153196, 151708, 153263, 152561, 153207, 152213, 152112, 153204, 151722, 152542, 151670, 198,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "response = requests.post(\n    host_llm + \"/completion\",\n    json={\n        \"prompt\": [prefix + words, *suffix],\n        \"n_predict\": 1024,\n        \"cache_prompt\": True,\n        \"return_tokens\": True,\n        \"samplers\": [\"top_k\"],\n        \"top_k\": 16,\n        \"seed\": 1003,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "response_json",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "response_json = response.json()\n#print(json.dumps(response_json, indent=4))\n#print(json.dumps(response_json[\"prompt\"], indent=4).replace(\"\\\\n\", \"\\n\"))\n#print(json.dumps(response_json[\"timings\"], indent=4))\n#print(json.dumps(response_json[\"tokens\"], indent=4))\ncodes = response_json[\"tokens\"]\ncodes = [t - 151672 for t in codes if t >= 151672 and t <= 155772]\nresponse = requests.post(\n    host_dec + \"/embeddings\",\n    json={",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "codes",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "codes = response_json[\"tokens\"]\ncodes = [t - 151672 for t in codes if t >= 151672 and t <= 155772]\nresponse = requests.post(\n    host_dec + \"/embeddings\",\n    json={\n        \"input\": [*codes],\n    }\n)\nresponse_json = response.json()\n#print(json.dumps(response_json, indent=4))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "codes",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "codes = [t - 151672 for t in codes if t >= 151672 and t <= 155772]\nresponse = requests.post(\n    host_dec + \"/embeddings\",\n    json={\n        \"input\": [*codes],\n    }\n)\nresponse_json = response.json()\n#print(json.dumps(response_json, indent=4))\n# spectrogram",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "response = requests.post(\n    host_dec + \"/embeddings\",\n    json={\n        \"input\": [*codes],\n    }\n)\nresponse_json = response.json()\n#print(json.dumps(response_json, indent=4))\n# spectrogram\nembd = response_json[0][\"embedding\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "response_json",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "response_json = response.json()\n#print(json.dumps(response_json, indent=4))\n# spectrogram\nembd = response_json[0][\"embedding\"]\nn_codes = len(embd)\nn_embd = len(embd[0])\nprint('spectrogram generated: n_codes: %d, n_embd: %d' % (n_codes, n_embd))\n# post-process the spectrogram to convert to audio\nprint('converting to audio ...')\naudio = embd_to_audio(embd, n_codes, n_embd)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "embd",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "embd = response_json[0][\"embedding\"]\nn_codes = len(embd)\nn_embd = len(embd[0])\nprint('spectrogram generated: n_codes: %d, n_embd: %d' % (n_codes, n_embd))\n# post-process the spectrogram to convert to audio\nprint('converting to audio ...')\naudio = embd_to_audio(embd, n_codes, n_embd)\nprint('audio generated: %d samples' % len(audio))\nfilename = \"output.wav\"\nsample_rate = 24000 # sampling rate",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "n_codes",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "n_codes = len(embd)\nn_embd = len(embd[0])\nprint('spectrogram generated: n_codes: %d, n_embd: %d' % (n_codes, n_embd))\n# post-process the spectrogram to convert to audio\nprint('converting to audio ...')\naudio = embd_to_audio(embd, n_codes, n_embd)\nprint('audio generated: %d samples' % len(audio))\nfilename = \"output.wav\"\nsample_rate = 24000 # sampling rate\n# zero out first 0.25 seconds",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "n_embd",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "n_embd = len(embd[0])\nprint('spectrogram generated: n_codes: %d, n_embd: %d' % (n_codes, n_embd))\n# post-process the spectrogram to convert to audio\nprint('converting to audio ...')\naudio = embd_to_audio(embd, n_codes, n_embd)\nprint('audio generated: %d samples' % len(audio))\nfilename = \"output.wav\"\nsample_rate = 24000 # sampling rate\n# zero out first 0.25 seconds\naudio[:24000 // 4] = 0.0",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "audio",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "audio = embd_to_audio(embd, n_codes, n_embd)\nprint('audio generated: %d samples' % len(audio))\nfilename = \"output.wav\"\nsample_rate = 24000 # sampling rate\n# zero out first 0.25 seconds\naudio[:24000 // 4] = 0.0\nsave_wav(filename, audio, sample_rate)\nprint('audio written to file \"%s\"' % filename)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "filename",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "filename = \"output.wav\"\nsample_rate = 24000 # sampling rate\n# zero out first 0.25 seconds\naudio[:24000 // 4] = 0.0\nsave_wav(filename, audio, sample_rate)\nprint('audio written to file \"%s\"' % filename)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "peekOfCode": "sample_rate = 24000 # sampling rate\n# zero out first 0.25 seconds\naudio[:24000 // 4] = 0.0\nsave_wav(filename, audio, sample_rate)\nprint('audio written to file \"%s\"' % filename)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.tools.tts.tts-outetts",
        "documentation": {}
    },
    {
        "label": "SentencePieceTokenTypes",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SentencePieceTokenTypes(IntEnum):\n    NORMAL = 1\n    UNKNOWN = 2\n    CONTROL = 3\n    USER_DEFINED = 4\n    UNUSED = 5\n    BYTE = 6\nclass ModelType(IntEnum):\n    TEXT = 1\n    MMPROJ = 2",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ModelType(IntEnum):\n    TEXT = 1\n    MMPROJ = 2\nAnyModel = TypeVar(\"AnyModel\", bound=\"type[ModelBase]\")\nclass ModelBase:\n    _model_classes: dict[ModelType, dict[str, type[ModelBase]]] = {\n        ModelType.TEXT: {},\n        ModelType.MMPROJ: {},\n    }\n    dir_model: Path",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ModelBase",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ModelBase:\n    _model_classes: dict[ModelType, dict[str, type[ModelBase]]] = {\n        ModelType.TEXT: {},\n        ModelType.MMPROJ: {},\n    }\n    dir_model: Path\n    ftype: gguf.LlamaFileType\n    fname_out: Path\n    is_big_endian: bool\n    endianess: gguf.GGUFEndian",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "TextModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class TextModel(ModelBase):\n    model_type = ModelType.TEXT\n    hf_arch: str\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if not self.is_mistral_format:\n            self.hf_arch = get_model_architecture(self.hparams, self.model_type)\n        else:\n            self.hf_arch = \"\"\n        if \"text_config\" in self.hparams:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MmprojModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MmprojModel(ModelBase):\n    model_type = ModelType.MMPROJ\n    model_arch = gguf.MODEL_ARCH.MMPROJ\n    preprocessor_config: dict[str, Any]\n    global_config: dict[str, Any]\n    n_block_keys = [\"n_layers\", \"num_hidden_layers\", \"n_layer\", \"num_layers\", \"depth\", \"encoder_layers\"]\n    has_vision_encoder: bool = True # by default\n    has_audio_encoder: bool = False\n    # for models having multiple encoders, we need to separate their hparams\n    hparams_vision: dict[str, Any] | None = None",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GPTNeoXModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GPTNeoXModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GPTNEOX\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_context_length(self.hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"intermediate_size\"])\n        self.gguf_writer.add_rope_dimension_count(\n            int(self.hparams[\"rotary_pct\"] * (self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])),\n        )",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BloomModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BloomModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BLOOM\n    def set_gguf_parameters(self):\n        n_embed = self.hparams.get(\"hidden_size\", self.hparams.get(\"n_embed\"))\n        n_head = self.hparams.get(\"n_head\", self.hparams.get(\"num_attention_heads\"))\n        self.gguf_writer.add_context_length(self.hparams.get(\"seq_length\", n_embed))\n        self.gguf_writer.add_embedding_length(n_embed)\n        self.gguf_writer.add_feed_forward_length(4 * n_embed)\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_head_count(n_head)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MPTModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MPTModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.MPT\n    def set_vocab(self):\n        try:\n            self._set_vocab_gpt2()\n        except Exception:\n            # Fallback for SEA-LION model\n            self._set_vocab_sentencepiece()\n            self.gguf_writer.add_add_bos_token(False)\n            self.gguf_writer.add_pad_token_id(3)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "OrionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class OrionModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.ORION\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n    def set_gguf_parameters(self):\n        head_count = self.hparams[\"num_attention_heads\"]\n        head_count_kv = self.hparams.get(\"num_key_value_heads\", head_count)\n        ctx_length = 0\n        if \"max_sequence_length\" in self.hparams:\n            ctx_length = self.hparams[\"max_sequence_length\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BaichuanModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BaichuanModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BAICHUAN\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_tensor_data_layout(\"Meta AI original pth\")\n        self.gguf_writer.add_rope_dimension_count(self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        head_count = self.hparams[\"num_attention_heads\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "XverseModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class XverseModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.XVERSE\n    def set_vocab(self):\n        assert (self.dir_model / \"tokenizer.json\").is_file()\n        dir_model = self.dir_model\n        hparams = self.hparams\n        tokens: list[bytes] = []\n        toktypes: list[int] = []\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(dir_model)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "FalconModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class FalconModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.FALCON\n    def set_gguf_parameters(self):\n        n_head = self.hparams.get(\"num_attention_heads\")\n        if n_head is None:\n            n_head = self.hparams[\"n_head\"]  # old name\n        n_head_kv = self.hparams.get(\"num_kv_heads\")\n        if n_head_kv is None:\n            n_head_kv = self.hparams.get(\"n_head_kv\", 1)  # old name\n        self.gguf_writer.add_context_length(2048)  # not in config.json",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "StarCoderModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class StarCoderModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.STARCODER\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_context_length(self.hparams[\"n_positions\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"n_embd\"])\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_head_count_kv(1)\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "RefactModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class RefactModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.REFACT\n    def set_vocab(self):\n        super().set_vocab()\n        # TODO: how to determine special FIM tokens automatically?\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=False,\n                                          special_token_types = ['prefix', 'suffix', 'middle', 'eot'])\n        special_vocab._set_special_token(\"prefix\", 1)\n        special_vocab._set_special_token(\"suffix\", 3)\n        special_vocab._set_special_token(\"middle\", 2)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "StableLMModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class StableLMModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.STABLELM\n    def set_vocab(self):\n        if (self.dir_model / \"tokenizer.json\").is_file():\n            self._set_vocab_gpt2()\n        else:\n            # StableLM 2 1.6B used to have a vocab in a similar format to Qwen's vocab\n            self._set_vocab_qwen()\n    def set_gguf_parameters(self):\n        hparams = self.hparams",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LlamaModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LlamaModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA\n    undo_permute = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # fix for SmolVLM2, missing `num_attention_heads` in config.json\n        if self.hf_arch == \"VLlama3ForCausalLM\":\n            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 32)\n        hparams = ModelBase.load_hparams(self.dir_model, is_mistral_format=False)\n        self.origin_hf_arch = hparams.get('architectures', [None])[0]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ArceeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ArceeModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.ARCEE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self._try_set_pooling_type()\n@ModelBase.register(\"AfmoeForCausalLM\")\nclass AfmoeModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.AFMOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "AfmoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class AfmoeModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.AFMOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        # MoE parameters\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (n_shared_experts := self.hparams.get(\"num_shared_experts\")) is not None:\n            self.gguf_writer.add_expert_shared_count(n_shared_experts)\n        if (moe_intermediate_size := self.hparams.get(\"moe_intermediate_size\")) is not None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LlavaVisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LlavaVisionModel(MmprojModel):\n    img_break_tok_id = -1\n    use_break_tok = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if self.hparams.get(\"model_type\") == \"pixtral\":\n            # layer_norm_eps is not in config.json, it is hard-coded in modeling_pixtral.py\n            self.hparams[\"layer_norm_eps\"] = self.hparams.get(\"layer_norm_eps\", 1e-5)\n            if self.use_break_tok:\n                self.img_break_tok_id = self.get_token_id(\"[IMG_BREAK]\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "SmolVLMModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SmolVLMModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if self.hparams[\"model_type\"] == \"smolvlm_vision\":\n            # fix for SmolVLM2, missing some keys in config.json\n            # default values are taken from transformers code\n            self.hparams[\"hidden_size\"] = self.hparams.get(\"hidden_size\", 1152)\n            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 16)\n            self.hparams[\"intermediate_size\"] = self.hparams.get(\"intermediate_size\", 3072)\n    def set_gguf_parameters(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Llama4Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Llama4Model(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA4\n    undo_permute = False\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # IMPORTANT: the normal \"intermediate_size\" is renamed to \"intermediate_size_mlp\", we need to undo this\n        self.hparams[\"intermediate_size_moe\"] = self.hparams[\"intermediate_size\"]\n        self.hparams[\"intermediate_size\"] = self.hparams[\"intermediate_size_mlp\"]\n    def set_vocab(self):\n        self._set_vocab_gpt2()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Llama4VisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Llama4VisionModel(MmprojModel):\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.LLAMA4)\n        self.gguf_writer.add_vision_attention_layernorm_eps(self.hparams[\"norm_eps\"])\n        self.gguf_writer.add_vision_projector_scale_factor(int(1.0 / self.hparams[\"pixel_shuffle_ratio\"]))\n        assert self.hparams[\"hidden_act\"] == \"gelu\"\n        self.gguf_writer.add_vision_use_gelu(True)\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid # unused",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Mistral3Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Mistral3Model(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.MISTRAL3\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # for compatibility, we use LLAMA arch for older models\n        # TODO: remove this once everyone has migrated to newer version of llama.cpp\n        if self.hparams.get(\"model_type\") != \"ministral3\":\n            self.model_arch = gguf.MODEL_ARCH.LLAMA\n            self.gguf_writer.arch = gguf.MODEL_ARCH_NAMES[self.model_arch]\n            self.gguf_writer.add_architecture()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DeciModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DeciModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.DECI\n    @staticmethod\n    def _ffn_mult_to_intermediate_size(ffn_mult: float, n_embd: int) -> int:\n        # DeciLM-specific code\n        intermediate_size = int(2 * ffn_mult * n_embd / 3)\n        return DeciModel._find_multiple(intermediate_size, 256)\n    @staticmethod\n    def _find_multiple(n: int, k: int) -> int:\n        # DeciLM-specific code",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BitnetModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BitnetModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BITNET\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n        self.gguf_writer.add_rope_scaling_factor(1.0)\n    def weight_quant(self, weight: Tensor) -> Tensor:\n        dtype = weight.dtype",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GrokModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GrokModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GROK\n    def set_vocab(self):\n        if (self.dir_model / 'tokenizer.model').is_file():\n            self._set_vocab_sentencepiece()\n            return\n        if not (self.dir_model / 'tokenizer.json').is_file() or not (self.dir_model / 'chat_template.jinja').is_file():\n            logger.error('Error: Missing vocab and chat template, download files from https://huggingface.co/alvarobartt/grok-2-tokenizer')\n            sys.exit(1)\n        self._set_vocab_gpt2()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DbrxModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DbrxModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.DBRX\n    def set_gguf_parameters(self):\n        ffn_config = self.hparams[\"ffn_config\"]\n        attn_config = self.hparams[\"attn_config\"]\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_context_length(self.hparams[\"max_seq_len\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"d_model\"])\n        self.gguf_writer.add_feed_forward_length(ffn_config[\"ffn_hidden_size\"])\n        self.gguf_writer.add_head_count(self.hparams[\"n_heads\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MiniCPMModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MiniCPMModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.MINICPM\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        embedding_scale = float(self.hparams[\"scale_emb\"])\n        self.gguf_writer.add_embedding_scale(embedding_scale)\n        logger.info(f\"gguf: (minicpm) embedding_scale = {embedding_scale}\")\n        residual_scale = self.hparams[\"scale_depth\"] / self.hparams[\"num_hidden_layers\"] ** 0.5\n        self.gguf_writer.add_residual_scale(residual_scale)\n        logger.info(f\"gguf: (minicpm) residual_scale = {residual_scale}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MiniCPM3Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MiniCPM3Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.MINICPM3\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        self.gguf_writer.add_file_type(self.ftype)\n        self.gguf_writer.add_context_length(hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_feed_forward_length(hparams[\"intermediate_size\"])\n        self.gguf_writer.add_head_count(hparams[\"num_attention_heads\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "QwenModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class QwenModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.QWEN\n    @staticmethod\n    def token_bytes_to_string(b):\n        from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n        byte_encoder = bytes_to_unicode()\n        return ''.join([byte_encoder[ord(char)] for char in b.decode('latin-1')])\n    @staticmethod\n    def bpe(mergeable_ranks: dict[bytes, int], token: bytes, max_rank: int | None = None) -> list[bytes]:\n        parts = [bytes([b]) for b in token]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.QWEN2\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self._try_set_pooling_type()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DreamModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DreamModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.DREAM\n    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n        tokens: list[str] = []\n        toktypes: list[int] = []\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n        vocab_dict = tokenizer.get_vocab()\n        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))\n        assert max(vocab_dict.values()) < vocab_size",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LLaDAModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LLaDAModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.LLADA\n    undo_permute = True\n    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n        tokens: list[str] = []\n        toktypes: list[int] = []\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n        vocab_dict = tokenizer.get_vocab()\n        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Ernie4_5Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Ernie4_5Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.ERNIE4_5\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        num_heads = self.hparams[\"num_attention_heads\"]\n        num_kv_heads = self.hparams[\"num_key_value_heads\"]\n        if (head_dim := self.hparams.get(\"head_dim\")) is None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Ernie4_5MoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Ernie4_5MoeModel(Ernie4_5Model):\n    model_arch = gguf.MODEL_ARCH.ERNIE4_5_MOE\n    _experts: list[dict[str, Tensor]] | None = None\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._experts = [{} for _ in range(self.block_count)]\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_count(self.hparams[\"moe_num_experts\"])\n        self.gguf_writer.add_expert_used_count(self.hparams[\"moe_k\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen2VLModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen2VLModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.QWEN2VL\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen2VLVisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen2VLVisionModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        self.hparams_vision[\"image_size\"] = self.hparams_vision.get(\"image_size\", 560)\n        # rename config.json values\n        self.hparams_vision[\"num_attention_heads\"] = self.hparams_vision.get(\"num_heads\")\n        self.hparams_vision[\"num_hidden_layers\"] = self.hparams_vision.get(\"depth\")\n        if \"embed_dim\" in self.hparams_vision: # qwen2vl\n            self.hparams_vision[\"intermediate_size\"] = self.hparams_vision.get(\"hidden_size\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen25OmniModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen25OmniModel(Qwen2VLVisionModel):\n    has_vision_encoder = True\n    has_audio_encoder = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_audio is not None\n        self.hparams_audio[\"hidden_size\"] = self.hparams_audio[\"d_model\"]\n        self.hparams_audio[\"intermediate_size\"] = self.hparams_audio[\"encoder_ffn_dim\"]\n        self.hparams_audio[\"num_attention_heads\"] = self.hparams_audio[\"encoder_attention_heads\"]\n    def set_gguf_parameters(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "InternVisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class InternVisionModel(MmprojModel):\n    def set_gguf_parameters(self):\n        assert self.hparams_vision is not None\n        if isinstance(self.hparams_vision['image_size'], list):\n            self.hparams_vision['image_size'] = self.hparams_vision['image_size'][0]\n        if isinstance(self.hparams_vision['patch_size'], list):\n            self.hparams_vision['patch_size'] = self.hparams_vision['patch_size'][0]\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.INTERNVL)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "WavTokenizerDecModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class WavTokenizerDecModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.WAVTOKENIZER_DEC\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n        if \\\n                name.endswith(\"codebook.cluster_size\") or \\\n                name.endswith(\"codebook.embed_avg\") or \\\n                name.endswith(\"codebook.inited\"):\n            logger.debug(f\"Skipping {name!r}\")\n            return []",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen2MoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen2MoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.QWEN2MOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (moe_intermediate_size := self.hparams.get(\"moe_intermediate_size\")) is not None:\n            self.gguf_writer.add_expert_feed_forward_length(moe_intermediate_size)\n            logger.info(f\"gguf: expert feed forward length = {moe_intermediate_size}\")\n        if (shared_expert_intermediate_size := self.hparams.get('shared_expert_intermediate_size')) is not None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3Model(Qwen2Model):\n    model_arch = gguf.MODEL_ARCH.QWEN3\n    # extra logic for rerank models\n    is_rerank: bool = False\n    is_tied_embeddings: bool = False\n    token_false_id: int | None = None\n    token_true_id: int | None = None\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # track for intern-s1-mini",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3MoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3MoeModel(Qwen2MoeModel):\n    model_arch = gguf.MODEL_ARCH.QWEN3MOE\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        hparams = ModelBase.load_hparams(self.dir_model, False)\n        self.origin_hf_arch = hparams.get('architectures', [None])[0]\n    def set_vocab(self):\n        # deal with intern-s1\n        if self.origin_hf_arch == 'InternS1ForConditionalGeneration':\n            self._set_vocab_interns1()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3NextModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3NextModel(Qwen2MoeModel):\n    model_arch = gguf.MODEL_ARCH.QWEN3NEXT\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_ssm_conv_kernel(self.hparams[\"linear_conv_kernel_dim\"])\n        self.gguf_writer.add_ssm_state_size(self.hparams[\"linear_key_head_dim\"])\n        self.gguf_writer.add_ssm_group_count(self.hparams[\"linear_num_key_heads\"])\n        self.gguf_writer.add_ssm_time_step_rank(self.hparams[\"linear_num_value_heads\"])\n        self.gguf_writer.add_ssm_inner_size(self.hparams[\"linear_value_head_dim\"] * self.hparams[\"linear_num_value_heads\"])\n        if (rope_dim := self.hparams.get(\"head_dim\")) is None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "RND1Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class RND1Model(Qwen2MoeModel):\n    model_arch = gguf.MODEL_ARCH.RND1\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        # RND1 specific parameters\n        # RND1 uses bidirectional attention\n        self.gguf_writer.add_causal_attention(False)\n        if (mask_token_id := self.hparams.get(\"mask_token_id\")) is not None:\n            self.gguf_writer.add_mask_token_id(mask_token_id)\n@ModelBase.register(\"Qwen3VLForConditionalGeneration\", \"Qwen3VLMoeForConditionalGeneration\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3VLVisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3VLVisionModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        # Compute image_size if not present\n        if \"image_size\" not in self.hparams_vision:\n            # For Qwen3VL/Qwen3VLMoe, compute from num_position_embeddings\n            num_pos = self.hparams_vision.get(\"num_position_embeddings\", 2304)\n            patch_size = self.hparams_vision.get(\"patch_size\", 16)\n            # num_position_embeddings = (image_size / patch_size) ** 2",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Glm4VVisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Glm4VVisionModel(Qwen3VLVisionModel):\n    def set_gguf_parameters(self):\n        MmprojModel.set_gguf_parameters(self) # skip Qwen3VLVisionModel parameters\n        assert self.hparams_vision is not None\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.GLM4V)\n        hidden_act = str(self.hparams_vision.get(\"hidden_act\", \"\")).lower()\n        if hidden_act == \"gelu\":\n            self.gguf_writer.add_vision_use_gelu(True)\n        elif hidden_act == \"silu\":\n            self.gguf_writer.add_vision_use_silu(True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3VLTextModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3VLTextModel(Qwen3Model):\n    model_arch = gguf.MODEL_ARCH.QWEN3VL\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        # Handle MRoPE (Multi-axis Rotary Position Embedding) for Qwen3-VL\n        vision_config = self.hparams.get(\"vision_config\", {})\n        deepstack_layer_num = len(vision_config.get(\"deepstack_visual_indexes\", []))\n        self.gguf_writer.add_num_deepstack_layers(deepstack_layer_num)\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # Skip vision tensors - they go in the mmproj file",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3VLMoeTextModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3VLMoeTextModel(Qwen3MoeModel):\n    model_arch = gguf.MODEL_ARCH.QWEN3VLMOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        vision_config = self.hparams.get(\"vision_config\", {})\n        deepstack_layer_num = len(vision_config.get(\"deepstack_visual_indexes\", []))\n        self.gguf_writer.add_num_deepstack_layers(deepstack_layer_num)\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # Skip vision tensors - they go in the mmproj file\n        if name.startswith(\"model.visual.\"):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GPT2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GPT2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.GPT2\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_context_length(self.hparams[\"n_ctx\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"n_embd\"])\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Phi2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Phi2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.PHI2\n    def set_gguf_parameters(self):\n        rot_pct = self.find_hparam([\"partial_rotary_factor\"])\n        n_embd = self.find_hparam([\"hidden_size\", \"n_embd\"])\n        n_head = self.find_hparam([\"num_attention_heads\", \"n_head\"])\n        self.gguf_writer.add_context_length(self.find_hparam([\"n_positions\", \"max_position_embeddings\"]))\n        self.gguf_writer.add_embedding_length(n_embd)\n        self.gguf_writer.add_feed_forward_length(4 * n_embd)\n        self.gguf_writer.add_block_count(self.block_count)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Phi3MiniModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Phi3MiniModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.PHI3\n    def set_vocab(self):\n        # Phi-4 model uses GPT2Tokenizer\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n                tokenizer_class = tokenizer_config_json['tokenizer_class']\n                if tokenizer_class == 'GPT2Tokenizer':",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "PhiMoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class PhiMoeModel(Phi3MiniModel):\n    model_arch = gguf.MODEL_ARCH.PHIMOE\n    _experts: list[dict[str, Tensor]] | None = None\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_used_count(self.hparams[\"num_experts_per_tok\"])\n        self.gguf_writer.add_expert_count(self.hparams[\"num_local_experts\"])\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # process the experts separately\n        if name.find(\"block_sparse_moe.experts\") != -1:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "PlamoModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class PlamoModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.PLAMO\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        self.gguf_writer.add_context_length(4096)  # not in config.json\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_feed_forward_length(hparams[\"intermediate_size\"])\n        self.gguf_writer.add_block_count(self.block_count)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Plamo2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Plamo2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.PLAMO2\n    def set_vocab(self):\n        self._set_vocab_plamo()\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(self.hparams[\"vocab_size\"])\n        # Which layers are Mamba layers\n        # PLaMo 2 uses mamba_step to indicate the pattern (e.g., 2 means every other layer)\n        # This logic matches modeling_plamo.py's is_mamba function",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Plamo3Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Plamo3Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.PLAMO3\n    def set_vocab(self):\n        self._set_vocab_plamo()\n        tokenizer_config_path = self.dir_model / \"tokenizer_config.json\"\n        tokenizer_config = {}\n        if tokenizer_config_path.is_file():\n            with open(tokenizer_config_path, encoding=\"utf-8\") as f:\n                tokenizer_config = json.load(f)\n        chat_template = tokenizer_config.get(\"chat_template\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "CodeShellModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class CodeShellModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.CODESHELL\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_context_length(self.hparams[\"n_positions\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"n_embd\"])\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_head_count_kv(self.hparams[\"num_query_groups\"])\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "InternLM2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class InternLM2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.INTERNLM2\n    def set_vocab(self):\n        # (TODO): Is there a better way?\n        # Copy from _set_vocab_sentencepiece, The only difference is that we will treat the character\n        # \\x00 specially and convert it into an emoji character to prevent it from being mistakenly\n        # recognized as an empty string in C++.\n        from sentencepiece import SentencePieceProcessor\n        from sentencepiece import sentencepiece_model_pb2 as model\n        tokenizer_path = self.dir_model / 'tokenizer.model'",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "InternLM3Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class InternLM3Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA\n    def set_vocab(self):\n        tokens, scores, toktypes = self._create_vocab_sentencepiece()\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BertModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BertModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.vocab_size = None\n        if cls_out_labels := self.hparams.get(\"id2label\"):\n            if len(cls_out_labels) == 2 and cls_out_labels[0] == \"LABEL_0\":\n                # Remove dummy labels added by AutoConfig\n                cls_out_labels = None\n        self.cls_out_labels = cls_out_labels",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DistilBertModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DistilBertModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_layer_norm_eps(1e-12)\n        logger.info(\"gguf: layer norm epsilon = 1e-12\")\n        super().set_gguf_parameters()\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        if name.startswith(\"distilbert.\"):\n            name = name[11:]\n        # These layers act as MLM head, so we don't need them",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class RobertaModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # we need the pad_token_id to know how to chop down position_embd matrix\n        if (pad_token_id := self.hparams.get(\"pad_token_id\")) is not None:\n            self._position_offset = 1 + pad_token_id\n            if \"max_position_embeddings\" in self.hparams:\n                self.hparams[\"max_position_embeddings\"] -= self._position_offset\n        else:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "NomicBertModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class NomicBertModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n    def __init__(self, dir_model: Path, ftype: gguf.LlamaFileType, fname_out: Path, **kwargs: Any):\n        hparams = kwargs.pop(\"hparams\", None)\n        if hparams is None:\n            hparams = ModelBase.load_hparams(dir_model, False)\n        self.is_moe = bool(hparams.get(\"moe_every_n_layers\"))\n        self.model_arch = gguf.MODEL_ARCH.NOMIC_BERT_MOE if self.is_moe else gguf.MODEL_ARCH.NOMIC_BERT\n        super().__init__(dir_model, ftype, fname_out, hparams=hparams, **kwargs)\n        self._tokenizer_is_xlmroberta = self._is_tokenizer_xlmroberta()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "NeoBert",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class NeoBert(BertModel):\n    model_arch = gguf.MODEL_ARCH.NEO_BERT\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        # NeoBERT uses 2/3 of the intermediate size as feed forward length\n        self.gguf_writer.add_feed_forward_length(int(2 * self.hparams[\"intermediate_size\"] / 3))\n        self.gguf_writer.add_rope_freq_base(10000.0)  # default value for NeoBERT\n        self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.NONE)\n        f_rms_eps = self.hparams.get(\"norm_eps\", 1e-6)  # default value for NeoBERT\n        self.gguf_writer.add_layer_norm_rms_eps(f_rms_eps)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "XLMRobertaModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class XLMRobertaModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n    _lora_files = {}\n    _lora_names = []\n    def __init__(self, dir_model: Path, ftype: gguf.LlamaFileType, fname_out: Path, **kwargs: Any):\n        hparams = kwargs.pop(\"hparams\", None)\n        if hparams is None:\n            hparams = ModelBase.load_hparams(dir_model, False)\n        if lora_names := hparams.get(\"lora_adaptations\"):\n            self._lora_names = lora_names",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GemmaModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GemmaModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GEMMA\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n        # TODO: these special tokens should be exported only for the CodeGemma family\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=False,\n                                          special_token_types = ['prefix', 'suffix', 'middle', 'fsep', 'eot'])\n        special_vocab._set_special_token(\"prefix\", 67)\n        special_vocab._set_special_token(\"suffix\", 69)\n        special_vocab._set_special_token(\"middle\", 68)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Gemma2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Gemma2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.GEMMA2\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n        self.gguf_writer.add_add_space_prefix(False)\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        self.gguf_writer.add_context_length(hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(self.block_count)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Gemma3Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Gemma3Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.GEMMA3\n    norm_shift = 1.0  # Gemma3RMSNorm adds 1.0 to the norm value\n    def set_vocab(self):\n        if (self.dir_model / \"tokenizer.model\").is_file():\n            self._set_vocab_sentencepiece()\n            self.gguf_writer.add_add_space_prefix(False)\n        else:\n            self._set_vocab_gpt2()\n    def set_gguf_parameters(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "EmbeddingGemma",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class EmbeddingGemma(Gemma3Model):\n    model_arch = gguf.MODEL_ARCH.GEMMA_EMBEDDING\n    module_paths = []\n    dense_features_dims = {}\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if self.sentence_transformers_dense_modules:\n            # read modules.json to determine if model has Dense layers\n            modules_file = self.dir_model / \"modules.json\"\n            if modules_file.is_file():",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Gemma3VisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Gemma3VisionModel(MmprojModel):\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.GEMMA3)\n        # default values below are taken from HF tranformers code\n        self.gguf_writer.add_vision_attention_layernorm_eps(hparams.get(\"layer_norm_eps\", 1e-6))\n        self.gguf_writer.add_vision_use_gelu(True)\n        # calculate proj_scale_factor (used by tinygemma3 test model)\n        image_seq_length = self.preprocessor_config.get(\"image_seq_length\", 256)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ConformerAudioModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ConformerAudioModel(MmprojModel):\n    _batch_norm_tensors: list[dict[str, Tensor]] | None = None\n    @staticmethod\n    def is_audio_tensor(name: str):\n        return any(p in name for p in [\"audio\", \"codebook\", \"conformer\", \"depth_embedding\", \"depthformer\", \"depth_linear\"])\n    def tensor_force_quant(self, name, new_name, bid, n_dims):\n        if ConformerAudioModel.is_audio_tensor(name):\n            if \".conv\" in name or \"_conv\" in name and \".weight\" in name:\n                return gguf.GGMLQuantizationType.F32\n        return super().tensor_force_quant(name, new_name, bid, n_dims)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Gemma3nVisionAudioModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Gemma3nVisionAudioModel(ConformerAudioModel):\n    has_audio_encoder = True\n    has_vision_encoder = True\n    # Double indexed mapping for MobileNetV5 blocks (not supported by tensor_mapping.py)\n    # This is the only known model having this, so we prefer implementing it outside of tensor_mapping.py\n    block_tensor_mapping = {\n        \"model.vision_tower.timm_model.blocks.{bid}.{sid}.conv_exp.weight\":             \"v.blk.{bid}.{sid}.conv_exp.weight\",\n        \"model.vision_tower.timm_model.blocks.{bid}.{sid}.bn1.weight\":                  \"v.blk.{bid}.{sid}.bn1.weight\",\n        \"model.vision_tower.timm_model.blocks.{bid}.{sid}.conv_pwl.weight\":             \"v.blk.{bid}.{sid}.conv_pwl.weight\",\n        \"model.vision_tower.timm_model.blocks.{bid}.{sid}.bn2.weight\":                  \"v.blk.{bid}.{sid}.bn2.weight\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Gemma3NModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Gemma3NModel(Gemma3Model):\n    model_arch = gguf.MODEL_ARCH.GEMMA3N\n    norm_shift = 0.0 # same value with Gemma3p5RMSNorm scale_shift on python code\n    _altup_proj: list[Tensor] = []\n    _altup_unembd: list[Tensor] = []\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams[\"altup_num_inputs\"] == 4, \"Current conversion only supports 4 altup inputs\"\n        self._altup_proj = [\n            torch.Tensor(), # to be replaced",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "StarCoder2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class StarCoder2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.STARCODER2\n@ModelBase.register(\"Rwkv6ForCausalLM\")\nclass Rwkv6Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.RWKV6\n    def set_vocab(self):\n        self._set_vocab_rwkv_world()\n    def set_gguf_parameters(self):\n        head_size = self.hparams[\"head_size\"]\n        hidden_size = self.hparams[\"hidden_size\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Rwkv6Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Rwkv6Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.RWKV6\n    def set_vocab(self):\n        self._set_vocab_rwkv_world()\n    def set_gguf_parameters(self):\n        head_size = self.hparams[\"head_size\"]\n        hidden_size = self.hparams[\"hidden_size\"]\n        layer_norm_eps = self.hparams[\"layer_norm_epsilon\"]\n        rescale_every_n_layers = self.hparams[\"rescale_every\"]\n        intermediate_size = self.hparams[\"intermediate_size\"] if self.hparams[\"intermediate_size\"] is not None else int((hidden_size * 3.5) // 32 * 32)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "RWKV6Qwen2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class RWKV6Qwen2Model(Rwkv6Model):\n    model_arch = gguf.MODEL_ARCH.RWKV6QWEN2\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        num_attention_heads = self.hparams[\"num_attention_heads\"]\n        num_key_value_heads = self.hparams[\"num_key_value_heads\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Rwkv7Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Rwkv7Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.RWKV7\n    def set_vocab(self):\n        self._set_vocab_rwkv_world()\n    def calc_lora_rank(self, hidden_size, exponent, multiplier):\n        return max(1, round(hidden_size ** exponent * multiplier / 32)) * 32\n    def set_gguf_parameters(self):\n        try:\n            head_size = self.hparams[\"head_size\"]\n            layer_norm_eps = self.hparams[\"layer_norm_epsilon\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ARwkv7Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ARwkv7Model(Rwkv7Model):\n    model_arch = gguf.MODEL_ARCH.ARWKV7\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        hidden_size = self.hparams[\"hidden_size\"]\n        head_size = self.hparams[\"head_size\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MaincoderModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MaincoderModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.MAINCODER\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (head_dim := self.hparams.get(\"head_dim\")) is not None:\n            self.gguf_writer.add_rope_dimension_count(head_dim)\n@ModelBase.register(\"MambaForCausalLM\", \"MambaLMHeadModel\", \"FalconMambaForCausalLM\")\nclass MambaModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.MAMBA\n    def __init__(self, dir_model: Path, *args, **kwargs):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MambaModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MambaModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.MAMBA\n    def __init__(self, dir_model: Path, *args, **kwargs):\n        # Avoid using AutoConfig for hparams\n        hparams = kwargs.pop(\"hparams\", None)\n        if hparams is None:\n            with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\n                hparams = json.load(f)\n        super().__init__(dir_model, *args, hparams=hparams, **kwargs)\n    def set_vocab(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Mamba2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Mamba2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.MAMBA2\n    def __init__(self, dir_model: Path, *args, **kwargs):\n        # Avoid using AutoConfig for hparams\n        # It wrongly assumes all Mamba2 models are Mamba-Codestral-7B-v0.1\n        hparams = kwargs.pop(\"hparams\", None)\n        if hparams is None:\n            with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\n                hparams = json.load(f)\n        super().__init__(dir_model, *args, hparams=hparams, **kwargs)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "JambaModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class JambaModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.JAMBA\n    def set_vocab(self):\n        if (self.dir_model / \"tokenizer.model\").is_file():\n            self._set_vocab_sentencepiece()\n        else:\n            self._set_vocab_llama_hf()\n            self.gguf_writer.add_add_space_prefix(False)\n    def set_gguf_parameters(self):\n        d_model = self.find_hparam([\"hidden_size\", \"mamba_d_model\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "CommandR2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class CommandR2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.COMMAND_R\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # max_position_embeddings = 8192 in config.json but model was actually\n        # trained on 128k context length\n        # aya-23 models don't have model_max_length specified\n        self.hparams[\"max_position_embeddings\"] = self.find_hparam([\"model_max_length\", \"max_position_embeddings\"])\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Cohere2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Cohere2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.COHERE2\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_logit_scale(self.hparams[\"logit_scale\"])\n        self.gguf_writer.add_sliding_window(self.hparams[\"sliding_window\"])\n        self.gguf_writer.add_vocab_size(self.hparams[\"vocab_size\"])\n        rotary_pct = self.hparams[\"rotary_pct\"]\n        hidden_size = self.hparams[\"hidden_size\"]\n        num_attention_heads = self.hparams[\"num_attention_heads\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "OlmoModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class OlmoModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.OLMO\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_layer_norm_eps(1e-5)\n        clip_qkv = self.hparams.get(\"clip_qkv\")\n        if clip_qkv is not None:\n            self.gguf_writer.add_clamp_kqv(clip_qkv)\n    # Same as super class, but permuting q_proj, k_proj\n    # Copied from: LlamaModel",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "SeedOssModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SeedOssModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.SEED_OSS\n@ModelBase.register(\"Olmo2ForCausalLM\")\n@ModelBase.register(\"Olmo3ForCausalLM\")\nclass Olmo2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.OLMO2\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if \"sliding_window\" in self.hparams:\n            self.gguf_writer.add_sliding_window(self.hparams[\"sliding_window\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Olmo2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Olmo2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.OLMO2\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if \"sliding_window\" in self.hparams:\n            self.gguf_writer.add_sliding_window(self.hparams[\"sliding_window\"])\n            sliding_window_pattern = []\n            if \"layer_types\" in self.hparams:\n                sliding_window_pattern = [t == \"sliding_attention\" for t in self.hparams[\"layer_types\"]]\n            else:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "OlmoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class OlmoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.OLMOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_layer_norm_rms_eps(1e-5)\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n    _experts: list[dict[str, Tensor]] | None = None\n    # Copied from: Qwen2MoeModel\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "JinaBertV2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class JinaBertV2Model(BertModel):\n    model_arch = gguf.MODEL_ARCH.JINA_BERT_V2\n    def set_vocab(self):\n        tokenizer_class = 'BertTokenizer'\n        with open(self.dir_model / \"tokenizer_config.json\", \"r\", encoding=\"utf-8\") as f:\n            tokenizer_class = json.load(f)['tokenizer_class']\n        if tokenizer_class == 'BertTokenizer':\n            super().set_vocab()\n        elif tokenizer_class == 'RobertaTokenizer':\n            self._set_vocab_gpt2()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "OpenELMModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class OpenELMModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.OPENELM\n    @staticmethod\n    def _make_divisible(v: float | int, divisor: int) -> int:\n        # ref: https://huggingface.co/apple/OpenELM-270M-Instruct/blob/eb111ff2e6724348e5b905984063d4064d4bc579/configuration_openelm.py#L34-L38\n        new_v = max(divisor, int(v + divisor / 2) // divisor * divisor)\n        # Make sure that round down does not go down by more than 10%.\n        if new_v < 0.9 * v:\n            new_v += divisor\n        return new_v",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ArcticModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ArcticModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.ARCTIC\n    def set_vocab(self):\n        # The reason for using a custom implementation here is that the\n        # snowflake-arctic-instruct model redefined tokens 31998 and 31999 from\n        # tokenizer.model and used them as BOS and EOS instead of adding new tokens.\n        from sentencepiece import SentencePieceProcessor\n        tokenizer_path = self.dir_model / 'tokenizer.model'\n        if not tokenizer_path.is_file():\n            logger.error(f'Error: Missing {tokenizer_path}')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DeepseekModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DeepseekModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.DEEPSEEK\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DeepseekV2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DeepseekV2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.DEEPSEEK2\n    def set_vocab(self):\n        try:\n            self._set_vocab_gpt2()\n            return\n        except Exception:\n            pass\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MiniMaxM2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MiniMaxM2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.MINIMAXM2\n    _experts_cache: dict[int, dict[str, Tensor]] = {}\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.hparams[\"num_experts\"] = self.hparams[\"num_local_experts\"]\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_feed_forward_length(self.find_hparam([\"intermediate_size\"]))\n        self.gguf_writer.add_rope_dimension_count(self.find_hparam([\"rotary_dim\"]))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MimoV2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MimoV2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.MIMO2\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        assert self.hparams[\"swa_head_dim\"] == self.hparams[\"head_dim\"]\n        assert self.hparams[\"swa_num_attention_heads\"] == self.hparams[\"num_attention_heads\"]\n        assert self.hparams[\"swa_v_head_dim\"] == self.hparams[\"v_head_dim\"]\n        assert self.hparams[\"topk_method\"] == \"noaux_tc\"\n        n_head_kv = self.hparams[\"num_key_value_heads\"]\n        n_head_kv_swa = self.hparams[\"swa_num_key_value_heads\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "PanguEmbeddedModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class PanguEmbeddedModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.PANGU_EMBED\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n                if \"add_prefix_space\" in tokenizer_config_json:\n                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Dots1Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Dots1Model(Qwen2MoeModel):\n    model_arch = gguf.MODEL_ARCH.DOTS1\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.hparams[\"num_experts\"] = self.hparams[\"n_routed_experts\"]\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_leading_dense_block_count(self.hparams[\"first_k_dense_replace\"])\n        self.gguf_writer.add_expert_shared_count(self.hparams[\"n_shared_experts\"])\n        self.gguf_writer.add_expert_weights_scale(self.hparams[\"routed_scaling_factor\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "PLMModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class PLMModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.PLM\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n        self.gguf_writer.add_kv_lora_rank(hparams[\"kv_lora_rank\"])\n        self.gguf_writer.add_key_length(hparams[\"qk_nope_head_dim\"] + hparams[\"qk_rope_head_dim\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "T5Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class T5Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.T5\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.shared_token_embeddings_found = False\n    def set_vocab(self):\n        # to avoid TypeError: Descriptors cannot be created directly\n        # exception when importing sentencepiece_model_pb2\n        os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n        from sentencepiece import SentencePieceProcessor",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "T5EncoderModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class T5EncoderModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.T5ENCODER\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.shared_token_embeddings_found = False\n    def set_vocab(self):\n        # to avoid TypeError: Descriptors cannot be created directly\n        # exception when importing sentencepiece_model_pb2\n        os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n        from sentencepiece import SentencePieceProcessor",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "JaisModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class JaisModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.JAIS\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # SwigLU activation\n        assert self.hparams[\"activation_function\"] == \"swiglu\"\n        # ALiBi position embedding\n        assert self.hparams[\"position_embedding_type\"] == \"alibi\"\n        # Embeddings scale\n        self.embeddings_scale = 1.0",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Glm4Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Glm4Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.GLM4\n    use_mrope = False\n    partial_rotary_factor = 0.5\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.partial_rotary_factor = self.rope_parameters.get(\"partial_rotary_factor\", 0.5)\n        if \"mrope_section\" in self.rope_parameters:\n            self.use_mrope = True\n            logger.info(\"Q/K weight will need to be permuted for M-RoPE\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Glm4MoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Glm4MoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GLM4_MOE\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # GLM4_MOE has num_hidden_layers + 1 actual layers (including NextN layer)\n        self.block_count = self.hparams[\"num_hidden_layers\"] + self.hparams.get(\"num_nextn_predict_layers\", 0)\n        self.tensor_map = gguf.get_tensor_name_map(self.model_arch, self.block_count)\n    def set_vocab(self):\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ChatGLMModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ChatGLMModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.CHATGLM\n    def set_vocab_chatglm3(self):\n        dir_model = self.dir_model\n        hparams = self.hparams\n        tokens: list[bytes] = []\n        toktypes: list[int] = []\n        scores: list[float] = []\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(dir_model, trust_remote_code=True)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "NemotronModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class NemotronModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.NEMOTRON\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n        self.gguf_writer.add_pad_token_id(0)\n        self.gguf_writer.add_unk_token_id(1)\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ExaoneModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ExaoneModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.EXAONE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        assert (hparams[\"activation_function\"] == \"silu\")\n        rotary_factor = self.find_hparam([\"partial_rotary_factor\", \"rope_pct\"], optional=True)\n        rotary_factor = rotary_factor if rotary_factor is not None else 1.0\n        self.gguf_writer.add_rope_dimension_count(int(rotary_factor * (hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"])))\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Exaone4Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Exaone4Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.EXAONE4\n    def set_vocab(self):\n        tokens, toktypes, tokpre = self.get_vocab_base()\n        self.gguf_writer.add_tokenizer_model(\"gpt2\")\n        self.gguf_writer.add_tokenizer_pre(tokpre)\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_types(toktypes)\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=True)\n        special_vocab.add_to_gguf(self.gguf_writer)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ExaoneMoEModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ExaoneMoEModel(Exaone4Model):\n    model_arch = gguf.MODEL_ARCH.EXAONE_MOE\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.block_count = self.hparams[\"num_hidden_layers\"] + self.hparams.get(\"num_nextn_predict_layers\", 0)\n        self.tensor_map = gguf.get_tensor_name_map(self.model_arch, self.block_count)\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_count(self.hparams[\"num_experts\"])\n        moe_intermediate_size = self.hparams[\"moe_intermediate_size\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GraniteModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GraniteModel(LlamaModel):\n    \"\"\"Conversion for IBM's GraniteForCausalLM\"\"\"\n    model_arch = gguf.MODEL_ARCH.GRANITE\n    def set_gguf_parameters(self):\n        \"\"\"Granite uses standard llama parameters with the following differences:\n        - No head_dim support\n        - New multiplier params:\n            - attention_scale\n            - embedding_scale\n            - residual_scale",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GraniteMoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GraniteMoeModel(GraniteModel):\n    \"\"\"Conversion for IBM's GraniteMoeForCausalLM\"\"\"\n    model_arch = gguf.MODEL_ARCH.GRANITE_MOE\n    def set_gguf_parameters(self):\n        \"\"\"GraniteMoeShared uses GraniteMoe parameters plus the following:\n        - shared_intermediate_size\n        \"\"\"\n        super().set_gguf_parameters()\n        if shared_feed_forward_length := self.hparams.get(\"shared_intermediate_size\"):\n            self.gguf_writer.add_expert_shared_feed_forward_length(shared_feed_forward_length)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GraniteHybridModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GraniteHybridModel(Mamba2Model, GraniteMoeModel):\n    \"\"\"GraniteHybrid is a hybrid SSM + Attention model that uses Mamba2 SSM\n    layers and optionally uses MoE w/ a shared expert\"\"\"\n    model_arch = gguf.MODEL_ARCH.GRANITE_HYBRID\n    undo_permute = True\n    def __init__(self, *args, **kwargs):\n        # Hybrid mamba models use a prefix for the mamba-specific params.\n        # TODO: Extend this if the prefix(es) need to be configurable\n        self.hparam_prefixes = [\"mamba\"]\n        super().__init__(*args, **kwargs)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "NemotronHModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class NemotronHModel(GraniteHybridModel):\n    \"\"\"Hybrid mamba2/attention model from NVIDIA\"\"\"\n    model_arch = gguf.MODEL_ARCH.NEMOTRON_H\n    is_moe: bool = False\n    def __init__(self, *args, **kwargs):\n        # We have to determine the correct model architecture (MoE vs non-MoE) before\n        # calling the parent __init__. This is because the parent constructor\n        # uses self.model_arch to build the tensor name map, and all MoE-specific\n        # mappings would be missed if it were called with the default non-MoE arch.\n        hparams = ModelBase.load_hparams(args[0], self.is_mistral_format)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LlamaEmbedNemotronModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LlamaEmbedNemotronModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA_EMBED\n@ModelBase.register(\"BailingMoeForCausalLM\")\nclass BailingMoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BAILINGMOE\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BailingMoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BailingMoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BAILINGMOE\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        if (rope_dim := hparams.get(\"head_dim\")) is None:\n            rope_dim = hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"]\n        self.gguf_writer.add_rope_dimension_count(rope_dim)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BailingMoeV2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BailingMoeV2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.BAILINGMOE2\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if nextn_layers := self.hparams.get(\"num_nextn_predict_layers\", 0):\n            self.block_count = self.hparams[\"num_hidden_layers\"] + nextn_layers\n            self.tensor_map = gguf.get_tensor_name_map(self.model_arch, self.block_count)\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n    def set_gguf_parameters(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GroveMoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GroveMoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GROVEMOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (moe_intermediate_size := self.hparams.get(\"moe_intermediate_size\")) is not None:\n            self.gguf_writer.add_expert_feed_forward_length(moe_intermediate_size)\n            logger.info(f\"gguf: expert feed forward length = {moe_intermediate_size}\")\n        # FIXME?: Hardcoded https://huggingface.co/inclusionAI/GroveMoE-Inst/blob/c4c69e5970d18907b5e6ddccdfd55176fe292df1/modeling_grove_moe.py#L299",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ChameleonModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ChameleonModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.CHAMELEON\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_swin_norm(self.hparams.get(\"swin_norm\", False))\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # ignore image tokenizer for now\n        # TODO: remove this once image support is implemented for Chameleon",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "UltravoxModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class UltravoxModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA # dummy\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        raise NotImplementedError(\"Ultravox does not have text decoder. Instead, it uses Llama or other models for text. If you want to get the audio encoder, please use --mmproj argument\")\n@ModelBase.register(\"GlmasrModel\")\nclass GlmASRWhisperEncoderModel(MmprojModel):\n    has_vision_encoder = False\n    has_audio_encoder = True\n    def __init__(self, *args, **kwargs):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GlmASRWhisperEncoderModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GlmASRWhisperEncoderModel(MmprojModel):\n    has_vision_encoder = False\n    has_audio_encoder = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if \"hidden_size\" not in self.hparams and \"intermediate_size\" not in self.hparams:\n            self.hparams[\"hidden_size\"] = self.hparams[\"d_model\"]\n            self.hparams[\"intermediate_size\"] = self.hparams[\"encoder_ffn_dim\"]\n            self.hparams[\"num_attention_heads\"] = self.hparams[\"encoder_attention_heads\"]\n    def set_gguf_parameters(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "WhisperEncoderModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class WhisperEncoderModel(MmprojModel):\n    has_vision_encoder = False # no vision encoder\n    has_audio_encoder = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if \"hidden_size\" not in self.hparams and \"intermediate_size\" not in self.hparams:\n            self.hparams[\"hidden_size\"] = self.hparams[\"d_model\"]\n            self.hparams[\"intermediate_size\"] = self.hparams[\"encoder_ffn_dim\"]\n            self.hparams[\"num_attention_heads\"] = self.hparams[\"encoder_attention_heads\"]\n    def set_gguf_parameters(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "UltravoxWhisperEncoderModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class UltravoxWhisperEncoderModel(WhisperEncoderModel):\n    has_vision_encoder = False # no vision encoder\n    has_audio_encoder = True\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.ULTRAVOX)\n        self.gguf_writer.add_audio_stack_factor(self.global_config[\"stack_factor\"])\n@ModelBase.register(\"VoxtralForConditionalGeneration\")\nclass VoxtralWhisperEncoderModel(WhisperEncoderModel):\n    has_vision_encoder = False # no vision encoder",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "VoxtralWhisperEncoderModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class VoxtralWhisperEncoderModel(WhisperEncoderModel):\n    has_vision_encoder = False # no vision encoder\n    has_audio_encoder = True\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.VOXTRAL)\n        self.gguf_writer.add_audio_stack_factor(4) # == intermediate_size // hidden_size\n@ModelBase.register(\"AudioFlamingo3ForConditionalGeneration\")\nclass AudioFlamingo3WhisperEncoderModel(WhisperEncoderModel):\n    def set_gguf_parameters(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "AudioFlamingo3WhisperEncoderModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class AudioFlamingo3WhisperEncoderModel(WhisperEncoderModel):\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.MUSIC_FLAMINGO)\n    def tensor_force_quant(self, name, new_name, bid, n_dims):\n        if \".conv\" in name and \".weight\" in name:\n            # Was trained in BF16, being safe, avoiding quantizing to FP16\n            return gguf.GGMLQuantizationType.F32\n        return super().tensor_force_quant(name, new_name, bid, n_dims)\n@ModelBase.register(\"FalconH1ForCausalLM\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "FalconH1Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class FalconH1Model(Mamba2Model):\n    model_arch = gguf.MODEL_ARCH.FALCON_H1\n    def __init__(self, *args, **kwargs):\n        # Set the hparam prefixes for Falcon Mamba2\n        self.hparam_prefixes = [\"mamba\"]\n        # Initialize the base Mamba2Model\n        super().__init__(*args, **kwargs)\n        # Use Llama conversion for attention\n        self._transformer_model_class = LlamaModel\n        # n_group and d_inner are used during reshape_tensors for mamba2",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "HunYuanMoEModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class HunYuanMoEModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.HUNYUAN_MOE\n    def set_vocab(self):\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n        # 1. Get the pre-tokenizer identifier hash\n        tokpre = self.get_vocab_base_pre(tokenizer)\n        # 2. Reverse-engineer the merges list from mergeable_ranks\n        merges = []\n        vocab = {}",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LLaDAMoEModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LLaDAMoEModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.LLADA_MOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (expert_intermediate_size := self.hparams.get(\"expert_intermediate_size\")) is not None:\n            self.gguf_writer.add_expert_feed_forward_length(expert_intermediate_size)\n        # number of experts used per token (top-k)\n        if (n_experts_used := self.hparams.get(\"num_experts_per_tok\")) is not None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "HunYuanModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class HunYuanModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.HUNYUAN_DENSE\n    def set_vocab(self):\n        if (self.dir_model / \"tokenizer.json\").is_file():\n            self._set_vocab_gpt2()\n        else:\n            from transformers import AutoTokenizer\n            tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n            # 1. Get the pre-tokenizer identifier hash\n            tokpre = self.get_vocab_base_pre(tokenizer)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "SmolLM3Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SmolLM3Model(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.SMOLLM3\n@ModelBase.register(\"GptOssForCausalLM\")\nclass GptOssModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GPT_OSS\n    # TODO: remove once MXFP4 is supported more generally\n    def dequant_model(self):\n        quant_config = self.hparams.get(\"quantization_config\")\n        if quant_config is not None and quant_config.get(\"quant_method\") == \"mxfp4\":\n            return",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GptOssModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GptOssModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GPT_OSS\n    # TODO: remove once MXFP4 is supported more generally\n    def dequant_model(self):\n        quant_config = self.hparams.get(\"quantization_config\")\n        if quant_config is not None and quant_config.get(\"quant_method\") == \"mxfp4\":\n            return\n        return super().dequant_model()\n    def transform_nibble_layout(self, tensor):\n        assert tensor.dtype == torch.uint8",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LFM2Model",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LFM2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.LFM2\n    def _add_feed_forward_length(self):\n        ff_dim = self.hparams[\"block_ff_dim\"]\n        auto_adjust_ff_dim = self.hparams[\"block_auto_adjust_ff_dim\"]\n        ff_dim = self.hparams[\"block_ff_dim\"]\n        ffn_dim_multiplier = self.hparams[\"block_ffn_dim_multiplier\"]\n        multiple_of = self.hparams[\"block_multiple_of\"]\n        if auto_adjust_ff_dim:\n            ff_dim = int(2 * ff_dim / 3)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LFM2ColBertModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LFM2ColBertModel(LFM2Model):\n    model_arch = gguf.MODEL_ARCH.LFM2\n    dense_tensor_name = \"dense_2\"\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        if not name.startswith(self.dense_tensor_name):\n            name = \"model.\" + name\n        return super().modify_tensors(data_torch, name, bid)\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:\n        # dense tensor is stored in a separate safetensors file\n        from safetensors.torch import load_file",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LFM2MoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LFM2MoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.LFM2MOE\n    def set_gguf_parameters(self):\n        # set num_key_value_heads only for attention layers\n        self.hparams[\"num_key_value_heads\"] = [\n            self.hparams[\"num_key_value_heads\"] if layer_type == \"full_attention\" else 0\n            for layer_type in self.hparams[\"layer_types\"]\n        ]\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_count(self.hparams[\"num_experts\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LFM2VLModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LFM2VLModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        # TODO(tarek): for dynamic resolution image_size is not specified, setting here for compatibility\n        self.hparams_vision[\"image_size\"] = 256\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.LFM2)\n        self.gguf_writer.add_vision_attention_layernorm_eps(self.find_vparam([\"layer_norm_eps\"]))",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LFM2AudioModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LFM2AudioModel(ConformerAudioModel):\n    has_vision_encoder = False\n    has_audio_encoder = True\n    model_name = \"Lfm2AudioEncoder\"\n    def get_audio_config(self) -> dict[str, Any] | None:\n        return self.global_config.get(\"encoder\")\n    def set_gguf_parameters(self):\n        assert self.hparams_audio is not None\n        self.hparams_audio[\"hidden_size\"] = self.hparams_audio[\"d_model\"]\n        self.hparams_audio[\"intermediate_size\"] = self.hparams_audio[\"d_model\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "SmallThinkerModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SmallThinkerModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.SMALLTHINKER\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (n_experts := self.hparams.get(\"num_experts\", self.hparams.get(\"moe_num_primary_experts\"))) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (n_experts_used := self.hparams.get(\"num_experts_per_tok\", self.hparams.get(\"moe_num_active_primary_experts\"))) is not None:\n            self.gguf_writer.add_expert_used_count(n_experts_used)\n        if (moe_intermediate_size := self.hparams.get(\"moe_ffn_hidden_size\")) is not None:\n            self.gguf_writer.add_expert_feed_forward_length(moe_intermediate_size)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ModernBertModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ModernBertModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.MODERN_BERT\n    def set_vocab(self):\n        self.gguf_writer.add_add_bos_token(True)\n        self.gguf_writer.add_add_eos_token(True)\n        self.gguf_writer.add_add_sep_token(True)\n        self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_sliding_window(self.hparams[\"local_attention\"])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ApertusModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ApertusModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.APERTUS\n    undo_permute = False\n    _alpha_n = {}\n    _alpha_p = {}\n    _beta = {}\n    _eps = {}\n    def modify_tensors(self, data_torch, name, bid):\n        # Handle xIELU activation parameters\n        n_layers = self.hparams[\"num_hidden_layers\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MistralModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MistralModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.MISTRAL3\n    model_name = \"Mistral\"\n    hf_arch = \"\"\n    is_mistral_format = True\n    undo_permute = False\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # for compatibility, we use LLAMA arch for older models\n        # TODO: remove this once everyone migrates to newer version of llama.cpp",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MistralMoeModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MistralMoeModel(DeepseekV2Model):\n    model_arch = gguf.MODEL_ARCH.DEEPSEEK2\n    model_name = \"Mistral\"\n    hf_arch = \"\"\n    is_mistral_format = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        logger.info(\"Using MistralMoeModel\")\n        # remap hparams from Mistral MoE format to DeepseekV2 format\n        # we do this way to be able to reuse DeepseekV2Model set_gguf_parameters logic",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "PixtralModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class PixtralModel(LlavaVisionModel):\n    model_name = \"Pixtral\"\n    hf_arch = \"\"\n    is_mistral_format = True\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.PIXTRAL)\n        self.gguf_writer.add_vision_attention_layernorm_eps(\n            self.find_hparam([\"norm_eps\"])\n        )",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LightOnOCRVisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LightOnOCRVisionModel(LlavaVisionModel):\n    is_mistral_format = False\n    use_break_tok = False\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.LIGHTONOCR)\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None):\n        name = name.replace(\"model.vision_encoder.\", \"vision_tower.\")\n        name = name.replace(\"model.vision_projection.\", \"multi_modal_projector.\")\n        return super().modify_tensors(data_torch, name, bid)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "KimiVLModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class KimiVLModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        self.hparams_vision[\"image_size\"] = 64 * 14 # for compatibility\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.KIMIVL)\n        self.gguf_writer.add_vision_use_gelu(True)\n        self.gguf_writer.add_vision_projector_scale_factor(2)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "CogVLMVisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class CogVLMVisionModel(MmprojModel):\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_vision_attention_layernorm_eps(self.hparams.get(\"layer_norm_eps\", 1e-6))\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.COGVLM)\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n        if not name.startswith(\"model.vision.\"):\n            return []\n        return [(self.map_tensor_name(name), data_torch)]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "CogVLMModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class CogVLMModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.COGVLM\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n        # block vision tensors\n        if name.startswith(\"model.vision.\"):\n            return []\n        return [(self.map_tensor_name(name), data_torch)]\n@ModelBase.register(\"JanusForConditionalGeneration\")\nclass JanusProModel(LlamaModel):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "JanusProModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class JanusProModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA  # reuse Llama arch\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # Skip vision, aligner, and generation tensors\n        skip_prefixes = (\n            'model.vision_model.',\n            'model.aligner.',\n            'model.vqmodel.',\n            'model.generation_embeddings.',\n            'model.generation_aligner.',",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "JanusProVisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class JanusProVisionModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        if \"intermediate_size\" not in self.hparams_vision:\n            mlp_ratio = self.hparams_vision.get(\"mlp_ratio\")\n            hidden_size = self.hparams_vision.get(\"hidden_size\")\n            if mlp_ratio is not None and hidden_size is not None:\n                self.hparams_vision[\"intermediate_size\"] = int(round(hidden_size * mlp_ratio))\n    def set_gguf_parameters(self):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "YoutuVLVisionModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class YoutuVLVisionModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        self.hparams_vision[\"image_size\"] = self.hparams_vision.get(\"image_size\", 560)\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.YOUTUVL)\n        self.gguf_writer.add_vision_attention_layernorm_eps(self.hparams.get(\"layer_norm_eps\", 1e-6))\n        # Handle activation function",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "SolarOpenModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SolarOpenModel(Glm4MoeModel):\n    model_arch = gguf.MODEL_ARCH.GLM4_MOE\n    def set_vocab(self):\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model)\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=True)\n        tokens, toktypes, tokpre = self.get_vocab_base()\n        self.gguf_writer.add_tokenizer_model(\"gpt2\")\n        self.gguf_writer.add_tokenizer_pre(tokpre)\n        self.gguf_writer.add_token_list(tokens)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LazyTorchTensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LazyTorchTensor(gguf.LazyBase):\n    _tensor_type = torch.Tensor\n    # to keep the type-checker happy\n    dtype: torch.dtype\n    shape: torch.Size\n    # only used when converting a torch.Tensor to a np.ndarray\n    _dtype_map: dict[torch.dtype, type] = {\n        torch.float16: np.float16,\n        torch.float32: np.float32,\n        torch.uint8: np.uint8,",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "def parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Convert a huggingface model to a GGML compatible file\")\n    parser.add_argument(\n        \"--vocab-only\", action=\"store_true\",\n        help=\"extract only the vocab\",\n    )\n    parser.add_argument(\n        \"--outfile\", type=Path,\n        help=\"path to write to; default: based on input. {ftype} will be replaced by the outtype.\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "split_str_to_n_bytes",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "def split_str_to_n_bytes(split_str: str) -> int:\n    if split_str.endswith(\"K\"):\n        n = int(split_str[:-1]) * 1000\n    elif split_str.endswith(\"M\"):\n        n = int(split_str[:-1]) * 1000 * 1000\n    elif split_str.endswith(\"G\"):\n        n = int(split_str[:-1]) * 1000 * 1000 * 1000\n    elif split_str.isnumeric():\n        n = int(split_str)\n    else:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "get_model_architecture",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "def get_model_architecture(hparams: dict[str, Any], model_type: ModelType) -> str:\n    # TODO @ngxson : this won't work correctly if the model has both audio & vision encoders\n    # maybe we should fallback to text model's arch in that case, since not many models have both\n    text_config = hparams.get(\"text_config\", {})\n    vision_config = hparams.get(\"vision_config\", {})\n    arch = None\n    if (arches := hparams.get(\"architectures\")) is not None and len(arches) > 0:\n        arch = arches[0]\n    elif \"ssm_cfg\" in hparams:\n        # For non-hf Mamba and Mamba2 models",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "def main() -> None:\n    args = parse_args()\n    if args.print_supported_models:\n        logger.error(\"Supported models:\")\n        ModelBase.print_registered_models()\n        sys.exit(0)\n    if args.verbose:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "logger = logging.getLogger(\"hf-to-gguf\")\n###### MODEL DEFINITIONS ######\nclass SentencePieceTokenTypes(IntEnum):\n    NORMAL = 1\n    UNKNOWN = 2\n    CONTROL = 3\n    USER_DEFINED = 4\n    UNUSED = 5\n    BYTE = 6\nclass ModelType(IntEnum):",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "AnyModel",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "AnyModel = TypeVar(\"AnyModel\", bound=\"type[ModelBase]\")\nclass ModelBase:\n    _model_classes: dict[ModelType, dict[str, type[ModelBase]]] = {\n        ModelType.TEXT: {},\n        ModelType.MMPROJ: {},\n    }\n    dir_model: Path\n    ftype: gguf.LlamaFileType\n    fname_out: Path\n    is_big_endian: bool",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "TOKENIZER_TYPE",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "class TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\nDOC_STRING = \"\"\"\nThis script downloads the tokenizer models of the specified models from Huggingface and\ngenerates the get_vocab_base_pre() function for convert_hf_to_gguf.py\n/!\\\\ It is intended to be used by contributors and is not meant to be run by end users\nThis is necessary in order to analyze the type of pre-tokenizer used by the model and",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "download_file_with_auth",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "def download_file_with_auth(url, token, save_path):\n    headers = {\"Authorization\": f\"Bearer {token}\"} if token else None\n    response = sess.get(url, headers=headers)\n    response.raise_for_status()\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    with open(save_path, 'wb') as downloaded_file:\n        downloaded_file.write(response.content)\n    logger.info(f\"File {save_path} downloaded successfully\")\ndef download_model(model):\n    name = model[\"name\"]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "download_model",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "def download_model(model):\n    name = model[\"name\"]\n    repo = model[\"repo\"]\n    tokt = model[\"tokt\"]\n    os.makedirs(f\"models/tokenizers/{name}\", exist_ok=True)\n    files = [\"config.json\", \"tokenizer.json\", \"tokenizer_config.json\"]\n    if name == \"gpt-4o\":\n        # Xenova/gpt-4o is tokenizer-only, it does not contain config.json\n        files = [\"tokenizer.json\", \"tokenizer_config.json\"]\n    if tokt == TOKENIZER_TYPE.SPM:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "get_existing_models",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "def get_existing_models(convert_py):\n    pattern = r'if chkhsh == \"([a-f0-9]{64})\":\\s*\\n\\s*.*\\s*res = \"([^\"]+)\"'\n    matches = re.findall(pattern, convert_py)\n    output = {}\n    for chkhsh, res in matches:\n        output[res] = chkhsh\n    return output\nexisting_models = {}\nall_models = models.copy()\nif not args.full:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "logger = logging.getLogger(\"convert_hf_to_gguf_update\")\nsess = requests.Session()\nconvert_py_pth = pathlib.Path(\"convert_hf_to_gguf.py\")\nconvert_py = convert_py_pth.read_text(encoding=\"utf-8\")\nhf_token_pth = pathlib.Path.home() / \".cache\" / \"huggingface\" / \"token\"\nhf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "sess",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "sess = requests.Session()\nconvert_py_pth = pathlib.Path(\"convert_hf_to_gguf.py\")\nconvert_py = convert_py_pth.read_text(encoding=\"utf-8\")\nhf_token_pth = pathlib.Path.home() / \".cache\" / \"huggingface\" / \"token\"\nhf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "convert_py_pth",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "convert_py_pth = pathlib.Path(\"convert_hf_to_gguf.py\")\nconvert_py = convert_py_pth.read_text(encoding=\"utf-8\")\nhf_token_pth = pathlib.Path.home() / \".cache\" / \"huggingface\" / \"token\"\nhf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\nDOC_STRING = \"\"\"",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "convert_py",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "convert_py = convert_py_pth.read_text(encoding=\"utf-8\")\nhf_token_pth = pathlib.Path.home() / \".cache\" / \"huggingface\" / \"token\"\nhf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\nDOC_STRING = \"\"\"\nThis script downloads the tokenizer models of the specified models from Huggingface and",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "hf_token_pth",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "hf_token_pth = pathlib.Path.home() / \".cache\" / \"huggingface\" / \"token\"\nhf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\nDOC_STRING = \"\"\"\nThis script downloads the tokenizer models of the specified models from Huggingface and\ngenerates the get_vocab_base_pre() function for convert_hf_to_gguf.py",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "hf_token",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "hf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\nDOC_STRING = \"\"\"\nThis script downloads the tokenizer models of the specified models from Huggingface and\ngenerates the get_vocab_base_pre() function for convert_hf_to_gguf.py\n/!\\\\ It is intended to be used by contributors and is not meant to be run by end users",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "DOC_STRING",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "DOC_STRING = \"\"\"\nThis script downloads the tokenizer models of the specified models from Huggingface and\ngenerates the get_vocab_base_pre() function for convert_hf_to_gguf.py\n/!\\\\ It is intended to be used by contributors and is not meant to be run by end users\nThis is necessary in order to analyze the type of pre-tokenizer used by the model and\nprovide the necessary information to llama.cpp via the GGUF header in order to implement\nthe same pre-tokenizer.\nref: https://github.com/ggml-org/llama.cpp/pull/6920\nInstructions:\n- Add a new model to the \"models\" list",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "parser = argparse.ArgumentParser(description=DOC_STRING, formatter_class=argparse.RawTextHelpFormatter)\nparser.add_argument(\n    \"--full\", action=\"store_true\",\n    help=\"download full list of models - make sure you have access to all of them\",\n)\nparser.add_argument(\n    \"--check-missing\", action=\"store_true\",\n    help=\"only check for missing pre-tokenizer hashes\",\n)\nparser.add_argument(",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "args = parser.parse_args()\nhf_token = args.hf_token if args.hf_token is not None else hf_token\nif hf_token is None:\n    logger.warning(\"HF token not found. You can provide it as an argument or set it in ~/.cache/huggingface/token\")\nif args.check_missing and args.full:\n    logger.warning(\"Downloading full list of models requested, ignoring --check-missing!\")\n    args.check_missing = False\n# TODO: this string has to exercise as much pre-tokenizer functionality as possible\n#       will be updated with time - contributions welcome\nCHK_TXT = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n (normal)  (multiple emojis concatenated)   3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3  ?apple1314151 ------=======    \\'\\'\\'\\'\\'\\'```````\\\"\\\"\\\"\\\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "hf_token",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "hf_token = args.hf_token if args.hf_token is not None else hf_token\nif hf_token is None:\n    logger.warning(\"HF token not found. You can provide it as an argument or set it in ~/.cache/huggingface/token\")\nif args.check_missing and args.full:\n    logger.warning(\"Downloading full list of models requested, ignoring --check-missing!\")\n    args.check_missing = False\n# TODO: this string has to exercise as much pre-tokenizer functionality as possible\n#       will be updated with time - contributions welcome\nCHK_TXT = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n (normal)  (multiple emojis concatenated)   3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3  ?apple1314151 ------=======    \\'\\'\\'\\'\\'\\'```````\\\"\\\"\\\"\\\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'\n# TODO: add models here, base models preferred",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "CHK_TXT",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "CHK_TXT = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n (normal)  (multiple emojis concatenated)   3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3  ?apple1314151 ------=======    \\'\\'\\'\\'\\'\\'```````\\\"\\\"\\\"\\\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'\n# TODO: add models here, base models preferred\nmodels = [\n    {\"name\": \"llama-spm\",        \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/meta-llama/Llama-2-7b-hf\", },\n    {\"name\": \"llama-bpe\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Meta-Llama-3-8B\", },\n    {\"name\": \"phi-3\",            \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\", },\n    {\"name\": \"deepseek-llm\",     \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\", },\n    {\"name\": \"deepseek-coder\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\", },\n    {\"name\": \"falcon\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tiiuae/falcon-7b\", },\n    {\"name\": \"bert-bge\",         \"tokt\": TOKENIZER_TYPE.WPM, \"repo\": \"https://huggingface.co/BAAI/bge-small-en-v1.5\", },",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "models = [\n    {\"name\": \"llama-spm\",        \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/meta-llama/Llama-2-7b-hf\", },\n    {\"name\": \"llama-bpe\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Meta-Llama-3-8B\", },\n    {\"name\": \"phi-3\",            \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\", },\n    {\"name\": \"deepseek-llm\",     \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\", },\n    {\"name\": \"deepseek-coder\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\", },\n    {\"name\": \"falcon\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tiiuae/falcon-7b\", },\n    {\"name\": \"bert-bge\",         \"tokt\": TOKENIZER_TYPE.WPM, \"repo\": \"https://huggingface.co/BAAI/bge-small-en-v1.5\", },\n    {\"name\": \"falcon3\",          \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tiiuae/Falcon3-7B-Base\", },\n    {\"name\": \"bert-bge-large\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/BAAI/bge-large-zh-v1.5\", },",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "pre_computed_hashes",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "pre_computed_hashes = [\n    # chatglm-bpe has 2 hashes, why?\n    {\"name\": \"chatglm-bpe\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/THUDM/glm-4-9b-chat\", \"chkhsh\": \"b6e8e1518dc4305be2fe39c313ed643381c4da5db34a98f6a04c093f8afbe99b\"},\n    {\"name\": \"chatglm-bpe\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/THUDM/glm-4-9b-chat\", \"chkhsh\": \"81d72c7348a9f0ebe86f23298d37debe0a5e71149e29bd283904c02262b27516\"},\n    {\"name\": \"glm4\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/THUDM/glm-4-9b-hf\", \"chkhsh\": \"a1336059768a55c99a734006ffb02203cd450fed003e9a71886c88acf24fdbc2\"},\n    {\"name\": \"glm4\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/zai-org/GLM-4.5-Air\", \"chkhsh\": \"9ca2dd618e8afaf09731a7cf6e2105b373ba6a1821559f258b272fe83e6eb902\"},\n    {\"name\": \"minerva-7b\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/sapienzanlp/Minerva-7B-base-v1.0\", \"chkhsh\": \"1431a23e583c97432bc230bff598d103ddb5a1f89960c8f1d1051aaa944d0b35\"},\n    {\"name\": \"hunyuan\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tencent/Hunyuan-A13B-Instruct\", \"chkhsh\": \"7e57df22b1fe23a7b1e1c7f3dc4e3f96d43a4eb0836d0c6bdc3436d7b2f1c664\"},\n    {\"name\": \"hunyuan-dense\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tencent/Hunyuan-4B-Instruct\", \"chkhsh\": \"bba3b3366b646dbdded5dbc42d59598b849371afc42f7beafa914afaa5b70aa6\"},\n    # falcon-h1 series uses 4 different tokenizers across model sizes (0.5b - 34b), hence we need to define 4 different hashes",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "existing_models",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "existing_models = {}\nall_models = models.copy()\nif not args.full:\n    # Filter out models that already exist in convert_hf_to_gguf.py\n    existing_models = get_existing_models(convert_py)\n    all_models = models.copy()\n    models = [model for model in all_models if model[\"name\"] not in existing_models]\nif not args.check_missing:\n    logging.info(f\"Downloading {len(models)} models...\")\n    for model in models:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "all_models",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "all_models = models.copy()\nif not args.full:\n    # Filter out models that already exist in convert_hf_to_gguf.py\n    existing_models = get_existing_models(convert_py)\n    all_models = models.copy()\n    models = [model for model in all_models if model[\"name\"] not in existing_models]\nif not args.check_missing:\n    logging.info(f\"Downloading {len(models)} models...\")\n    for model in models:\n        try:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "src_ifs",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "src_ifs = \"\"\nfor model in [*pre_computed_hashes, *all_models]:\n    name = model[\"name\"]\n    tokt = model[\"tokt\"]\n    chkhsh = model.get(\"chkhsh\")\n    if tokt == TOKENIZER_TYPE.SPM or tokt == TOKENIZER_TYPE.UGM:\n        continue\n    # create the tokenizer\n    if chkhsh is not None:\n        # if the model has a pre-computed hash, use it",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "src_func",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "src_func = f\"\"\"\n    def get_vocab_base_pre(self, tokenizer) -> str:\n        # encoding this string and hashing the resulting tokens would (hopefully) give us a unique identifier that\n        # is specific for the BPE pre-tokenizer used by the model\n        # we will use this unique identifier to write a \"tokenizer.ggml.pre\" entry in the GGUF file which we can\n        # use in llama.cpp to implement the same pre-tokenizer\n        chktxt = {repr(CHK_TXT)}\n        chktok = tokenizer.encode(chktxt)\n        chkhsh = sha256(str(chktok).encode()).hexdigest()\n        logger.debug(f\"chktok: {{chktok}}\")",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "convert_py",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "convert_py = re.sub(\n    r\"(# Marker: Start get_vocab_base_pre)(.+?)( +# Marker: End get_vocab_base_pre)\",\n    lambda m: m.group(1) + src_func + m.group(3),\n    convert_py,\n    flags=re.DOTALL | re.MULTILINE,\n)\nconvert_py_pth.write_text(convert_py, encoding=\"utf-8\")\nlogger.info(\"+++ convert_hf_to_gguf.py was updated\")\n# generate tests for each tokenizer model\ntests = [",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "tests",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "tests = [\n    \"ied 4  months\",\n    \"pfel\",\n    \"\",\n    \" \",\n    \"  \",\n    \"   \",\n    \"\\t\",\n    \"\\n\",\n    \"\\n\\n\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "GGMLFormat",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class GGMLFormat(IntEnum):\n    GGML = 0\n    GGMF = 1\n    GGJT = 2\nclass GGMLFType(IntEnum):\n    ALL_F32              = 0\n    MOSTLY_F16           = 1\n    MOSTLY_Q4_0          = 2\n    MOSTLY_Q4_1          = 3\n    MOSTLY_Q4_1_SOME_F16 = 4",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "GGMLFType",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class GGMLFType(IntEnum):\n    ALL_F32              = 0\n    MOSTLY_F16           = 1\n    MOSTLY_Q4_0          = 2\n    MOSTLY_Q4_1          = 3\n    MOSTLY_Q4_1_SOME_F16 = 4\n    MOSTLY_Q8_0          = 7\n    MOSTLY_Q5_0          = 8\n    MOSTLY_Q5_1          = 9\n    MOSTLY_Q2_K          = 10",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "Hyperparameters",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class Hyperparameters:\n    def __init__(self):\n        self.n_vocab = self.n_embd = self.n_mult = self.n_head = 0\n        self.n_layer = self.n_rot = self.n_ff = 0\n        self.ftype = GGMLFType.ALL_F32\n    def set_n_ff(self, model):\n        ff_tensor_idx = model.tensor_map.get(b'layers.0.feed_forward.w1.weight')\n        assert ff_tensor_idx is not None, 'Missing layer 0 FF tensor'\n        ff_tensor = model.tensors[ff_tensor_idx]\n        self.n_ff = ff_tensor.dims[1]",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "Vocab",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class Vocab:\n    def __init__(self, load_scores = True):\n        self.items = []\n        self.load_scores = load_scores\n    def load(self, data, offset, n_vocab):\n        orig_offset = offset\n        for _ in range(n_vocab):\n            itemlen = struct.unpack('<I', data[offset:offset + 4])[0]\n            assert itemlen < 4096, 'Absurd vocab item length'\n            offset += 4",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class Tensor:\n    def __init__(self, use_padding = True):\n        self.name = None\n        self.dims: tuple[int, ...] = ()\n        self.dtype = None\n        self.start_offset = 0\n        self.len_bytes = np.int64(0)\n        self.use_padding = use_padding\n    def load(self, data, offset):\n        orig_offset = offset",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "GGMLModel",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class GGMLModel:\n    file_format: GGMLFormat\n    format_version: int\n    def __init__(self):\n        self.hyperparameters = None\n        self.vocab = None\n        self.tensor_map = {}\n        self.tensors = []\n    def validate_header(self, data, offset):\n        magic = bytes(data[offset:offset + 4])",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "GGMLToGGUF",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class GGMLToGGUF:\n    def __init__(self, ggml_model, data, cfg, params_override = None, vocab_override = None, special_vocab = None):\n        hp = ggml_model.hyperparameters\n        self.model = ggml_model\n        self.data = data\n        self.cfg = cfg\n        self.params_override = params_override\n        self.vocab_override = vocab_override\n        self.special_vocab = special_vocab\n        if params_override is not None:",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "handle_metadata",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "def handle_metadata(cfg, hp):\n    import examples.convert_legacy_llama as convert\n    assert cfg.model_metadata_dir.is_dir(), 'Metadata dir is not a directory'\n    hf_config_path   = cfg.model_metadata_dir / \"config.json\"\n    orig_config_path = cfg.model_metadata_dir / \"params.json\"\n    # We pass a fake model here. \"original\" mode will check the shapes of some\n    # tensors if information is missing in the .json file: other than that, the\n    # model data isn't used so this should be safe (at least for now).\n    fakemodel = {\n        'tok_embeddings.weight': convert.LazyTensor.__new__(convert.LazyTensor),",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "handle_args",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "def handle_args():\n    parser = argparse.ArgumentParser(description = 'Convert GGML models to GGUF')\n    parser.add_argument('--input', '-i', type = Path, required = True,\n                        help = 'Input GGMLv3 filename')\n    parser.add_argument('--output', '-o', type = Path, required = True,\n                        help ='Output GGUF filename')\n    parser.add_argument('--name',\n                        help = 'Set model name')\n    parser.add_argument('--desc',\n                        help = 'Set model description')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "def main():\n    cfg = handle_args()\n    logging.basicConfig(level=logging.DEBUG if cfg.verbose else logging.INFO)\n    logger.info(f'* Using config: {cfg}')\n    logger.warning('=== WARNING === Be aware that this conversion script is best-effort. Use a native GGUF model if possible. === WARNING ===')\n    if cfg.model_metadata_dir is None and (cfg.gqa == 1 or cfg.eps == '5.0e-06'):\n        logger.info('- Note: If converting LLaMA2, specifying \"--eps 1e-5\" is required. 70B models also need \"--gqa 8\".')\n    data = np.memmap(cfg.input, mode = 'r')\n    model = GGMLModel()\n    logger.info('* Scanning GGML input file')",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "logger = logging.getLogger(\"ggml-to-gguf\")\nclass GGMLFormat(IntEnum):\n    GGML = 0\n    GGMF = 1\n    GGJT = 2\nclass GGMLFType(IntEnum):\n    ALL_F32              = 0\n    MOSTLY_F16           = 1\n    MOSTLY_Q4_0          = 2\n    MOSTLY_Q4_1          = 3",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "PartialLoraTensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "class PartialLoraTensor:\n    A: Tensor | None = None\n    B: Tensor | None = None\n# magic to support tensor shape modifications and splitting\nclass LoraTorchTensor:\n    _lora_A: Tensor  # (n_rank, row_size)\n    _lora_B: Tensor  # (col_size, n_rank)\n    _rank: int\n    def __init__(self, A: Tensor, B: Tensor):\n        assert len(A.shape) == len(B.shape)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "LoraTorchTensor",
        "kind": 6,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "class LoraTorchTensor:\n    _lora_A: Tensor  # (n_rank, row_size)\n    _lora_B: Tensor  # (col_size, n_rank)\n    _rank: int\n    def __init__(self, A: Tensor, B: Tensor):\n        assert len(A.shape) == len(B.shape)\n        assert A.shape[-2] == B.shape[-1]\n        if A.dtype != B.dtype:\n            A = A.to(torch.float32)\n            B = B.to(torch.float32)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "get_base_tensor_name",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "def get_base_tensor_name(lora_tensor_name: str) -> str:\n    base_name = lora_tensor_name.replace(\"base_model.model.\", \"\")\n    base_name = base_name.replace(\".lora_A.weight\", \".weight\")\n    base_name = base_name.replace(\".lora_B.weight\", \".weight\")\n    # models produced by mergekit-extract-lora have token embeddings in the adapter\n    base_name = base_name.replace(\".lora_embedding_A\", \".weight\")\n    base_name = base_name.replace(\".lora_embedding_B\", \".weight\")\n    return base_name\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "def parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Convert a Hugging Face PEFT LoRA adapter to a GGUF file\")\n    parser.add_argument(\n        \"--outfile\", type=Path,\n        help=\"path to write to; default: based on input. {ftype} will be replaced by the outtype.\",\n    )\n    parser.add_argument(\n        \"--outtype\", type=str, choices=[\"f32\", \"f16\", \"bf16\", \"q8_0\", \"auto\"], default=\"f32\",\n        help=\"output format - use f32 for float32, f16 for float16, bf16 for bfloat16, q8_0 for Q8_0, auto for the highest-fidelity 16-bit float type depending on the first loaded tensor type\",",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "load_hparams_from_hf",
        "kind": 2,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "def load_hparams_from_hf(hf_model_id: str) -> tuple[dict[str, Any], Path | None]:\n    from huggingface_hub import try_to_load_from_cache\n    # normally, adapter does not come with base model config, we need to load it from AutoConfig\n    config = AutoConfig.from_pretrained(hf_model_id)\n    cache_dir = try_to_load_from_cache(hf_model_id, \"config.json\")\n    cache_dir = Path(cache_dir).parent if isinstance(cache_dir, str) else None\n    return config.to_dict(), cache_dir\nif __name__ == '__main__':\n    args = parse_args()\n    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "description": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "logger = logging.getLogger(\"lora-to-gguf\")\n@dataclass\nclass PartialLoraTensor:\n    A: Tensor | None = None\n    B: Tensor | None = None\n# magic to support tensor shape modifications and splitting\nclass LoraTorchTensor:\n    _lora_A: Tensor  # (n_rank, row_size)\n    _lora_B: Tensor  # (col_size, n_rank)\n    _rank: int",
        "detail": "modules.native-llm.android.src.main.cpp.third_party.llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "_Known",
        "kind": 6,
        "importPath": "node_modules.flatted.python.flatted",
        "description": "node_modules.flatted.python.flatted",
        "peekOfCode": "class _Known:\n    def __init__(self):\n        self.key = []\n        self.value = []\nclass _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0",
        "detail": "node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": "node_modules.flatted.python.flatted",
        "description": "node_modules.flatted.python.flatted",
        "peekOfCode": "class _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0\n    for _ in value:\n        keys.append(i)\n        i += 1\n    return keys",
        "detail": "node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "node_modules.flatted.python.flatted",
        "description": "node_modules.flatted.python.flatted",
        "peekOfCode": "def parse(value, *args, **kwargs):\n    json = _json.loads(value, *args, **kwargs)\n    wrapped = []\n    for value in json:\n        wrapped.append(_wrap(value))\n    input = []\n    for value in wrapped:\n        if isinstance(value, _String):\n            input.append(value.value)\n        else:",
        "detail": "node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "node_modules.flatted.python.flatted",
        "description": "node_modules.flatted.python.flatted",
        "peekOfCode": "def stringify(value, *args, **kwargs):\n    known = _Known()\n    input = []\n    output = []\n    i = int(_index(known, input, value))\n    while i < len(input):\n        output.append(_transform(known, input, input[i]))\n        i += 1\n    return _json.dumps(output, *args, **kwargs)",
        "detail": "node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "d",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "d = None\nwith open(r\"tokenizer.json\", 'r', encoding='utf-8') as f:\n    d = json.load(f)\n# Extract the vocabulary as a list of token strings\nvocab = []\nfor token in d['model']['vocab']:\n    vocab.append(token)\n# Transform the vocabulary into a UTF-8 String delimited by line breaks, base64 encode it, and save to a file\nwith open('vocab_base64.txt', 'wb') as f:\n    f.write(base64.b64encode(('\\n').join(vocab).encode(\"utf-8\")))",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    },
    {
        "label": "vocab",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "vocab = []\nfor token in d['model']['vocab']:\n    vocab.append(token)\n# Transform the vocabulary into a UTF-8 String delimited by line breaks, base64 encode it, and save to a file\nwith open('vocab_base64.txt', 'wb') as f:\n    f.write(base64.b64encode(('\\n').join(vocab).encode(\"utf-8\")))\n# Extract the merge data as a list of strings, where location in list indicates priority of merge.\n# Example: one merge might be \"gr a\" (indicating that \"gr\" and \"a\" merge into \"gra\")\nmerges = []\nfor merge in d['model']['merges']:",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    },
    {
        "label": "merges",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "merges = []\nfor merge in d['model']['merges']:\n    merges.append(merge)\n# Create helper map where keys are token Strings, values are their positions in the vocab.\n# Note that positions of the vocabulary do not have any special meaning in the tokenizer,\n# we are merely using them to aid with compressing the data.\nvocab_map = {}\nfor i,v in enumerate(vocab):\n    vocab_map[v] = i\nprint(len(vocab_map))",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    },
    {
        "label": "vocab_map",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "vocab_map = {}\nfor i,v in enumerate(vocab):\n    vocab_map[v] = i\nprint(len(vocab_map))\n# Each merge can be represented with 2 integers, e.g. \"merge the 5th and the 11th token in vocab\".\nintegers = []\nfor merge in merges:\n    f, t = merge.split(\" \")\n    integers.append(vocab_map[f])\n    integers.append(vocab_map[t])",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    },
    {
        "label": "integers",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "integers = []\nfor merge in merges:\n    f, t = merge.split(\" \")\n    integers.append(vocab_map[f])\n    integers.append(vocab_map[t])\n# We are going to compress the merge data into a binary format.\n# Since the vocabulary has fewer than 2^17 entries, each integer can be represented with 17 bits.\n# The first 2*17 bits define the first merge, the next 2*17 bits define the second merge, and so on.\n# First, construct an array of 17-character strings where each character is either '0' or '1'\nbit_strings = []",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    },
    {
        "label": "bit_strings",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "bit_strings = []\nfor integer in integers:\n    bit_string = bin(integer)[2:]\n    bit_string_padded = bit_string.zfill(17)\n    bit_strings.append(bit_string_padded)\n# We are going to transform the bit_strings array into a combined bit string\n# We need the combined bit string to be divisible by 8 in order to convert it to bytes,\n# so add a zero-padding-string at the end if needed.\ncount_bits = 17 * len(bit_strings)\ncount_needed_padding = count_bits % 8",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    },
    {
        "label": "count_bits",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "count_bits = 17 * len(bit_strings)\ncount_needed_padding = count_bits % 8\nif count_needed_padding != 0:\n    pad_string = '0' * count_needed_padding\n    bit_strings.append(pad_string)\n# String of zeros and ones, representing all the merges\ncombined_bit_string = ''.join(bit_strings)\n# Partition the string into 8-bit chunks and convert them to ints\nint_chunks = [int(combined_bit_string[i:i+8], 2) for i in range(0, len(combined_bit_string), 8)]\n# Convert the integer chunks into bytes",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    },
    {
        "label": "count_needed_padding",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "count_needed_padding = count_bits % 8\nif count_needed_padding != 0:\n    pad_string = '0' * count_needed_padding\n    bit_strings.append(pad_string)\n# String of zeros and ones, representing all the merges\ncombined_bit_string = ''.join(bit_strings)\n# Partition the string into 8-bit chunks and convert them to ints\nint_chunks = [int(combined_bit_string[i:i+8], 2) for i in range(0, len(combined_bit_string), 8)]\n# Convert the integer chunks into bytes\nbyte_chunks = struct.pack(f'{len(int_chunks)}B', *int_chunks)",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    },
    {
        "label": "combined_bit_string",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "combined_bit_string = ''.join(bit_strings)\n# Partition the string into 8-bit chunks and convert them to ints\nint_chunks = [int(combined_bit_string[i:i+8], 2) for i in range(0, len(combined_bit_string), 8)]\n# Convert the integer chunks into bytes\nbyte_chunks = struct.pack(f'{len(int_chunks)}B', *int_chunks)\n# Save the byte array as base64 encoded file\nwith open('merges_binary.bin', 'wb') as file:\n    file.write(base64.b64encode(byte_chunks))",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    },
    {
        "label": "int_chunks",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "int_chunks = [int(combined_bit_string[i:i+8], 2) for i in range(0, len(combined_bit_string), 8)]\n# Convert the integer chunks into bytes\nbyte_chunks = struct.pack(f'{len(int_chunks)}B', *int_chunks)\n# Save the byte array as base64 encoded file\nwith open('merges_binary.bin', 'wb') as file:\n    file.write(base64.b64encode(byte_chunks))",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    },
    {
        "label": "byte_chunks",
        "kind": 5,
        "importPath": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "description": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "peekOfCode": "byte_chunks = struct.pack(f'{len(int_chunks)}B', *int_chunks)\n# Save the byte array as base64 encoded file\nwith open('merges_binary.bin', 'wb') as file:\n    file.write(base64.b64encode(byte_chunks))",
        "detail": "node_modules.llama3-tokenizer-js.src.data-conversion",
        "documentation": {}
    }
]